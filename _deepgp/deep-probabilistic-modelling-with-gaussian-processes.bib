@string{phdthesis = {PhD Theses}}

@string{article = {Journal Papers}}
@string{book = {Books}}
@string{techreport = {Technical Reports}}
@string{unpublished = {Submitted Papers}}
@string{inproceedings = {Refereed Conference Papers}}
@string{proceedings = {Proceedings}}
@string{incollection = {In Collected Volumes}}
@string{collection = {Volumes of Collected Papers}}
@string{misc = {Miscellaneous}}
@string{patent = {Patents}}
@string{talk = {Talks}}
@string{poster = {Posters}}
@string{mainheading = {Machine Learning Publications}}
@String{bioinf = {Bioinformatics}}
@String{bmcbioinf = {BMC Bioinformatics}}

@string{RMP =      {Reviews of Modern Physics}}
@string{ieeecomp = {IEEE Computer Society Press}}
@string{pCVPR =    {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}}
@string{jasa = {Journal of the American Statistical Association}}
@string{icml =     {Proceedings of the International Conference in
                   Machine Learning}}
@string{auai =      {AUAI Press}}
@string{uai =      {Uncertainty in Artificial Intelligence}}
@string{icann =    {International Conference on Artificial Neural Networks}}
@String{jmbcell = {Mol. Biol. Cell.}}
@String{pnasusa = {Proc. Natl. Acad. Sci. USA}}
@String{jair = {Journal of Artificial Intelligence Research}}
@String{jmlr = {Journal of Machine Learning Research}}
@String{lncs = {Lecture Notes in Computer Science}}
@string{nips =     {Advances in Neural Information Processing Systems}}
@string{NC =       {Neural Computation}}
@string{ML =       {Machine Learning}}
@string{NN =       {Neural Networks}}
@string{NW =       {Network: Computation in Neural Systems}}
@string{IJNS =     {International Journal of Neural Systems}}
@string{PRa =      {Physical Review A}}
@string{PRL =      {Physical Review Letters}}
@string{EPL =      {Europhysics Letters}}
@string{icassp =   {International Conference on Acoustics, Speech and Signal Processing}}
@string{IEEE =     {IEEE Transactions on Neural Networks}}
@string{TIT =      {IEEE Transactions on Information Theory}}
@string{TKDE =     {IEEE Transactions on Knowledge and Data Engineering}}
@string{AMS =      {Annals of Mathematical Statistics}}
@string{PAMI =     {IEEE Transactions on Pattern Analysis and
                   Machine Intelligence}}
@string{DOKLADY =  {Doklady Akademiia Nauk SSSR}}
@string{network =  {Network: Computation in Neural Systems}}
@string{ijcnn =    {Proceedings of the International Joint Conference on
                   Neural Networks}}

@string{addison =  {Addison-Wesley}}
@string{mcgraw =   {McGraw-Hill}}
@string{nholland = {North Holland}}
@string{ams = {AMS}}
@string{springer = {Springer-Verlag}}
@string{harvard =      {Harvard University Press}}
@string{mit =      {MIT Press}}
@string{cup =      {Cambridge University Press}}
@string{mk =       {Morgan Kauffman}}
@string{wiley =    {John Wiley and Sons}}
@string{JRSSb =    {Journal of the Royal Statistical Society, B}}
@string{JMB =    {Journal of Molecular Biology}}

@INPROCEEDINGS{Snelson:pseudo05,
  author = {Edward Snelson and Zoubin Ghahramani},
  title = {Sparse {G}aussian Processes using Pseudo-inputs},
  abstract = {We present a new Gaussian process (GP) regression model whose covariance
	is parameterized by the locations of $M$ pseudo-input points, which
	we learn by gradient based optimization. We take $M<<N$, where $N$
	is the number of real data points, and hence obtain a sparse regression
	method which has $O(M^2N)$ training cost and $O(M^2)$ prediction
	cost per test case. We also find hyperparameters of the covariance
	function in the same joint optimization. The method can be viewed
	as a Bayesian regression model with a particular input dependent
	noise. The method turns out to be closely related to several other
	sparse GP approaches, and we discuss the relation in detail. We finally
	demonstrate its performance on some large data sets, and make a direct
	comparision to other sparse GP methods. We show that our method can
	match full GP performance with small $M$, i.e. very sparse solutions,
	and it significantly outperforms other approaches in this regime.},
  file = {SPGP_draft.pdf:http\://www.gatsby.ucl.ac.uk/~snelson/SPGP_draft.pdf:PDF},
  group = {spgp},
  booktitle = nips,
  year = {2006},
  editor = {Yair Weiss and Bernhard Sch\"olkopf and John C. Platt},
  volume = {18},
  address = {Cambridge, MA},
  publisher = {MIT Press}
}
@InProceedings{Gal:distributed14,
  title = {Distributed Variational Inference in Sparse {G}aussian Process Regression and Latent Variable Models},
author = {Yarin Gal and Mark van der Wilk and Carl E. Rasmussen},
  crossref={Ghahramani:nips14},
  year = 	 2014,
  booktitle =	 nips,
  editor    =    {Zoubin Ghahramani and Max Welling and Corinna Cortes and Neil D. Lawrence and Kilian Q. Weinberger},
  volume =	 27,
  address =	 {Cambridge, MA}
}
@misc{Dai:gpu14,
Author = {Zhenwen Dai and Andreas Damianou and James Hensman and Neil D. Lawrence},
Title = {Gaussian Process Models with Parallelization and {GPU} acceleration},
Year = {2014},
Eprint = {arXiv:1410.4984},
}
@ARTICLE{Seeger:auto17,
   author = {{Seeger}, M. and {Hetzel}, A. and {Dai}, Z. and {Lawrence}, N.~D.
	},
    title = "{Auto-Differentiating Linear Algebra}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1710.08717},
 keywords = {Computer Science - Mathematical Software, Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = oct
}


@INPROCEEDINGS{Titsias:variational09,
  author = {Michalis K. Titsias},
  title = {Variational Learning of Inducing Variables in Sparse {G}aussian Processes},
  pages = {567--574},
  title = {Artificial Intelligence and Statistics},
  year = {2009},
  editor = {David {van Dyk} and Max Welling},
  volume = {5},
  address = {Clearwater Beach, FL},
  publisher = {JMLR W\&CP 5},
  month = {16-18 April},
  booktitle = {Proceedings of the Twelfth International Workshop on Artificial Intelligence and Statistics}
}
  author =	 {Neil D. Lawrence},
  title =	 {Learning for Larger Datasets with the {G}aussian
                  Process Latent Variable Model},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {243--250},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  linkpdf =	 shefftp # {gplvmLarger.pdf},
  linksoftware = sheffieldgit # {GPmat/},
  abstract =	 {In this paper we apply the latest techniques in
                  sparse Gaussian process regression (GPR) to the
                  Gaussian process latent variable model (GP-LVM). We
                  review three techniques and discuss how they may be
                  implemented in the context of the GP-LVM. Each
                  approach is then implemented on a well known
                  benchmark data set and compared with earlier
                  attempts to sparsify the model.},
  group =	 {shefml,gp,spgp,gplvm,dimensional reduction},
  title = {Artificial Intelligence and Statistics},
  year = {2007},
  editor = {Marina Meila and Xiaotong Shen},
  address = {San Juan, Puerto Rico},
  publisher = {Omnipress},
  month = {21-24 March},
  booktitle = {Proceedings of the Eleventh International Workshop on Artificial
	Intelligence and Statistics}  
}
@ARTICLE{Quinonero:unifying05,
  author = {Joaquin {Qui\~nonero Candela} and Carl Edward Rasmussen},
  title = {A Unifying View of Sparse Approximate {G}aussian Process Regression},
  journal = jmlr,
  year = {2005},
  volume = {6},
  pages = {1939--1959},
  abstract = {We provide a new unifying view, including all existing proper probabilistic
	sparse approximations for Gaussian process regression. Our approach
	relies on expressing the \emph{effective prior} which the methods
	are using. This allows new insights to be gained, and highlights
	the relashionship between existing methods. It also allows for a
	clear theoretically justified ranking of the closeness of the known
	approximations to the corresponding full GPs. Finally we point directly
	to designs of new better sparse approximations, combining the best
	of the existing strategies, within attractive computational constraints.},
  file = {quinonero-candela05a.pdf:http\://jmlr.csail.mit.edu/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf:PDF},
  group = {spgp}
}

@ARTICLE{Tipping:probpca99,
  author = {Michael E. Tipping and Christopher M. Bishop},
  title = {Probabilistic Principal Component Analysis},
  journal = JRSSb,
  year = {1999},
  volume = {6},
  pages = {611--622},
  number = {3},
  abstract = {Principal component analysis (PCA) is a ubiquitous technique for data
	analysis and processing, but one which is not based upon a probability
	model. In this paper we demonstrate how the principal axes of a set
	of observed data vectors may be determined through maximum-likelihood
	estimation of parameters in a latent variable model closely related
	to factor analysis. We consider the properties of the associated
	likelihood function, giving an EM algorithm for estimating the principal
	subspace iteratively, and discuss, with illustrative examples, the
	advantages conveyed by this probabilistic approach to PCA.},
  doi = {doi:10.1111/1467-9868.00196},
  file = {:http\://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf:PDF},
  linkpdf = {http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf},
  linkps = {http://www.gatsby.ucl.ac.uk/~quaid/course/readings/ppca.ps}
}
@BOOK{Rasmussen:book06,
  title = {Gaussian Processes for Machine Learning},
  publisher = mit,
  year = {2006},
  author = {Carl Edward Rasmussen and Christopher K. I. Williams},
  address = {Cambridge, MA},
  abstract = {Gaussian Processes (GPs) provide a principled, practical, probabilistic
	approach to learning in kernel machines. GPs have received increased
	attention in the machine-learning community over the past decase,
	and this book provides a long-needed systematic and unified treatment
	of theoretical and practical aspects of GPs in machine learning.
	The treatment is comprehensive and self-contained, targetd at researchers
	and students in machine learning and applied statistics.\\\\
	
	 The book deals with the supervised-learning problem for both regression
	and classification, and includes detailed algorithms. A wide variety
	of covariance (kernel) functions are presented and their properties
	discussed. Model selection is discussed both from a Bayesian and
	a classical perspective. Many connections to other well known techniques
	from machine learning and statistics are discussed, including support-vector
	machines, neural networks, splines, regularization networks, relevance
	vector machines, and others. Theoretical issues including learning
	curves and the PAC-Bayesian framework are treated, and several approximation
	methods for learning with large datasets are discussed. The book
	contains illustrative examples and exercises, and code and datasets
	are available on the Web. Appendixes provide mathematical background
	and a discussion of Gaussian Markov processes.},
  group = {gp},
  isbn = {0-262-18253-X},
  label1 = {Website},
  link1 = {http://www.GaussianProcess.org/gpml}
}

@InProceedings{Cho:deep09,
title = {Kernel Methods for Deep Learning},
author = {Youngmin Cho and Lawrence K. Saul},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
pages = {342--350},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}
}

@InProceedings{Ioffe:batch15,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Sergey Ioffe and Christian Szegedy},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.}
}

@INCOLLECTION{MacKay:gpintroduction98,
  author = {David J. C. {MacKay}},
  title = {Introduction to {G}aussian {P}rocesses},
  pages = {133--166},
  abstract = {Feedforward neural networks such as multilayer perceptrons are popular
	tools for nonlinear regression and classification problems. From
	a Bayesian perspective, a choice of a neural network model can be
	viewed as defining a prior probability distribution over non-linear
	functions, and the neural network's learning process can be interpreted
	in terms of the posterior probability distribution over the unknown
	function. (Some learning algorithms search for the function with
	maximum posterior probability and other Monte Carlo methods draw
	samples from this posterior probability). In the limit of large but
	otherwise standard networks, \cite{Neal:book96} has shown that the
	prior distribution over non-linear functions implied by the Bayesian
	neural network falls in a class of probability distributions known
	as Gaussian processes. The hyperparameters of the neural network
	model determine the characteristic lengthscales of the Gaussian process.
	Neal's observation motivates the idea of discarding parameterized
	networks and working directly with Gaussian processes. Computations
	in which the parameters of the network are optimized are then replaced
	by simple matrix operations using the covariance matrix of the Gaussian
	process. In this chapter I will review work on this idea by \cite{Williams:Gaussian96},
	\cite{Neal:montecarlogp97}, \cite{Barber:Gaussian97} and \cite{Gibbs:variational00},
	and will assess whether, for supervised regression and classification
	tasks, the feedforward network has been superceded.},
  crossref = {Bishop:neural98},
  group = {gp},
  linkpsgz = {http://www.cs.toronto.edu/~mackay/gpB.ps.gz}
}
@InProceedings{Hensman:bigdata13,
  author = 	 {James Hensman and Nicol\'o Fusi and Neil D. Lawrence},
  title = 	 {{G}aussian Processes for Big Data},
  title = uai,
  year = {2013},
  editor = {Ann Nicholson and Padhraic Smyth},
  volume = {29},
  publisher = auai,
  booktitle = uai,
  linkpdf =	 {http://auai.org/uai2013/prints/papers/244.pdf},
}
@PhDThesis{Neal:bayesian94,
author = {Radford M. Neal},
year = {1994},
title = {Bayesian Learning for Neural Networks}, 
school = {Dept. of Computer Science, University of Toronto},
pdf = {http://www.cs.toronto.edu/~radford/ftp/thesis.pdf},
softwarelink = {http://www.cs.toronto.edu/~radford/fbm.software.html},
abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present difficulties - the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.

I address the first problem by defining classes of prior distributions for network parameters that reach sensible limits as the size of the network goes to infinity. In this limit, the properties of these priors can be elucidated. Some priors converge to Gaussian processes, in which functions computed by the network may be smooth, Brownian, or fractionally Brownian. Other priors converge to non-Gaussian stable processes. Interesting effects are obtained by combining priors of both sorts in networks with more than one hidden layer.

The problem of integrating over the posterior can be solved using Markov chain Monte Carlo methods. I demonstrate that the hybrid Monte Carlo algorithm, which is based on dynamical simulation, is superior to methods based on simple random walks.

I use a hybrid Monte Carlo implementation to test the performance of Bayesian neural network models on several synthetic and real data sets. Good results are obtained on small data sets when large networks are used in conjunction with priors designed to reach limits as network size increases, confirming that with Bayesian learning one need not restrict the complexity of the network based on the size of the data set. A Bayesian approach is also found to be effective in automatically determining the relevance of inputs.

Ph.D. Thesis, Dept. of Computer Science, University of Toronto, 195 pages}
}

@PhDThesis{MacKay:bayesian92,
author = {David J. C. MacKay},
year = {1992},
title = {Bayesian Methods for Adaptive Models}, 
school = {California Institute of Technology},
pdf = {http://www.inference.org.uk/mackay/thesis.pdf},
abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying
interpolation and classification problems modelled with both linear and non–linear models.
This framework quantitatively embodies `Occam's razor'. Over–complex and under–
regularised models are automatically inferred to be less probable, even though their flexibility
allows them to fit the data better.
When applied to `neural networks', the Bayesian framework makes possible (1) objective
comparison of solutions using alternative network architectures; (2) objective stopping rules
for network pruning or growing procedures; (3) objective choice of type of weight decay
terms (or regularisers); (4) on–line techniques for optimising weight decay (or regularisation
constant) magnitude; (5) a measure of the effective number of well–determined parameters
in a model; (6) quantified estimates of the error bars on network parameters and on network
output. In the case of classification models, it is shown that the careful incorporation of
error bar information into a classifier’s predictions yields improved performance.
Comparisons of the inferences of the Bayesian framework with more traditional cross–
validation methods help detect poor underlying assumptions in learning models.
The relationship of the Bayesian learning framework to `active learning' is examined.
Objective functions are discussed which measure the expected informativeness of candidate
data measurements, in the context of both interpolation and classification problems.
The concepts and methods described in this thesis are quite general and will be applicable
to other data modelling problems whether they involve regression, classification or
density estimation.}
}
@ARTICLE{McCulloch:neuron43,
  author = {Warren S. McCulloch and Walter Pitts},
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  journal = {Bulletin of Mathematical Biophysics},
  year = {1943},
  volume = {5},
  pages = {115--133},
  pdf = {https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf},
  abstract = {Because of the "all-or-none" character of nervous activity, neural
events and the relations among them can be treated by means of propositional
logic. It is found that the behavior of every net can be described
in these terms, with the addition of more complicated logical means for
nets containing circles; and that for any logical expression satisfying
certain conditions, one can find a net behaving in the fashion it describes.
It is shown that many particular choices among possible neurophysiological
assumptions are equivalent, in the sense that for every net behaving
under one assumption, there exists another net which behaves under
the other and gives the same results, although perhaps not in the
same time. Various applications of the calculus are discussed. }
}

@incollection{Snoek:practical12,
title = {Practical Bayesian Optimization of Machine Learning Algorithms},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {2951--2959},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf}
}
@Article{Lawrence:pnpca05,
  author =	 {Neil D. Lawrence},
  title =	 {Probabilistic Non-linear Principal Component
                  Analysis with {G}aussian Process Latent Variable
                  Models},
  journal =	 jmlr,
  year =	 2005,
  volume =	 6,
  pages =	 {1783--1816},
  month =	 11,
  pdf = {http://www.jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf},
  abstract =	 {Summarising a high dimensional data set with a low
                  dimensional embedding is a standard approach for
                  exploring its structure. In this paper we provide an
                  overview of some existing techniques for discovering
                  such embeddings. We then introduce a novel
                  probabilistic interpretation of principal component
                  analysis (PCA) that we term dual probabilistic PCA
                  (DPPCA). The DPPCA model has the additional
                  advantage that the linear mappings from the embedded
                  space can easily be non-linearised through Gaussian
                  processes. We refer to this model as a Gaussian
                  process latent variable model (GP-LVM). Through
                  analysis of the GP-LVM objective function, we relate
                  the model to popular spectral techniques such as
                  kernel PCA and multidimensional scaling. We then
                  review a practical algorithm for GP-LVMs in the
                  context of large data sets and develop it to also
                  handle discrete valued data and missing
                  attributes. We demonstrate the model on a range of
                  real-world and artificially generated data sets.}
}
@article{Kennedy:code00,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2673557},
 abstract = {We consider prediction and uncertainty analysis for complex computer codes which can be run at different levels of sophistication. In particular, we wish to improve efficiency by combining expensive runs of the most complex versions of the code with relatively cheap runs from one or more simpler approximations. A Bayesian approach is described in which prior beliefs about the codes are represented in terms of Gaussian processes. An example is presented using two versions of an oil reservoir simulator.},
 author = {M. C. Kennedy and A. O'Hagan},
 journal = {Biometrika},
 number = {1},
 pages = {1-13},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Predicting the Output from a Complex Computer Code When Fast Approximations Are Available},
 volume = {87},
 year = {2000}
}
@ARTICLE{LeGratiet:cokriging12,
   author = {{Le Gratiet}, L.},
    title = "{Recursive co-kriging model for Design of Computer experiments with multiple levels of fidelity with an application to hydrodynamic}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1210.0686},
 primaryClass = "math.ST",
 keywords = {Mathematics - Statistics Theory},
     year = 2012,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1210.0686L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Kandasamy:multifidelity16,
   author = {{Kandasamy}, K. and {Dasarathy}, G. and {Oliva}, J.~B. and {Schneider}, J. and 
	{Poczos}, B.},
    title = "{Multi-fidelity Gaussian Process Bandit Optimisation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1603.06288},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning},
     year = 2016,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160306288K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{Alonso:virtual17,
  title = {Virtual vs. {R}eal: Trading Off Simulations and Physical Experiments in Reinforcement Learning with {B}ayesian Optimization},
  author = {Marco Alonso and Felix Berkenkamp and Philipp Hennig and Schoellig, Angela P. and Krause, Andreas and Schaal, Stefan and Trimpe, Sebastian},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {1557-1563},
  month = may,
  year = {2017},
  month_numeric = {5}
}

@article {Perdikaris:multifidelity17,
	author = {Perdikaris, P. and Raissi, M. and Damianou, A. and Lawrence, N. D. and Karniadakis, G. E.},
	title = {Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling},
	volume = {473},
	number = {2198},
	year = {2017},
	doi = {10.1098/rspa.2016.0751},
	publisher = {The Royal Society},
	abstract = {Multi-fidelity modelling enables accurate inference of quantities of interest by synergistically combining realizations of low-cost/low-fidelity models with a small set of high-fidelity observations. This is particularly effective when the low- and high-fidelity models exhibit strong correlations, and can lead to significant computational gains over approaches that solely rely on high-fidelity models. However, in many cases of practical interest, low-fidelity models can only be well correlated to their high-fidelity counterparts for a specific range of input parameters, and potentially return wrong trends and erroneous predictions if probed outside of their validity regime. Here we put forth a probabilistic framework based on Gaussian process regression and nonlinear autoregressive schemes that is capable of learning complex nonlinear and space-dependent cross-correlations between models of variable fidelity, and can effectively safeguard against low-fidelity models that provide wrong trends. This introduces a new class of multi-fidelity information fusion algorithms that provide a fundamental extension to the existing linear autoregressive methodologies, while still maintaining the same algorithmic complexity and overall computational cost. The performance of the proposed methods is tested in several benchmark problems involving both synthetic and real multi-fidelity datasets from computational fluid dynamics simulations.},
	issn = {1364-5021},
	URL = {http://rspa.royalsocietypublishing.org/content/473/2198/20160751},
	eprint = {http://rspa.royalsocietypublishing.org/content/473/2198/20160751.full.pdf},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences}
}
@InProceedings{Ranganath-survival16,
  title = 	 {Deep Survival Analysis},
  author = 	 {Rajesh Ranganath and Adler Perotte and Noémie Elhadad and David Blei},
  booktitle = 	 {Proceedings of the 1st Machine Learning for Healthcare Conference},
  pages = 	 {101--114},
  year = 	 {2016},
  editor = 	 {Finale Doshi-Velez and Jim Fackler and David Kale and Byron Wallace and Jenna Wiens},
  volume = 	 {56},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Children's Hospital LA, Los Angeles, CA, USA},
  month = 	 {18--19 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v56/Ranganath16.pdf},
  url = 	 {http://proceedings.mlr.press/v56/Ranganath16.html},
  abstract = 	 {The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we introduce deep survival analysis, a hierarchical generative  approach to survival analysis in the context of the EHR. It departs from previous approaches in two main ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it handles heterogeneous data types that occur in the EHR. We validate deep survival analysis by stratifying patients according to risk of developing coronary heart disease (CHD) on 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is superior in stratifying patients according to their risk.}
}

@incollection{Schulam:counterfactual17,
title = {Counterfactual Gaussian Processes for Reliable Decision-making and What-if Reasoning},
author = {Schulam, Peter and Saria, Suchi},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {1696--1706},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6767-counterfactual-gaussian-processes-for-reliable-decision-making-and-what-if-reasoning.pdf}
}
@ARTICLE{Dunlop:deep2017,
   author = {{Dunlop}, M.~M. and {Girolami}, M. and {Stuart}, A.~M. and {Teckentrup}, A.~L.
	},
    title = "{How Deep Are Deep Gaussian Processes?}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.11280},
 primaryClass = "math.ST",
 keywords = {Mathematics - Statistics Theory},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171111280D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@InProceedings{Duvenaud:pathologies14,
  title = 	 {{Avoiding pathologies in very deep networks}},
  author = 	 {David Duvenaud and Oren Rippel and Ryan Adams and Zoubin Ghahramani},
  booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {202--210},
  year = 	 {2014},
  editor = 	 {Samuel Kaski and Jukka Corander},
  volume = 	 {33},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Reykjavik, Iceland},
  month = 	 {22--25 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v33/duvenaud14.pdf},
  url = 	 {http://proceedings.mlr.press/v33/duvenaud14.html},
  abstract = 	 {Choosing appropriate architectures and regularization strategies of deep networks is crucial to good predictive performance.  To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions.  Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network.  We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit.  We propose an alternate network architecture which does not suffer from this pathology.  We also examine deep covariance functions, obtained by composing infinitely many feature transforms.  Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.}
}

@incollection{Anqi:gpspike2017,
title = {Gaussian process based nonlinear latent structure discovery in multivariate spike train data},
author = {Wu, Anqi and Roy, Nicholas G and Keeley, Stephen and Pillow, Jonathan W},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {3499--3508},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6941-gaussian-process-based-nonlinear-latent-structure-discovery-in-multivariate-spike-train-data.pdf}
}

@incollection{Alaa:deep2017,
title = {Deep Multi-task {G}aussian Processes for Survival Analysis with Competing Risks},
author = {Ahmed M. Alaa and Mihaela {van der Schaar}},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {2326--2334},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6827-deep-multi-task-gaussian-processes-for-survival-analysis-with-competing-risks.pdf}
}
@PhdThesis{Saul:thesis2016,
  author = 	 {Alan Daniel Saul},
  title = 	 {Gaussian Process Based Approaches for Survival Analysis},
  school = 	 {University of Sheffield},
  year = 	 {2016},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@PhdThesis{Damianou:thesis2015,
  author = 	 {Andreas Damianou},
  title = 	 {Deep Gaussian Processes and Variational Propagation of Uncertainty},
  school = 	 {University of Sheffield},
  year = 	 {2015},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@article{Mattos:recurrent15,
  author    = {C{\'{e}}sar Lincoln C. Mattos and
               Zhenwen Dai and
               Andreas C. Damianou and
               Jeremy Forth and
               Guilherme A. Barreto and
               Neil D. Lawrence},
  title     = {Recurrent Gaussian Processes},
  journal   = {CoRR},
  volume    = {abs/1511.06644},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06644},
  archivePrefix = {arXiv},
  eprint    = {1511.06644},
  timestamp = {Wed, 07 Jun 2017 14:42:48 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/MattosDDFBL15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@incollection{Salimbeni:doubly2017,
title = {Doubly Stochastic Variational Inference for Deep Gaussian Processes},
author = {Salimbeni, Hugh and Deisenroth, Marc},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4591--4602},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf}
}

@article{Bengio:deep09,
 author = {Yoshua Bengio},
 title = {{Learning Deep Architectures for AI}},
 journal = {Found. Trends Mach. Learn.},
 issue_date = {January 2009},
 volume = {2},
 number = {1},
 month = jan,
 year = {2009},
 issn = {1935-8237},
 pages = {1--127},
 numpages = {127},
 doi = {10.1561/2200000006},
 acmid = {1658424},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@ARTICLE{Hinton:fast06,
    author = {Geoffrey E. Hinton and Simon Osindero},
    title = {A fast learning algorithm for deep belief nets},
    journal = {Neural Computation},
    year = {2006},
    volume = {18},
    pages = {2006}
}

@INPROCEEDINGS{Salakhutdinov:quantitative08,
  author = {Ruslan Salakhutdinov and Iain Murray},
  title = {On the Quantitative Analysis of Deep Belief Networks},
  pages = {872--879},
  year = {2008},
  editor = {Sam Roweis and Andrew McCallum},
  volume = {25},
  publisher = {Omnipress},
  booktitle = icml
}



@Article{Steele:predictive12,
  author = 	 {Steele, S and Bilchik, A and Eberhardt, J and Kalina, P and Nissan, A and Johnson, E and Avital, I and Stojadinovic, A},
  title = 	 {Using Machine-Learned {B}ayesian Belief Networks to Predict Perioperative Risk of Clostridium Difficile Infection Following Colon Surgery},
  journal = 	 {Interact J Med Res},
  year = 	 {2012},
  OPTkey = 	 {},
  volume =	 {1},
  number =	 {2},
  pages =	 {e6},
  doi = 	 {10.2196/ijmr.2131},
  url = 	 {https://www.i-jmr.org/2012/2/e6},
  OPTannote = 	 {}
}

@article{Thang:unifying17,
  author  = {Thang D. Bui and Josiah Yan and Richard E. Turner},
  title   = {A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {104},
  pages   = {1-72},
  url     = {http://jmlr.org/papers/v18/16-603.html}
}

