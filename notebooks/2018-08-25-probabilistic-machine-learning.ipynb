{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), Amazon Cambridge and University of Sheffield\n",
    "### 2018-08-25\n",
    "\n",
    "**Abstract**: In this talk we review the *probabilistic* approach to machine learning.\n",
    "We start with a review of probability, and introduce the concepts of\n",
    "probabilistic modelling. We then apply the approach in practice to Naive\n",
    "Bayesian classification. In this lecture we review the Bayesian\n",
    "formalism in the context of linear models, reviewing initially maximum\n",
    "likelihood and introducing basis functions as a way of driving\n",
    "non-linearity in the model.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "What is machine learning? At its most basic level machine learning is a\n",
    "combination of\n",
    "\n",
    "$$ \\text{data} + \\text{model} \\xrightarrow{\\text{compute}} \\text{prediction}$$\n",
    "\n",
    "where *data* is our observations. They can be actively or passively\n",
    "acquired (meta-data). The *model* contains our assumptions, based on\n",
    "previous experience. That experience can be other data, it can come from\n",
    "transfer learning, or it can merely be our beliefs about the\n",
    "regularities of the universe. In humans our models include our inductive\n",
    "biases. The *prediction* is an action to be taken or a categorization or\n",
    "a quality score. The reason that machine learning has become a mainstay\n",
    "of artificial intelligence is the importance of predictions in\n",
    "artificial intelligence. The data and the model are combined through\n",
    "computation.\n",
    "\n",
    "In practice we normally perform machine learning using two functions. To\n",
    "combine data with a model we typically make use of:\n",
    "\n",
    "**a prediction function** a function which is used to make the\n",
    "predictions. It includes our beliefs about the regularities of the\n",
    "universe, our assumptions about how the world works, e.g. smoothness,\n",
    "spatial similarities, temporal similarities.\n",
    "\n",
    "**an objective function** a function which defines the cost of\n",
    "misprediction. Typically it includes knowledge about the world's\n",
    "generating processes (probabilistic objectives) or the costs we pay for\n",
    "mispredictions (empiricial risk minimization).\n",
    "\n",
    "The combination of data and model through the prediction function and\n",
    "the objectie function leads to a *learning algorithm*. The class of\n",
    "prediction functions and objective functions we can make use of is\n",
    "restricted by the algorithms they lead to. If the prediction function or\n",
    "the objective function are too complex, then it can be difficult to find\n",
    "an appropriate learning algorithm. Much of the acdemic field of machine\n",
    "learning is the quest for new learning algorithms that allow us to bring\n",
    "different types of models and data together.\n",
    "\n",
    "A useful reference for state of the art in machine learning is the UK\n",
    "Royal Society Report, [Machine Learning: Power and Promise of Computers\n",
    "that Learn by\n",
    "Example](https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf).\n",
    "\n",
    "You can also check my blog post on [\"What is Machine\n",
    "Learning?\"](http://inverseprobability.com/2017/07/17/what-is-machine-learning)\n",
    "\n",
    "## Probabilities\n",
    "\n",
    "We are now going to do some simple review of probabilities and use this\n",
    "review to explore some aspects of our data.\n",
    "\n",
    "A probability distribution expresses uncertainty about the outcome of an\n",
    "event. We often encode this uncertainty in a variable. So if we are\n",
    "considering the outcome of an event, $Y$, to be a coin toss, then we\n",
    "might consider $Y=1$ to be heads and $Y=0$ to be tails. We represent the\n",
    "probability of a given outcome with the notation: $$\n",
    "P(Y=1) = 0.5\n",
    "$$ The first rule of probability is that the probability must normalize.\n",
    "The sum of the probability of all events must equal 1. So if the\n",
    "probability of heads ($Y=1$) is 0.5, then the probability of tails (the\n",
    "only other possible outcome) is given by $$\n",
    "P(Y=0) = 1-P(Y=1) = 0.5\n",
    "$$\n",
    "\n",
    "Probabilities are often defined as the limit of the ratio between the\n",
    "number of positive outcomes (e.g. *heads*) given the number of trials.\n",
    "If the number of positive outcomes for event $y$ is denoted by $n$ and\n",
    "the number of trials is denoted by $N$ then this gives the ratio $$\n",
    "P(Y=y) = \\lim_{N\\rightarrow\n",
    "\\infty}\\frac{n_y}{N}.\n",
    "$$ In practice we never get to observe an event infinite times, so\n",
    "rather than considering this we often use the following estimate $$\n",
    "P(Y=y) \\approx \\frac{n_y}{N}.\n",
    "$$ Let's use this rule to compute the approximate probability that a\n",
    "film from the movie body count website has over 40 deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths = (film_deaths.Body_Count>40).sum()  # number of positive outcomes (in sum True counts as 1, False counts as 0)\n",
    "total_films = film_deaths.Body_Count.count()\n",
    "\n",
    "prob_death = float(deaths)/float(total_films)\n",
    "print(\"Probability of deaths being greather than 40 is:\", prob_death)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "We now have an estimate of the probability a film has greater than 40\n",
    "deaths. The estimate seems quite high. What could be wrong with the\n",
    "estimate? Do you think any film you go to in the cinema has this\n",
    "probability of having greater than 40 deaths?\n",
    "\n",
    "Why did we have to use `float` around our counts of deaths and total\n",
    "films? What would the answer have been if we hadn't used the `float`\n",
    "command? If we were using Python 3 would we have this problem?\n",
    "\n",
    "*20 marks*\n",
    "\n",
    "### Write your answer to Question 4 here\n",
    "\n",
    "## Conditioning\n",
    "\n",
    "When predicting whether a coin turns up head or tails, we might think\n",
    "that this event is *independent* of the year or time of day. If we\n",
    "include an observation such as time, then in a probability this is known\n",
    "as *condtioning*. We use this notation, $P(Y=y|T=t)$, to condition the\n",
    "outcome on a second variable (in this case time). Or, often, for a\n",
    "shorthand we use $P(y|t)$ to represent this distribution (the $Y=$ and\n",
    "$T=$ being implicit). Because we don't believe a coin toss depends on\n",
    "time then we might write that $$\n",
    "P(y|t) =\n",
    "p(y).\n",
    "$$ However, we might believe that the number of deaths is dependent on\n",
    "the year. For this we can try estimating $P(Y>40 | T=2000)$ and compare\n",
    "the result, for example to $P(Y>40|2002)$ using our empirical estimate\n",
    "of the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [2000, 2002]:\n",
    "    deaths = (film_deaths.Body_Count[film_deaths.Year==year]>40).sum()\n",
    "    total_films = (film_deaths.Year==year).sum()\n",
    "\n",
    "    prob_death = float(deaths)/float(total_films)\n",
    "    print(\"Probability of deaths being greather than 40 in year\", year, \"is:\", prob_death)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Compute the probability for the number of deaths being over 40 for each\n",
    "year we have in our `film_deaths` data frame. Store the result in a\n",
    "`numpy` array and plot the probabilities against the years using the\n",
    "`plot` command from `matplotlib`. Do you think the estimate we have\n",
    "created of $P(y|t)$ is a good estimate? Write your code and your written\n",
    "answers in the box below.\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Question 5 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 Answer Text\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "#### Notes for Question 5\n",
    "\n",
    "Make sure the plot is included in *this* notebook file (the `IPython`\n",
    "magic command `%matplotlib inline` we ran above will do that for you, it\n",
    "only needs to be run once per file).\n",
    "\n",
    "  Terminology   Mathematical notation   Description\n",
    "  ------------- ----------------------- ----------------------------------\n",
    "  joint         $P(X=x, Y=y)$           prob. that X=x *and* Y=y\n",
    "  marginal      $P(X=x)$                prob. that X=x *regardless of* Y\n",
    "  conditional   $P(X=x\\vert Y=y)$       prob. that X=x *given that* Y=y\n",
    "\n",
    "<center>\n",
    "The different basic probability distributions.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.prob_diagram(diagrams='../slides/diagrams/mlai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Pictorial Definition of Probability\n",
    "\n",
    "<img src=\"../slides/diagrams/mlai/prob_diagram.svg\" align=\"\">\n",
    "\n",
    "[Inspired by lectures from Christopher Bishop]{align=\"right\"}\n",
    "\n",
    "### Definition of probability distributions.\n",
    "\n",
    "  Terminology                                             Definition                                                                                                                                                                              Probability Notation\n",
    "  ------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------\n",
    "  Joint Probability                                       $\\lim_{N\\rightarrow\\infty}\\frac{n_{X=3,Y=4}}{N}$                                                                                                                                        $P\\left(X=3,Y=4\\right)$\n",
    "  Marginal Probability                                    $\\lim_{N\\rightarrow\\infty}\\frac{n_{X=5}}{N}$                                                                                                                                            $P\\left(X=5\\right)$\n",
    "  Conditional Probability                                 $\\lim_{N\\rightarrow\\infty}\\frac{n_{X=3,Y=4}}{n_{Y=4}}$                                                                                                                                  $P\\left(X=3\\vert Y=4\\right)$\n",
    "\n",
    "### Notational Details\n",
    "\n",
    "Typically we should write out $P\\left(X=x,Y=y\\right)$, but in practice\n",
    "we often shorten this to $P\\left(x,y\\right)$. This looks very much like\n",
    "we might write a multivariate function, *e.g.*\n",
    "\n",
    "$$f\\left(x,y\\right)=\\frac{x}{y},$$\n",
    "\n",
    "but for a multivariate funciton\n",
    "\n",
    "$$f\\left(x,y\\right)\\neq f\\left(y,x\\right)$$.\n",
    "\n",
    "However\n",
    "\n",
    "$$P\\left(x,y\\right)=P\\left(y,x\\right)$$\n",
    "\n",
    "because\n",
    "\n",
    "$$P\\left(X=x,Y=y\\right)=P\\left(Y=y,X=x\\right).$$\n",
    "\n",
    "Sometimes I think of this as akin to the way in Python we can write\n",
    "'keyword arguments' in functions. If we use keyword arguments, the\n",
    "ordering of arguments doesn't matter.\n",
    "\n",
    "We've now introduced conditioning and independence to the notion of\n",
    "probability and computed some conditional probabilities on a practical\n",
    "example The scatter plot of deaths vs year that we created above can be\n",
    "seen as a *joint* probability distribution. We represent a joint\n",
    "probability using the notation $P(Y=y, T=t)$ or $P(y, t)$ for short.\n",
    "Computing a joint probability is equivalent to answering the\n",
    "simultaneous questions, what's the probability that the number of deaths\n",
    "was over 40 and the year was 2002? Or any other question that may occur\n",
    "to us. Again we can easily use pandas to ask such questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2000\n",
    "deaths = (film_deaths.Body_Count[film_deaths.Year==year]>40).sum()\n",
    "total_films = film_deaths.Body_Count.count() # this is total number of films\n",
    "prob_death = float(deaths)/float(total_films)\n",
    "print(\"Probability of deaths being greather than 40 and year being\", year, \"is:\", prob_death)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Product Rule\n",
    "\n",
    "This number is the joint probability, $P(Y, T)$ which is much *smaller*\n",
    "than the conditional probability. The number can never be bigger than\n",
    "the conditional probabililty because it is computed using the *product\n",
    "rule*. $$\n",
    "p(Y=y, T=t) = p(Y=y|T=t)p(T=t)\n",
    "$$ and $$p(T=t)$$ is a probability distribution, which is equal or less\n",
    "than 1, ensuring the joint distribution is typically smaller than the\n",
    "conditional distribution.\n",
    "\n",
    "The product rule is a *fundamental* rule of probability, and you must\n",
    "remember it! It gives the relationship between the two questions: 1)\n",
    "What's the probability that a film was made in 2002 and has over 40\n",
    "deaths? and 2) What's the probability that a film has over 40 deaths\n",
    "given that it was made in 2002?\n",
    "\n",
    "In our shorter notation we can write the product rule as $$\n",
    "p(y, t) = p(y|t)p(t)\n",
    "$$ We can see the relation working in practice for our data above by\n",
    "computing the different values for $t=2000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = float((film_deaths.Year==2002).sum())/float(film_deaths.Body_Count.count())\n",
    "p_y_given_t = float((film_deaths.Body_Count[film_deaths.Year==2002]>40).sum())/float((film_deaths.Year==2002).sum())\n",
    "p_y_and_t = float((film_deaths.Body_Count[film_deaths.Year==2002]>40).sum())/float(film_deaths.Body_Count.count())\n",
    "\n",
    "print(\"P(t) is\", p_t)\n",
    "print(\"P(y|t) is\", p_y_given_t)\n",
    "print(\"P(y,t) is\", p_y_and_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sum Rule\n",
    "\n",
    "The other *fundamental rule* of probability is the *sum rule* this tells\n",
    "us how to get a *marginal* distribution from the joint distribution.\n",
    "Simply put it says that we need to sum across the value we'd like to\n",
    "remove. $$\n",
    "P(Y=y) = \\sum_{t} P(Y=y, T=t)\n",
    "$$ Or in our shortened notation $$\n",
    "P(y) = \\sum_{t}\n",
    "P(y, t)\n",
    "$$\n",
    "\n",
    "### Question 6\n",
    "\n",
    "Write code that computes $P(y)$ by adding $P(y, t)$ for all values of\n",
    "$t$.\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Question 6 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes’ Rule\n",
    "\n",
    "Bayes rule is a very simple rule, it's hardly worth the name of a rule\n",
    "at all. It follows directly from the product rule of probability.\n",
    "Because $P(y, t) = P(y|t)P(t)$ and by symmetry\n",
    "$P(y,t)=P(t,y)=P(t|y)P(y)$ then by equating these two equations and\n",
    "dividing through by $P(y)$ we have $$\n",
    "P(t|y) =\n",
    "\\frac{P(y|t)P(t)}{P(y)}\n",
    "$$ which is known as Bayes' rule (or Bayes's rule, it depends how you\n",
    "choose to pronounce it). It's not difficult to derive, and its\n",
    "importance is more to do with the semantic operation that it enables.\n",
    "Each of these probability distributions represents the answer to a\n",
    "question we have about the world. Bayes rule (via the product rule)\n",
    "tells us how to *invert* the probability.\n",
    "\n",
    "### Probabilities for Extracting Information from Data\n",
    "\n",
    "What use is all this probability in data science? Let's think about how\n",
    "we might use the probabilities to do some decision making. Let's load up\n",
    "a little more information about the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('./R-vs-Python-master/Deadliest movies scrape/code/film-death-counts-Python.csv')\n",
    "movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Now we see we have several additional features including the quality\n",
    "rating (`IMDB_Rating`). Let's assume we want to predict the rating given\n",
    "the other information in the data base. How would we go about doing it?\n",
    "\n",
    "Using what you've learnt about joint, conditional and marginal\n",
    "probabilities, as well as the sum and product rule, how would you\n",
    "formulate the question you want to answer in terms of probabilities?\n",
    "Should you be using a joint or a conditional distribution? If it's\n",
    "conditional, what should the distribution be over, and what should it be\n",
    "conditioned on?\n",
    "\n",
    "*20 marks*\n",
    "\n",
    "### Write your answer to Question 7 here\n",
    "\n",
    "### Probabilistic Modelling\n",
    "\n",
    "This Bayesian approach is designed to deal with uncertainty arising from\n",
    "fitting our prediction function to the data we have, a reduced data set.\n",
    "\n",
    "The Bayesian approach can be derived from a broader understanding of\n",
    "what our objective is. If we accept that we can jointly represent all\n",
    "things that happen in the world with a probability distribution, then we\n",
    "can interogate that probability to make predictions. So, if we are\n",
    "interested in predictions, $\\dataScalar_*$ at future points input\n",
    "locations of interest, $\\inputVector_*$ given previously training data,\n",
    "$\\dataVector$ and corresponding inputs, $\\inputMatrix$, then we are\n",
    "really interogating the following probability density, $$\n",
    "p(\\dataScalar_*|\\dataVector, \\inputMatrix, \\inputVector_*),\n",
    "$$ there is nothing controversial here, as long as you accept that you\n",
    "have a good joint model of the world around you that relates test data\n",
    "to training data,\n",
    "$p(\\dataScalar_*, \\dataVector, \\inputMatrix, \\inputVector_*)$ then this\n",
    "conditional distribution can be recovered through standard rules of\n",
    "probability\n",
    "($\\text{data} + \\text{model} \\rightarrow \\text{prediction}$).\n",
    "\n",
    "We can construct this joint density through the use of the following\n",
    "decomposition: $$\n",
    "p(\\dataScalar_*|\\dataVector, \\inputMatrix, \\inputVector_*) = \\int p(\\dataScalar_*|\\inputVector_*, \\mappingMatrix) p(\\mappingMatrix | \\dataVector, \\inputMatrix) \\text{d} \\mappingMatrix\n",
    "$$\n",
    "\n",
    "where, for convenience, we are assuming *all* the parameters of the\n",
    "model are now represented by $\\parameterVector$ (which contains\n",
    "$\\mappingMatrix$ and $\\mappingMatrixTwo$) and\n",
    "$p(\\parameterVector | \\dataVector, \\inputMatrix)$ is recognised as the\n",
    "posterior density of the parameters given data and\n",
    "$p(\\dataScalar_*|\\inputVector_*, \\parameterVector)$ is the *likelihood*\n",
    "of an individual test data point given the parameters.\n",
    "\n",
    "The likelihood of the data is normally assumed to be independent across\n",
    "the parameters, $$\n",
    "p(\\dataVector|\\inputMatrix, \\mappingMatrix) \\prod_{i=1}^\\numData p(\\dataScalar_i|\\inputVector_i, \\mappingMatrix),$$\n",
    "\n",
    "and if that is so, it is easy to extend our predictions across all\n",
    "future, potential, locations, $$\n",
    "p(\\dataVector_*|\\dataVector, \\inputMatrix, \\inputMatrix_*) = \\int p(\\dataVector_*|\\inputMatrix_*, \\parameterVector) p(\\parameterVector | \\dataVector, \\inputMatrix) \\text{d} \\parameterVector.\n",
    "$$\n",
    "\n",
    "The likelihood is also where the *prediction function* is incorporated.\n",
    "For example in the regression case, we consider an objective based\n",
    "around the Gaussian density, $$\n",
    "p(\\dataScalar_i | \\mappingFunction(\\inputVector_i)) = \\frac{1}{\\sqrt{2\\pi \\dataStd^2}} \\exp\\left(-\\frac{\\left(\\dataScalar_i - \\mappingFunction(\\inputVector_i)\\right)^2}{2\\dataStd^2}\\right)\n",
    "$$\n",
    "\n",
    "In short, that is the classical approach to probabilistic inference, and\n",
    "all approaches to Bayesian neural networks fall within this path. For a\n",
    "deep probabilistic model, we can simply take this one stage further and\n",
    "place a probability distribution over the input locations, $$\n",
    "p(\\dataVector_*|\\dataVector) = \\int p(\\dataVector_*|\\inputMatrix_*, \\parameterVector) p(\\parameterVector | \\dataVector, \\inputMatrix) p(\\inputMatrix) p(\\inputMatrix_*) \\text{d} \\parameterVector \\text{d} \\inputMatrix \\text{d}\\inputMatrix_*\n",
    "$$ and we have *unsupervised learning* (from where we can get deep\n",
    "generative models).\n",
    "\n",
    "### Graphical Models\n",
    "\n",
    "One way of representing a joint distribution is to consider conditional\n",
    "dependencies between data. Conditional dependencies allow us to\n",
    "factorize the distribution. For example, a Markov chain is a\n",
    "factorization of a distribution into components that represent the\n",
    "conditional relationships between points that are neighboring, often in\n",
    "time or space. It can be decomposed in the following form.\n",
    "$$p(\\dataVector) = p(\\dataScalar_\\numData | \\dataScalar_{\\numData-1}) p(\\dataScalar_{\\numData-1}|\\dataScalar_{\\numData-2}) \\dots p(\\dataScalar_{2} | \\dataScalar_{1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica']}, size=30)\n",
    "rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = daft.PGM(shape=[3, 1],\n",
    "               origin=[0, 0], \n",
    "               grid_unit=5, \n",
    "               node_unit=1.9, \n",
    "               observed_style='shaded',\n",
    "              line_width=3)\n",
    "\n",
    "\n",
    "pgm.add_node(daft.Node(\"y_1\", r\"$y_1$\", 0.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_2\", r\"$y_2$\", 1.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_3\", r\"$y_3$\", 2.5, 0.5, fixed=False))\n",
    "pgm.add_edge(\"y_1\", \"y_2\")\n",
    "pgm.add_edge(\"y_2\", \"y_3\")\n",
    "\n",
    "pgm.render().figure.savefig(\"../slides/diagrams/ml/markov.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../slides/diagrams/ml/markov.svg\" align=\"\">\n",
    "\n",
    "By specifying conditional independencies we can reduce the\n",
    "parameterization required for our data, instead of directly specifying\n",
    "the parameters of the joint distribution, we can specify each set of\n",
    "parameters of the conditonal independently. This can also give an\n",
    "advantage in terms of interpretability. Understanding a conditional\n",
    "independence structure gives a structured understanding of data. If\n",
    "developed correctly, according to causal methodology, it can even inform\n",
    "how we should intervene in the system to drive a desired result\n",
    "[@Pearl:causality95].\n",
    "\n",
    "However, a challenge arise when the data becomes more complex. Consider\n",
    "the graphical model shown below, used to predict the perioperative risk\n",
    "of *C Difficile* infection following colon surgery\n",
    "[@Steele:predictive12].\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/bayes-net-diagnosis.png\" width=\"40%\" align=\"center\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "\n",
    "To capture the complexity in the interelationship between the data the\n",
    "graph becomes more complex, and less interpretable.\n",
    "\n",
    "### References {#references .unnumbered}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
