---
title: "The Atomic Human"
venue: "St Andrews’ Distinguished Lecture Series"
abstract: "<p>A vital perspective is missing from the discussions we’re
having about Artificial Intelligence: what does it mean for our
identity?</p>
<p>Our fascination with AI stems from the perceived uniqueness of human
intelligence. We believe it’s what differentiates us. Fears of AI not
only concern how it invades our digital lives, but also the implied
threat of an intelligence that displaces us from our position at the
centre of the world.</p>
<p>Atomism, proposed by Democritus, suggested it was impossible to
continue dividing matter down into ever smaller components: eventually
we reach a point where a cut cannot be made (the Greek for uncuttable is
‘atom’). In the same way, by slicing away at the facets of human
intelligence that can be replaced by machines, AI uncovers what is left:
an indivisible core that is the essence of humanity.</p>
<p>By contrasting our own (evolved, locked-in, embodied) intelligence
with the capabilities of machine intelligence through history, The
Atomic Human reveals the technical origins, capabilities and limitations
of AI systems, and how they should be wielded. Not just by the experts,
but ordinary people. Either AI is a tool for us, or we become a tool of
AI. Understanding this will enable us to choose the future we want.</p>
<p>This talk is based on Neil’s forthcoming book to be published with
Allen Lane in June 2024. Machine learning solutions, in particular those
based on deep learning methods, form an underpinning of the current
revolution in “artificial intelligence” that has dominated popular press
headlines and is having a significant influence on the wider tech
agenda.</p>
<p>In this talk I will give an overview of where we are now with machine
learning solutions, and what challenges we face both in the near and far
future. These include practical application of existing algorithms in
the face of the need to explain decision making, mechanisms for
improving the quality and availability of data, dealing with large
unstructured datasets.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_ai/the-atomic-human-st-andrews.md
date: 2024-03-12
published: 2024-03-12
reveal: 2024-03-12-the-atomic-human-st-andrews.slides.html
transition: None
ipynb: 2024-03-12-the-atomic-human-st-andrews.ipynb
slidesipynb: 2024-03-12-the-atomic-human-st-andrews.slides.ipynb
layout: talk
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="henry-fords-faster-horse">Henry Ford’s Faster Horse</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/henry-ford-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/henry-ford-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="ford-model-t-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/1925_Ford_Model_T_touring.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="ford-model-t-magnify" class="magnify"
onclick="magnifyFigure(&#39;ford-model-t&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ford-model-t-caption" class="caption-frame">
<p>Figure: A 1925 Ford Model T built at Henry Ford’s Highland Park Plant
in Dearborn, Michigan. This example now resides in Australia, owned by
the founder of FordModelT.net. From <a
href="https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg"
class="uri">https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg</a></p>
</div>
</div>
<p>It’s said that Henry Ford’s customers wanted a “a faster horse”. If
Henry Ford was selling us artificial intelligence today, what would the
customer call for, “a smarter human”? That’s certainly the picture of
machine intelligence we find in science fiction narratives, but the
reality of what we’ve developed is much more mundane.</p>
<p>Car engines produce prodigious power from petrol. Machine
intelligences deliver decisions derived from data. In both cases the
scale of consumption enables a speed of operation that is far beyond the
capabilities of their natural counterparts. Unfettered energy
consumption has consequences in the form of climate change. Does
unbridled data consumption also have consequences for us?</p>
<p>If we devolve decision making to machines, we depend on those
machines to accommodate our needs. If we don’t understand how those
machines operate, we lose control over our destiny. Our mistake has been
to see machine intelligence as a reflection of our intelligence. We
cannot understand the smarter human without understanding the human. To
understand the machine, we need to better understand ourselves.</p>
<h2 id="the-diving-bell-and-the-butterfly">The Diving Bell and the
Butterfly</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/the-diving-bell-and-the-butterfly.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-diving-bell-and-the-butterfly.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="diving-bell-and-butterfly-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-diving-bell-and-the-butterfly.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="diving-bell-and-butterfly-magnify" class="magnify"
onclick="magnifyFigure(&#39;diving-bell-and-butterfly&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="diving-bell-and-butterfly-caption" class="caption-frame">
<p>Figure: The Diving Bell and the Buttefly is the autobiography of Jean
Dominique Bauby.</p>
</div>
</div>
<p><a
href="https://www.penguinrandomhouse.com/books/9616/the-diving-bell-and-the-butterfly-by-jean-dominique-bauby/">The
Diving Bell and the Butterfly</a> is the autobiography of Jean Dominique
Bauby. Jean Dominique, the editor of French Elle magazine, suffered a
major stroke at the age of 43 in 1995. The stroke paralyzed him and
rendered him speechless. He was only able to blink his left eyelid, he
became a sufferer of locked in syndrome.</p>
<div class="figure">
<div id="diving-bell-letters-figure" class="figure-frame">
<div style="text-align:center;font-size:200%">
E S A R I N T U L <br> O M D P C F B V <br> H G J Q Z Y X K W
</div>
</div>
<div id="diving-bell-letters-magnify" class="magnify"
onclick="magnifyFigure(&#39;diving-bell-letters&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="diving-bell-letters-caption" class="caption-frame">
<p>Figure: The ordering of the letters that Bauby used for writing his
autobiography.</p>
</div>
</div>
<p>How could he do that? Well, first, they set up a mechanism where he
could scan across letters and blink at the letter he wanted to use. In
this way, he was able to write each letter.</p>
<p>It took him 10 months of four hours a day to write the book. Each
word took two minutes to write.</p>
<p>Imagine doing all that thinking, but so little speaking, having all
those thoughts and so little ability to communicate.</p>
<p>The idea behind this talk is that we are all in that situation. While
not as extreme as for Bauby, we all have somewhat of a locked in
intelligence.</p>
<div class="figure">
<div id="jean-dominique-bauby-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jean-Dominique_Bauby.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="jean-dominique-bauby-magnify" class="magnify"
onclick="magnifyFigure(&#39;jean-dominique-bauby&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="jean-dominique-bauby-caption" class="caption-frame">
<p>Figure: Jean Dominique Bauby was the Editor in Chief of the French
Elle Magazine, he suffered a stroke that destroyed his brainstem,
leaving him only capable of moving one eye. Jean Dominique became a
victim of locked in syndrome.</p>
</div>
</div>
<p>Incredibly, Jean Dominique wrote his book after he became locked in.
It took him 10 months of four hours a day to write the book. Each word
took two minutes to write.</p>
<p>The idea behind embodiment factors is that we are all in that
situation. While not as extreme as for Bauby, we all have somewhat of a
locked in intelligence.</p>
<div class="figure">
<div id="bauby-shannon-figure" class="figure-frame">
<table>
<tr>
<td width>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jean-Dominique_Bauby.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ClaudeShannon_MFO3807.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="bauby-shannon-magnify" class="magnify"
onclick="magnifyFigure(&#39;bauby-shannon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="bauby-shannon-caption" class="caption-frame">
<p>Figure: Claude Shannon developed information theory which allows us
to quantify how much Bauby can communicate. This allows us to compare
how locked in he is to us.</p>
</div>
</div>
<h2 id="the-atomic-human">The Atomic Human</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-eye.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-eye.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-atomic-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/the-atomic-eye.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-atomic-eye-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-atomic-eye&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-atomic-eye-caption" class="caption-frame">
<p>Figure: The Atomic Eye, by slicing away aspects of the human that we
used to believe to be unique to us, but are now the preserve of the
machine, we learn something about what it means to be human.</p>
</div>
</div>
<div class="figure">
<div id="colossus-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//computing/Colossus.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="colossus-magnify" class="magnify"
onclick="magnifyFigure(&#39;colossus&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="colossus-caption" class="caption-frame">
<p>Figure: A Colossus Mark 2 codebreaking computer being operated by
Dorothy Du Boisson (left) and Elsie Booker (right). Colossus was
designed by Tommy Flowers, but programmed and operated by groups of
Wrens based at Bletchley Park.</p>
</div>
</div>
<h2 id="embodiment-factors">Embodiment Factors</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
<div id="embodiment-factors-table-magnify" class="magnify"
onclick="magnifyFigure(&#39;embodiment-factors-table&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="embodiment-factors-table-caption" class="caption-frame">
<p>Figure: Embodiment factors are the ratio between our ability to
compute and our ability to communicate. Relative to the machine we are
also locked in. In the table we represent embodiment as the length of
time it would take to communicate one second’s worth of computation. For
computers it is a matter of minutes, but for a human, it is a matter of
thousands of millions of years. See also “Living Together: Mind and
Machine Intelligence” <span class="citation"
data-cites="Lawrence:embodiment17">Lawrence (2017)</span></p>
</div>
</div>
<p>There is a fundamental limit placed on our intelligence based on our
ability to communicate. Claude Shannon founded the field of information
theory. The clever part of this theory is it allows us to separate our
measurement of information from what the information pertains to.<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></p>
<p>Shannon measured information in bits. One bit of information is the
amount of information I pass to you when I give you the result of a coin
toss. Shannon was also interested in the amount of information in the
English language. He estimated that on average a word in the English
language contains 12 bits of information.</p>
<p>Given typical speaking rates, that gives us an estimate of our
ability to communicate of around 100 bits per second <span
class="citation" data-cites="Reed-information98">(Reed and Durlach,
1998)</span>. Computers on the other hand can communicate much more
rapidly. Current wired network speeds are around a billion bits per
second, ten million times faster.</p>
<p>When it comes to compute though, our best estimates indicate our
computers are slower. A typical modern computer can process make around
100 billion floating-point operations per second, each floating-point
operation involves a 64 bit number. So the computer is processing around
6,400 billion bits per second.</p>
<p>It’s difficult to get similar estimates for humans, but by some
estimates the amount of compute we would require to <em>simulate</em> a
human brain is equivalent to that in the UK’s fastest computer <span
class="citation" data-cites="Ananthanarayanan-cat09">(Ananthanarayanan
et al., 2009)</span>, the MET office machine in Exeter, which in 2018
ranked as the 11th fastest computer in the world. That machine simulates
the world’s weather each morning, and then simulates the world’s climate
in the afternoon. It is a 16-petaflop machine, processing around 1,000
<em>trillion</em> bits per second.</p>
<h2 id="new-flow-of-information">New Flow of Information</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classically the field of statistics focused on mediating the
relationship between the machine and the human. Our limited bandwidth of
communication means we tend to over-interpret the limited information
that we are given, in the extreme we assign motives and desires to
inanimate objects (a process known as anthropomorphizing). Much of
mathematical statistics was developed to help temper this tendency and
understand when we are valid in drawing conclusions from data.</p>
<div class="figure">
<div id="new-flow-of-information-3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information003.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-3-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-3&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-3-caption" class="caption-frame">
<p>Figure: The trinity of human, data, and computer, and highlights the
modern phenomenon. The communication channel between computer and data
now has an extremely high bandwidth. The channel between human and
computer and the channel between data and human is narrow. New direction
of information flow, information is reaching us mediated by the
computer. The focus on classical statistics reflected the importance of
the direct communication between human and data. The modern challenges
of data science emerge when that relationship is being mediated by the
machine.</p>
</div>
</div>
<p>Data science brings new challenges. In particular, there is a very
large bandwidth connection between the machine and data. This means that
our relationship with data is now commonly being mediated by the
machine. Whether this is in the acquisition of new data, which now
happens by happenstance rather than with purpose, or the interpretation
of that data where we are increasingly relying on machines to summarize
what the data contains. This is leading to the emerging field of data
science, which must not only deal with the same challenges that
mathematical statistics faced in tempering our tendency to over
interpret data but must also deal with the possibility that the machine
has either inadvertently or maliciously misrepresented the underlying
data.</p>
<h3 id="bandwidth-constrained-conversations">Bandwidth Constrained
Conversations</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/anne-bob-talk.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/anne-bob-talk.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-bob-conversation-civil-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation006.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-bob-conversation-civil-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-bob-conversation-civil&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-bob-conversation-civil-caption" class="caption-frame">
<p>Figure: Conversation relies on internal models of other
individuals.</p>
</div>
</div>
<div class="figure">
<div id="anne-bob-conversation-argument-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation007.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-bob-conversation-argument-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-bob-conversation-argument&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-bob-conversation-argument-caption" class="caption-frame">
<p>Figure: Misunderstanding of context and who we are talking to leads
to arguments.</p>
</div>
</div>
<p>Embodiment factors imply that, in our communication between humans,
what is <em>not</em> said is, perhaps, more important than what is said.
To communicate with each other we need to have a model of who each of us
are.</p>
<p>To aid this, in society, we are required to perform roles. Whether as
a parent, a teacher, an employee or a boss. Each of these roles requires
that we conform to certain standards of behaviour to facilitate
communication between ourselves.</p>
<p>Control of self is vitally important to these communications.</p>
<p>The high availability of data available to humans undermines
human-to-human communication channels by providing new routes to
undermining our control of self.</p>
<p>The consequences between this mismatch of power and delivery are to
be seen all around us. Because, just as driving an F1 car with bicycle
wheels would be a fine art, so is the process of communication between
humans.</p>
<p>If I have a thought and I wish to communicate it, I first need to
have a model of what you think. I should think before I speak. When I
speak, you may react. You have a model of who I am and what I was trying
to say, and why I chose to say what I said. Now we begin this dance,
where we are each trying to better understand each other and what we are
saying. When it works, it is beautiful, but when mis-deployed, just like
a badly driven F1 car, there is a horrible crash, an argument.</p>
<h2 id="sistine-chapel-ceiling">Sistine Chapel Ceiling</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-sistine-chapel-ceiling.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-sistine-chapel-ceiling.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="cappella-sistina-ceiling-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-sistine-chapel-ceiling.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="cappella-sistina-ceiling-magnify" class="magnify"
onclick="magnifyFigure(&#39;cappella-sistina-ceiling&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="cappella-sistina-ceiling-caption" class="caption-frame">
<p>Figure: The ceiling of the Sistine Chapel.</p>
</div>
</div>
<p><a href="https://www.mmll.cam.ac.uk/pb127">Patrick Boyde</a>’s talks
on the Sistine Chapel focussed on both the structure of the chapel
ceiling, describing the impression of height it was intended to give, as
well as the significance and positioning of each of the panels and the
meaning of the individual figures.</p>
<h2 id="the-creation-of-man">The Creation of Man</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-the-creation-of-man.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-the-creation-of-man.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-creation-of-man-michelangelo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-the-creation-of-man.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-creation-of-man-michelangelo-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-creation-of-man-michelangelo&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-creation-of-man-michelangelo-caption"
class="caption-frame">
<p>Figure: Photo of Detail of Creation of Man from the Sistine chapel
ceiling.</p>
</div>
</div>
<p>One of the most famous panels is central in the ceiling, it’s the
creation of man. Here, God in the guise of a pink-robed bearded man
reaches out to a languid Adam.</p>
<p>The representation of God in this form seems typical of the time,
because elsewhere in the Vatican Museums there are similar
representations.</p>
<div class="figure">
<div id="the-creation-of-man-detail-god-michelangelo-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-the-creation-of-man-detail-god.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-creation-of-man-detail-god-michelangelo-magnify"
class="magnify"
onclick="magnifyFigure(&#39;the-creation-of-man-detail-god-michelangelo&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-creation-of-man-detail-god-michelangelo-caption"
class="caption-frame">
<p>Figure: Photo detail of God.</p>
</div>
</div>
<p><a
href="https://commons.wikimedia.org/wiki/File:Michelangelo,_Creation_of_Adam_04.jpg"
class="uri">https://commons.wikimedia.org/wiki/File:Michelangelo,_Creation_of_Adam_04.jpg</a></p>
<h3 id="a-six-word-novel">A Six Word Novel</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/baby-shoes.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/baby-shoes.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="classic-baby-shoes-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Classic_baby_shoes.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="classic-baby-shoes-magnify" class="magnify"
onclick="magnifyFigure(&#39;classic-baby-shoes&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="classic-baby-shoes-caption" class="caption-frame">
<p>Figure: Consider the six-word novel, apocryphally credited to Ernest
Hemingway, “For sale: baby shoes, never worn”. To understand what that
means to a human, you need a great deal of additional context. Context
that is not directly accessible to a machine that has not got both the
evolved and contextual understanding of our own condition to realize
both the implication of the advert and what that implication means
emotionally to the previous owner.</p>
</div>
</div>
<p>But this is a very different kind of intelligence than ours. A
computer cannot understand the depth of the Ernest Hemingway’s
apocryphal six-word novel: “For Sale, Baby Shoes, Never worn”, because
it isn’t equipped with that ability to model the complexity of humanity
that underlies that statement.</p>
<h2 id="revolution">Revolution</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/cuneiform.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/cuneiform.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Arguably the information revolution we are experiencing is
unprecedented in history. But changes in the way we share information
have a long history. Over 5,000 years ago in the city of Uruk, on the
banks of the Euphrates, communities which relied on the water to
irrigate their corps developed an approach to recording transactions in
clay. Eventually the system of recording system became sophisticated
enough that their oral histories could be recorded in the form of the
first epic: Gilgamesh.</p>
<div class="figure">
<div id="chicago-cuneiform-stone-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//cuneiform/chicago-cuneiform-stone.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="chicago-cuneiform-stone-magnify" class="magnify"
onclick="magnifyFigure(&#39;chicago-cuneiform-stone&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="chicago-cuneiform-stone-caption" class="caption-frame">
<p>Figure: Chicago Stone, side 2, recording sale of a number of fields,
probably from Isin, Early Dynastic Period, c. 2600 BC, black basalt</p>
</div>
</div>
<p>It was initially develoepd for people as a recordd of who owed what
to whom, expanding individuals’ capacity to remember. But over a five
hundred year period writing evolved to become a tool for literature as
well. More pithily put, writing was invented by accountants not poets
(see e.g. <a href="https://www.bbc.co.uk/news/business-39870485">this
piece by Tim Harford</a>).</p>
<p>In some respects today’s revolution is different, because it involves
also the creation of stories as well as their curation. But in some
fundamental ways we can see what we have produced as another tool for us
in the information revolution.</p>
<h1 id="the-future-of-professions">The Future of Professions</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/the-future-of-professions.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-future-of-professions.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-future-of-professions-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/the-future-of-professions.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-future-of-professions-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-future-of-professions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-future-of-professions-caption" class="caption-frame">
<p>Figure: <a
href="https://www.amazon.co.uk/Future-Professions-Technology-Transform-Experts/dp/0198713398">The
Future of Professions</a> <span class="citation"
data-cites="Susskind-future15">(Susskind and Susskind, 2015)</span> is a
2015 book focussed on how the next wave of technology revolution is
going to effect the professions.</p>
</div>
</div>
<h2 id="coin-pusher">Coin Pusher</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_policy/includes/coin-pusher.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_policy/includes/coin-pusher.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Disruption of society is like a coin pusher, it’s those who are
already on the edge who are most likely to be effected by
disruption.</p>
<div class="figure">
<div id="coin-pusher-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//policy/Coin_pusher_2.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="coin-pusher-magnify" class="magnify"
onclick="magnifyFigure(&#39;coin-pusher&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="coin-pusher-caption" class="caption-frame">
<p>Figure: A coin pusher is a game where coins are dropped into th etop
of the machine, and they disrupt those on the existing steps. With any
coin drop, many coins move, but it is those on the edge, who are often
only indirectly effected, but also most traumatically effected by the
change.</p>
</div>
</div>
<p>One danger of the current hype around ChatGPT is that we are overly
focussing on the fact that it seems to have significant effect on
professional jobs, people are naturally asking the question “what does
it do for my role?”. No doubt, there will be disruption, but the coin
pusher hypothesis suggests that that disruption will likely involve
movement on the same step. However it is those on the edge already, who
are often not working directly in the information economy, who often
have less of a voice in the policy conversation who are likely to be
most disrupted.</p>
<!-- AI Fallacy -->
<h1 id="the-great-ai-fallacy">The Great AI Fallacy</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/the-great-ai-fallacy.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/the-great-ai-fallacy.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>There is a lot of variation in the use of the term artificial
intelligence. I’m sometimes asked to define it, but depending on whether
you’re speaking to a member of the public, a fellow machine learning
researcher, or someone from the business community, the sense of the
term differs.</p>
<p>However, underlying its use I’ve detected one disturbing trend. A
trend I’m beginining to think of as “The Great AI Fallacy”.</p>
<p>The fallacy is associated with an implicit promise that is embedded
in many statements about Artificial Intelligence. Artificial
Intelligence, as it currently exists, is merely a form of automated
decision making. The implicit promise of Artificial Intelligence is that
it will be the first wave of automation where the machine adapts to the
human, rather than the human adapting to the machine.</p>
<p>How else can we explain the suspension of sensible business judgment
that is accompanying the hype surrounding AI?</p>
<p>This fallacy is particularly pernicious because there are serious
benefits to society in deploying this new wave of data-driven automated
decision making. But the AI Fallacy is causing us to suspend our
calibrated skepticism that is needed to deploy these systems safely and
efficiently.</p>
<p>The problem is compounded because many of the techniques that we’re
speaking of were originally developed in academic laboratories in
isolation from real-world deployment.</p>
<div class="figure">
<div id="jeeves-springtime-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jeeves_in_the_Springtime_01.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="jeeves-springtime-magnify" class="magnify"
onclick="magnifyFigure(&#39;jeeves-springtime&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="jeeves-springtime-caption" class="caption-frame">
<p>Figure: We seem to have fallen for a perspective on AI that suggests
it will adapt to our schedule, rather in the manner of a 1930s
manservant.</p>
</div>
</div>
<p>In Greek mythology, Panacea was the goddess of the universal remedy.
One consequence of the pervasive potential of AI is that it is
positioned, like Panacea, as the purveyor of a universal solution.
Whether it is overcoming industry’s productivity challenges, or as a
salve for strained public sector services, or a remedy for pressing
global challenges in sustainable development, AI is presented as an
elixir to resolve society’s problems.</p>
<p>In practice, translation of AI technology into practical benefit is
not simple. Moreover, a growing body of evidence shows that risks and
benefits from AI innovations are unevenly distributed across
society.</p>
<p>When carelessly deployed, AI risks exacerbating existing social and
economic inequalities.</p>
<!-- Mathematical Statistics -->
<h2 id="lies-and-damned-lies">Lies and Damned Lies</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/lies-damned-lies.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/lies-damned-lies.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
<p>Arthur Balfour was quoting the lawyer James Munro<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
when he said that there three types of lies: lies, damned lies and
statistics in 1892. This is 20 years before the first academic
department of applied statistics was founded at UCL. If Balfour were
alive today, it is likely that he’d rephrase his quote:</p>
<blockquote>
<p>There are three types of lies, lies damned lies and <em>big
data</em>.</p>
</blockquote>
<p>Why? Because the challenges of understanding and interpreting big
data today are similar to those that Balfour (who was a Conservative
politician and statesman and would later become Prime Minister) faced in
governing an empire through statistics in the latter part of the 19th
century.</p>
<p>The quote lies, damned lies and statistics was also credited to
Benjamin Disraeli by Mark Twain in Twain’s autobiography.<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> It
characterizes the idea that statistic can be made to prove anything. But
Disraeli died in 1881 and Mark Twain died in 1910. The important
breakthrough in overcoming our tendency to over-interpet data came with
the formalization of the field through the development of
<em>mathematical statistics</em>.</p>
<p>Data has an elusive quality, it promises so much but can deliver
little, it can mislead and misrepresent. To harness it, it must be
tamed. In Balfour and Disraeli’s time during the second half of the 19th
century, numbers and data were being accumulated, the social sciences
were being developed. There was a large-scale collection of data for the
purposes of government.</p>
<p>The modern ‘big data era’ is on the verge of delivering the same
sense of frustration that Balfour experienced, the early promise of big
data as a panacea is evolving to demands for delivery. For me,
personally, peak-hype coincided with an email I received inviting
collaboration on a project to deploy “<em>Big Data</em> and <em>Internet
of Things</em> in an <em>Industry 4.0</em> environment”. Further
questioning revealed that the actual project was optimization of the
efficiency of a manufacturing production line, a far more tangible and
<em>realizable</em> goal.</p>
<p>The antidote to this verbiage is found in increasing awareness. When
dealing with data the first trap to avoid is the games of buzzword bingo
that we are wont to play. The first goal is to quantify what challenges
can be addressed and what techniques are required. Behind the hype
fundamentals are changing. The phenomenon is about the increasing access
we have to data. The way customers’ information is recorded and
processes are codified and digitized with little overhead. Internet of
things is about the increasing number of cheap sensors that can be
easily interconnected through our modern network structures. But
businesses are about making money, and these phenomena need to be recast
in those terms before their value can be realized.</p>
<p>For more thoughts on the challenges that statistics brings see
Chapter 8 of <span class="citation"
data-cites="Lawrence-atomic24">Lawrence (2024)</span>.</p>
<h2 id="mathematical-statistics"><em>Mathematical</em> Statistics</h2>
<p><a href="https://en.wikipedia.org/wiki/Karl_Pearson">Karl Pearson</a>
(1857-1936), <a
href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a>
(1890-1962) and others considered the question of what conclusions can
truly be drawn from data. Their mathematical studies act as a restraint
on our tendency to over-interpret and see patterns where there are none.
They introduced concepts such as randomized control trials that form a
mainstay of our decision making today, from government, to clinicians to
large scale A/B testing that determines the nature of the web interfaces
we interact with on social media and shopping.</p>
<div class="figure">
<div id="portrait-of-karl-pearson-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Portrait_of_Karl_Pearson.jpg" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="portrait-of-karl-pearson-magnify" class="magnify"
onclick="magnifyFigure(&#39;portrait-of-karl-pearson&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="portrait-of-karl-pearson-caption" class="caption-frame">
<p>Figure: Karl Pearson (1857-1936), one of the founders of Mathematical
Statistics.</p>
</div>
</div>
<p>Their movement did the most to put statistics to rights, to eradicate
the ‘damned lies’. It was known as <a
href="https://en.wikipedia.org/wiki/Mathematical_statistics">‘mathematical
statistics’</a>. Today I believe we should look to the emerging field of
<em>data science</em> to provide the same role. Data science is an
amalgam of statistics, data mining, computer systems, databases,
computation, machine learning and artificial intelligence. Spread across
these fields are the tools we need to realize data’s potential. For many
businesses this might be thought of as the challenge of ‘converting bits
into atoms’. Bits: the data stored on computer, atoms: the physical
manifestation of what we do; the transfer of goods, the delivery of
service. From fungible to tangible. When solving a challenge through
data there are a series of obstacles that need to be addressed.</p>
<p>Firstly, data awareness: what data you have and where its stored.
Sometimes this includes changing your conception of what data is and how
it can be obtained. From automated production lines to apps on employee
smart phones. Often data is locked away: manual logbooks, confidential
data, personal data. For increasing awareness an internal audit can
help. The website <a href="https://data.gov.uk/">data.gov.uk</a> hosts
data made available by the UK government. To create this website the
government’s departments went through an audit of what data they each
hold and what data they could make available. Similarly, within private
businesses this type of audit could be useful for understanding their
internal digital landscape: after all the key to any successful campaign
is a good map.</p>
<p>Secondly, availability. How well are the data sources interconnected?
How well curated are they? The curse of Disraeli was associated with
unreliable data and <em>unreliable statistics</em>. The
misrepresentations this leads to are worse than the absence of data as
they give a false sense of confidence to decision making. Understanding
how to avoid these pitfalls involves an improved sense of data and its
value, one that needs to permeate the organization.</p>
<p>The final challenge is analysis, the accumulation of the necessary
expertise to digest what the data tells us. Data requires
interpretation, and interpretation requires experience. Analysis is
providing a bottleneck due to a skill shortage, a skill shortage made
more acute by the fact that, ideally, analysis should be carried out by
individuals not only skilled in data science but also equipped with the
domain knowledge to understand the implications in a given application,
and to see opportunities for improvements in efficiency.</p>
<h2 id="mathematical-data-science">‘Mathematical Data Science’</h2>
<p>As a term ‘big data’ promises much and delivers little, to get true
value from data, it needs to be curated and evaluated. The three stages
of awareness, availability and analysis provide a broad framework
through which organizations should be assessing the potential in the
data they hold. Hand waving about big data solutions will not do, it
will only lead to self-deception. The castles we build on our data
landscapes must be based on firm foundations, process and scientific
analysis. If we do things right, those are the foundations that will be
provided by the new field of data science.</p>
<p>Today the statement “There are three types of lies: lies, damned lies
and ‘big data’” may be more apt. We are revisiting many of the mistakes
made in interpreting data from the 19th century. Big data is laid down
by happenstance, rather than actively collected with a particular
question in mind. That means it needs to be treated with care when
conclusions are being drawn. For data science to succeed it needs the
same form of rigor that Pearson and Fisher brought to statistics, a
“mathematical data science” is needed.</p>
<p>You can also check my blog post on <a
href="http://inverseprobability.com/2016/11/19/lies-damned-lies-big-data">Lies,
Damned Lies and Big Data</a>.</p>
<p>I’m reminded of this because from 2015 to 2017 I was on the Working
Group that compiled the Royal Society’s machine learning report. The
process of constructing the report went across the UK Referendum, and
the 2016 US election. I remember vividly a meeting we convened at the
Society in London which had experts alongside MPs from all parties,
policy advisors and civil servants. One of the MPs (likely correctly)
pointed out “I suspect no one around this table voted for Brexit” to
which I replied “But isn’t that the problem? There are a large number of
people who aren’t empowered who are experiencing quite a different
reality than us. And they aren’t reprented in these forums.” So it’s no
surprise that so much of the press conversation around AI is still
focussed on how it is likely to effect middle class jobs. We shouldn’t
underestimate these effects, but it’s often the case that better
educated people are better placed to deal with such challenges. For
example, when stock brokers’ roles disappeared they simply moved on to
other roles in banks and related industries.</p>
<h2 id="royal-society-report">Royal Society Report</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/rs-report-machine-learning.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/rs-report-machine-learning.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="ml-report-cover-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/ml-report-cover-page.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="ml-report-cover-magnify" class="magnify"
onclick="magnifyFigure(&#39;ml-report-cover&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ml-report-cover-caption" class="caption-frame">
<p>Figure: The Royal Society report on Machine Learning was released on
25th April 2017</p>
</div>
</div>
<p>A useful reference for state of the art in machine learning is the UK
Royal Society Report, <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a>.</p>
<h2 id="public-research">Public Research</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/rs-report-mori-poll-art.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/rs-report-mori-poll-art.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="rs-report-mori-poll-cover-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-report-mori-poll-cover.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="rs-report-mori-poll-cover-magnify" class="magnify"
onclick="magnifyFigure(&#39;rs-report-mori-poll-cover&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="rs-report-mori-poll-cover-caption" class="caption-frame">
<p>Figure: The Royal Society comissioned <a
href="https://royalsociety.org/-/media/policy/projects/machine-learning/publications/public-views-of-machine-learning-ipsos-mori.pdf">public
research from Mori</a> as part of the machine learning review.</p>
</div>
</div>
<div class="figure">
<div id="rs-report-mori-poll-1-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-1.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="rs-report-mori-poll-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;rs-report-mori-poll-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="rs-report-mori-poll-1-caption" class="caption-frame">
<p>Figure: One of the questions focussed on machine learning
applications.</p>
</div>
</div>
<div class="figure">
<div id="rs-report-mori-poll-2-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-2.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="rs-report-mori-poll-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;rs-report-mori-poll-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="rs-report-mori-poll-2-caption" class="caption-frame">
<p>Figure: The public were broadly supportive of a range of application
areas.</p>
</div>
</div>
<div class="figure">
<div id="rs-report-mori-poll-3-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-3.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="rs-report-mori-poll-3-magnify" class="magnify"
onclick="magnifyFigure(&#39;rs-report-mori-poll-3&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="rs-report-mori-poll-3-caption" class="caption-frame">
<p>Figure: But they failed to see the point in AI’s that could produce
poetry.</p>
</div>
</div>
<!-- Fritz Heider -->
<h3 id="heider-and-simmel-1944">Heider and Simmel (1944)</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/heider-simmel.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/heider-simmel.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="heider-simmel-shapes-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/8FIEZXMUM2I?start=7" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="heider-simmel-shapes-magnify" class="magnify"
onclick="magnifyFigure(&#39;heider-simmel-shapes&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="heider-simmel-shapes-caption" class="caption-frame">
<p>Figure: Fritz Heider and Marianne Simmel’s video of shapes from <span
class="citation" data-cites="Heider-experimental44">Heider and Simmel
(1944)</span>.</p>
</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Fritz_Heider">Fritz Heider</a>
and <a href="https://en.wikipedia.org/wiki/Marianne_Simmel">Marianne
Simmel</a>’s experiments with animated shapes from 1944 <span
class="citation" data-cites="Heider-experimental44">(Heider and Simmel,
1944)</span>. Our interpretation of these objects as showing motives and
even emotion is a combination of our desire for narrative, a need for
understanding of each other, and our ability to empathize. At one level,
these are crudely drawn objects, but in another way, the animator has
communicated a story through simple facets such as their relative
motions, their sizes and their actions. We apply our psychological
representations to these faceless shapes to interpret their actions.</p>
<p>See also a recent review paper on Human Cooperation by <span
class="citation" data-cites="Joseph-origins21">Henrich and Muthukrishna
(2021)</span>.</p>
<!-- Conversation LLM -->
<h2 id="computer-conversations">Computer Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-computer.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-computer.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-computer-conversation-6-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation006.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-computer-conversation-6-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-computer-conversation-6&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-computer-conversation-6-caption" class="caption-frame">
<p>Figure: Conversation relies on internal models of other
individuals.</p>
</div>
</div>
<div class="figure">
<div id="anne-computer-conversation-8-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation007.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-computer-conversation-8-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-computer-conversation-8&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-computer-conversation-8-caption" class="caption-frame">
<p>Figure: Misunderstanding of context and who we are talking to leads
to arguments.</p>
</div>
</div>
<p>Similarly, we find it difficult to comprehend how computers are
making decisions. Because they do so with more data than we can possibly
imagine.</p>
<p>In many respects, this is not a problem, it’s a good thing. Computers
and us are good at different things. But when we interact with a
computer, when it acts in a different way to us, we need to remember
why.</p>
<p>Just as the first step to getting along with other humans is
understanding other humans, so it needs to be with getting along with
our computers.</p>
<p>Embodiment factors explain why, at the same time, computers are so
impressive in simulating our weather, but so poor at predicting our
moods. Our complexity is greater than that of our weather, and each of
us is tuned to read and respond to one another.</p>
<p>Their intelligence is different. It is based on very large quantities
of data that we cannot absorb. Our computers don’t have a complex
internal model of who we are. They don’t understand the human condition.
They are not tuned to respond to us as we are to each other.</p>
<p>Embodiment factors encapsulate a profound thing about the nature of
humans. Our locked in intelligence means that we are striving to
communicate, so we put a lot of thought into what we’re communicating
with. And if we’re communicating with something complex, we naturally
anthropomorphize them.</p>
<p>We give our dogs, our cats, and our cars human motivations. We do the
same with our computers. We anthropomorphize them. We assume that they
have the same objectives as us and the same constraints. They don’t.</p>
<p>This means, that when we worry about artificial intelligence, we
worry about the wrong things. We fear computers that behave like more
powerful versions of ourselves that will struggle to outcompete us.</p>
<p>In reality, the challenge is that our computers cannot be human
enough. They cannot understand us with the depth we understand one
another. They drop below our cognitive radar and operate outside our
mental models.</p>
<p>The real danger is that computers don’t anthropomorphize. They’ll
make decisions in isolation from us without our supervision because they
can’t communicate truly and deeply with us.</p>
<h2 id="probability-conversations">Probability Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-probability-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-probability-conversation.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-probability-conversation-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-probability-conversation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-probability-conversation-caption" class="caption-frame">
<p>Figure: The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.</p>
</div>
</div>
<p>In practice we know that probabilities can be very unintuitive, for
example in court there is a fallacy known as the “prosecutor’s fallacy”
that confuses conditional probabilities. This can cause problems in jury
trials <span class="citation" data-cites="Thompson-juries89">(Thompson,
1989)</span>.</p>
<h2 id="networked-interactions">Networked Interactions</h2>
<p>Our modern society intertwines the machine with human interactions.
The key question is who has control over these interfaces between humans
and machines.</p>
<div class="figure">
<div id="human-computers-interacting-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-computers-interacting.svg" width="80%" style=" ">
</object>
</div>
<div id="human-computers-interacting-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-computers-interacting&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-computers-interacting-caption" class="caption-frame">
<p>Figure: Humans and computers interacting should be a major focus of
our research and engineering efforts.</p>
</div>
</div>
<p>So the real challenge that we face for society is understanding which
systemic interventions will encourage the right interactions between the
humans and the machine at all of these interfaces.</p>
<div class="figure">
<div id="human-culture-interacting-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-culture-interacting.svg" width="80%" style=" ">
</object>
</div>
<div id="human-culture-interacting-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-culture-interacting&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-culture-interacting-caption" class="caption-frame">
<p>Figure: Humans use culture, facts and ‘artefacts’ to communicate.</p>
</div>
</div>
<h2 id="the-blue-marble">The Blue Marble</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/the-earth-seen-from-apollo-17.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/the-earth-seen-from-apollo-17.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-blue-marble-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/the-earth-seen-from-apollo-17.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-blue-marble-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-blue-marble&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-blue-marble-caption" class="caption-frame">
<p>Figure: The Blue Marble, a photo of Earth taken from Apollo 17.</p>
</div>
</div>
<h2 id="eagle-from-columbia">Eagle from Columbia</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/eagle-from-columbia.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/eagle-from-columbia.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="eagle-from-columbia-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/2131px-Earth,_Moon_and_Lunar_Module,_AS11-44-6643.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="eagle-from-columbia-magnify" class="magnify"
onclick="magnifyFigure(&#39;eagle-from-columbia&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eagle-from-columbia-caption" class="caption-frame">
<p>Figure: Eagle photographed from Columbia on its return from the Lunar
surface.</p>
</div>
</div>
<h2 id="amelia-earhart">Amelia Earhart</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/little-red-bus.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/little-red-bus.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-little-red-bus-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/the-little-red-bus.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-little-red-bus-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-little-red-bus&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-little-red-bus-caption" class="caption-frame">
<p>Figure: The Little Red Bus, Amelia Earhart’s plane in Derry after
landing.</p>
</div>
</div>
<h2 id="naca-langley">NACA Langley</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/naca-proving.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/naca-proving.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="naca-lmal-42612-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/NACA-LMAL-42612.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="naca-lmal-42612-magnify" class="magnify"
onclick="magnifyFigure(&#39;naca-lmal-42612&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="naca-lmal-42612-caption" class="caption-frame">
<p>Figure: 1945 photo of the NACA test pilots, from left Mel Gough, Herb
Hoover, Jack Reeder, Stefan Cavallo and Bill Gray (photo NASA, NACA LMAL
42612)</p>
</div>
</div>
<p>The NACA Langley Field proving ground tested US aircraft. Bob Gilruth
worked on the <a
href="https://ntrs.nasa.gov/search.jsp?R=19930091834">flying qualities
of aircraft</a>. One of his collaborators suggested that</p>
<blockquote>
<p>Hawker Hurricane airplane. A heavily armed fighter airplane noted for
its role in the Battle of Britain, the Hurricane’s flying qualities were
found to be generally satisfactory. The most notable deficiencies were
heavy aileron forces at high speeds and large friction in the
controls.</p>
<p>W. Hewitt Phillips<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a></p>
</blockquote>
<p>and</p>
<blockquote>
<p>Supermarine Spitfire airplane. A high-performance fighter noted for
its role in the Battle of Britain and throughout WW II, the Spitfire had
desirably light elevator control forces in maneuvers and near neutral
longitudinal stability. Its greatest deficiency from the combat
standpoint was heavy aileron forces and sluggish roll response at high
speeds.</p>
<p>W. Hewitt Phillips<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a></p>
</blockquote>
<p>Gilruth went beyond the reports of feel to characterise how the plane
should respond to different inputs on the control stick. In other words
he quantified that feel of the plane.</p>
<h2 id="the-moniac">The MONIAC</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><a href="https://en.wikipedia.org/wiki/MONIAC">The MONIAC</a> was an
analogue computer designed to simulate the UK economy. Analogue
comptuers work through analogy, the analogy in the MONIAC is that both
money and water flow. The MONIAC exploits this through a system of
tanks, pipes, valves and floats that represent the flow of money through
the UK economy. Water flowed from the treasury tank at the top of the
model to other tanks representing government spending, such as health
and education. The machine was initially designed for teaching support
but was also found to be a useful economic simulator. Several were built
and today you can see the original at Leeds Business School, there is
also one in the London Science Museum and one <a
href="https://www.econ.cam.ac.uk/economics-alumni/drip-down-economics-phillips-machine">in
the Unisversity of Cambridge’s economics faculty</a>.</p>
<div class="figure">
<div id="the-moniac-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-moniac-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-moniac&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-moniac-caption" class="caption-frame">
<p>Figure: Bill Phillips and his MONIAC (completed in 1949). The machine
is an analogue computer designed to simulate the workings of the UK
economy.</p>
</div>
</div>
<h2 id="donald-mackay">Donald MacKay</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/donald-mackay-brain.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/donald-mackay-brain.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="donald-maccrimmon-mackay-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//people/DonaldMacKay1952.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="donald-maccrimmon-mackay-magnify" class="magnify"
onclick="magnifyFigure(&#39;donald-maccrimmon-mackay&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="donald-maccrimmon-mackay-caption" class="caption-frame">
<p>Figure: Donald M. MacKay (1922-1987), a physicist who was an early
member of the cybernetics community and member of the Ratio Club.</p>
</div>
</div>
<p>Donald MacKay was a physicist who worked on naval gun targetting
during the second world war. The challenge with gun targetting for ships
is that both the target and the gun platform are moving. The challenge
was tackled using analogue computers, for example in the US the <a
href="https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer">Mark I
fire control computer</a> which was a mechanical computer. MacKay worked
on radar systems for gun laying, here the velocity and distance of the
target could be assessed through radar and an mechanical electrical
analogue computer.</p>
<h2 id="fire-control-systems">Fire Control Systems</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/fire-control-systems.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/fire-control-systems.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Naval gunnery systems deal with targeting guns while taking into
account movement of ships. The Royal Navy’s Gunnery Pocket Book <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span> gives details of one system for gun laying.</p>
<p>Like many challenges we face today, in the second world war, fire
control was handled by a hybrid system of humans and computers. This
means deploying human beings for the tasks that they can manage, and
machines for the tasks that are better performed by a machine. This
leads to a division of labour between the machine and the human that can
still be found in our modern digital ecosystems.</p>
<div class="figure">
<div id="low-angle-fire-control-team-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/low-angle-fire-control-team.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="low-angle-fire-control-team-magnify" class="magnify"
onclick="magnifyFigure(&#39;low-angle-fire-control-team&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="low-angle-fire-control-team-caption" class="caption-frame">
<p>Figure: The fire control computer set at the centre of a system of
observation and tracking <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.</p>
</div>
</div>
<p>As analogue computers, fire control computers from the second world
war would contain components that directly represented the different
variables that were important in the problem to be solved, such as the
inclination between two ships.</p>
<div class="figure">
<div id="the-measurement-of-inclination-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-measurement-of-inclination.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-measurement-of-inclination-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-measurement-of-inclination&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-measurement-of-inclination-caption" class="caption-frame">
<p>Figure: Measuring inclination between two ships <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span>. Sophisticated fire control computers allowed the ship to
continue to fire while under maneuvers.</p>
</div>
</div>
<p>The fire control systems were electro-mechanical analogue computers
that represented the “state variables” of interest, such as inclination
and ship speed with gears and cams within the machine.</p>
<div class="figure">
<div id="typical-modern-fire-control-table-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/typical-modern-fire-control-table.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="typical-modern-fire-control-table-magnify" class="magnify"
onclick="magnifyFigure(&#39;typical-modern-fire-control-table&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="typical-modern-fire-control-table-caption"
class="caption-frame">
<p>Figure: A second world war gun computer’s control table <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span>.</p>
</div>
</div>
<p>For more details on fire control computers, you can watch a 1953 film
on the the US the <a
href="https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer">Mark
IA fire control computer</a> from Periscope Film.</p>
<div class="figure">
<div id="us-navy-training-film-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/gwf5mAlI7Ug?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="us-navy-training-film-magnify" class="magnify"
onclick="magnifyFigure(&#39;us-navy-training-film&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="us-navy-training-film-caption" class="caption-frame">
<p>Figure: U.S. Navy training film MN-6783a. Basic Mechanisms of Fire
Control Computers. Mechanical Computer Instructional Film 27794 (1953)
for the Mk 1A Fire Control Computer.</p>
</div>
</div>
<h2 id="behind-the-eye">Behind the Eye</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/behind-the-eye.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/behind-the-eye.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="behind-the-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/behind-the-eye.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="behind-the-eye-magnify" class="magnify"
onclick="magnifyFigure(&#39;behind-the-eye&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="behind-the-eye-caption" class="caption-frame">
<p>Figure: <a
href="https://www.amazon.co.uk/Behind-Eye-Gifford-Lectures-MACKAY/dp/0631173323">Behind
the Eye</a> <span class="citation" data-cites="Mackay-behind91">(MacKay,
1991)</span> summarises MacKay’s Gifford Lectures, where MacKay uses the
operation of the eye as a window on the operation of the brain.</p>
</div>
</div>
<p>Donald MacKay was at King’s College for his PhD. He was just down the
road from Bill Phillips at LSE who was building the MONIAC. He was part
of the Ratio Club. A group of early career scientists who were
interested in communication and control in animals and humans, or more
specifically they were interested in computers and brains. The were part
of an international movement known as cybernetics.</p>
<p>Donald MacKay wrote of the influence that his own work on radar had
on his interest in the brain.</p>
<blockquote>
<p>… during the war I had worked on the theory of automated and
electronic computing and on the theory of information, all of which are
highly relevant to such things as automatic pilots and automatic gun
direction. I found myself grappling with problems in the design of
artificial sense organs for naval gun-directors and with the principles
on which electronic circuits could be used to simulate situations in the
external world so as to provide goal-directed guidance for ships,
aircraft, missiles and the like.</p>
</blockquote>
<blockquote>
<p>Later in the 1940’s, when I was doing my Ph.D. work, there was much
talk of the brain as a computer and of the early digital computers that
were just making the headlines as “electronic brains.” As an analogue
computer man I felt strongly convinced that the brain, whatever it was,
was not a digital computer. I didn’t think it was an analogue computer
either in the conventional sense.</p>
</blockquote>
<blockquote>
<p>But this naturally rubbed under my skin the question: well, if it is
not either of these, what kind of system is it? Is there any way of
following through the kind of analysis that is appropriate to their
artificial automata so as to understand better the kind of system the
human brain is? That was the beginning of my slippery slope into brain
research.</p>
<p><em>Behind the Eye</em> pg 40. Edited version of the 1986 Gifford
Lectures given by Donald M. MacKay and edited by Valerie MacKay</p>
</blockquote>
<p>Importantly, MacKay distinguishes between the <em>analogue</em>
computer and the <em>digital</em> computer. As he mentions, his
experience was with analogue machines. An analogue machine is
<em>literally</em> an analogue. The radar systems that Wiener and MacKay
both worked on were made up of electronic components such as resistors,
capacitors, inductors and/or mechanical components such as cams and
gears. Together these components could represent a physical system, such
as an anti-aircraft gun and a plane. The design of the analogue computer
required the engineer to simulate the real world in analogue
electronics, using dualities that exist between e.g. mechanical circuits
(mass, spring, damper) and electronic circuits (inductor, resistor,
capacitor). The analogy between mass and a damper, between spring and a
resistor and between capacitor and a damper works because the underlying
mathematics is approximated with the same linear system: a second order
differential equation. This mathematical analogy allowed the designer to
map from the real world, through mathematics, to a virtual world where
the components reflected the real world through analogy.</p>
<h2 id="human-analogue-machine">Human Analogue Machine</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/human-analogue-machines-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/human-analogue-machines-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The machine learning systems we have built today that can reconstruct
human text, or human classification of images, necessarily must have
some aspects to them that are analagous to our understanding. As MacKay
suggests the brain is neither a digital or an analogue computer, and the
same can be said of the modern neural network systems that are being
tagged as “artificial intelligence”.</p>
<p>I believe a better term for them is “human-analogue machines”,
because what we have built is not a system that can make intelligent
decisions from first principles (a rational approach) but one that
observes how humans have made decisions through our data and
reconstructs that process. Machine learning is more empiricist than
rational, but now we n empirical approach that distils our evolved
intelligence.</p>
<div class="figure">
<div id="human-analogue-machine-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/human-analogue-machine.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="human-analogue-machine-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-analogue-machine&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-analogue-machine-caption" class="caption-frame">
<p>Figure: The human analogue machine creates a feature space which is
analagous to that we use to reason, one way of doing this is to have a
machine attempt to compress all human generated text in an
auto-regressive manner.</p>
</div>
</div>
<p>The perils of developing this capability include counterfeit people,
a notion that the philosopher <a
href="https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/">Daniel
Dennett has described in <em>The Atlantic</em></a>. This is where
computers can represent themselves as human and fool people into doing
things on that basis.</p>
<h2 id="llm-conversations">LLM Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-llm.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-llm.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-llm-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-llm-conversation.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-llm-conversation-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-llm-conversation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-llm-conversation-caption" class="caption-frame">
<p>Figure: The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.</p>
</div>
</div>
<div class="figure">
<div id="ai-for-data-analytics-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/0sJjdxn5kcI?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="ai-for-data-analytics-magnify" class="magnify"
onclick="magnifyFigure(&#39;ai-for-data-analytics&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ai-for-data-analytics-caption" class="caption-frame">
<p>Figure: The Inner Monologue paper suggests using LLMs for robotic
planning <span class="citation" data-cites="Huang-inner22">(Huang et
al., 2023)</span>.</p>
</div>
</div>
<p>By interacting directly with machines that have an understanding of
human cultural context, it should be possible to share the nature of
uncertainty in the same way humans do. See for example the paper <a
href="https://innermonologue.github.io/">Inner Monologue: Embodied
Reasoning through Planning</a> <span class="citation"
data-cites="Huang-inner22">Huang et al. (2023)</span>.</p>
<h2 id="intellectual-debt">Intellectual Debt</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/intellectual-debt-blog-post.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/intellectual-debt-blog-post.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="intellectual-debt-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/2020-02-12-intellectual-debt.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="intellectual-debt-magnify" class="magnify"
onclick="magnifyFigure(&#39;intellectual-debt&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intellectual-debt-caption" class="caption-frame">
<p>Figure: Jonathan Zittrain’s term to describe the challenges of
explanation that come with AI is Intellectual Debt.</p>
</div>
</div>
<p>In the context of machine learning and complex systems, Jonathan
Zittrain has coined the term <a
href="https://medium.com/berkman-klein-center/from-technical-debt-to-intellectual-debt-in-ai-e05ac56a502c">“Intellectual
Debt”</a> to describe the challenge of understanding what you’ve
created. In <a
href="https://mlatcl.github.io/projects/data-oriented-architectures-for-ai-based-systems.html">the
ML@CL group we’ve been foucssing on developing the notion of a
<em>data-oriented architecture</em></a> to deal with intellectual debt
<span class="citation" data-cites="Cabrera-realworld23">(Cabrera et al.,
2023)</span>.</p>
<p>Zittrain points out the challenge around the lack of interpretability
of individual ML models as the origin of intellectual debt. In machine
learning I refer to work in this area as fairness, interpretability and
transparency or FIT models. To an extent I agree with Zittrain, but if
we understand the context and purpose of the decision making, I believe
this is readily put right by the correct monitoring and retraining
regime around the model. A concept I refer to as “progression testing”.
Indeed, the best teams do this at the moment, and their failure to do it
feels more of a matter of technical debt rather than intellectual,
because arguably it is a maintenance task rather than an explanation
task. After all, we have good statistical tools for interpreting
individual models and decisions when we have the context. We can
linearise around the operating point, we can perform counterfactual
tests on the model. We can build empirical validation sets that explore
fairness or accuracy of the model.</p>
<p>But if we can avoid the pitfalls of counterfeit people, this also
offers us an opportunity to <em>psychologically represent</em> <span
class="citation" data-cites="Heider:interpersonal58">(Heider,
1958)</span> the machine in a manner where humans can communicate
without special training. This in turn offers the opportunity to
overcome the challenge of <em>intellectual debt</em>.</p>
<p>Despite the lack of interpretability of machine learning models, they
allow us access to what the machine is doing in a way that bypasses many
of the traditional techniques developed in statistics. But understanding
this new route for access is a major new challenge.</p>
<h2 id="ham">HAM</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="new-flow-of-information-4-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information004.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-4-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-4-caption" class="caption-frame">
<p>Figure: The trinity of human, data, and computer, and highlights the
modern phenomenon. The communication channel between computer and data
now has an extremely high bandwidth. The channel between human and
computer and the channel between data and human is narrow. New direction
of information flow, information is reaching us mediated by the
computer. The focus on classical statistics reflected the importance of
the direct communication between human and data. The modern challenges
of data science emerge when that relationship is being mediated by the
machine.</p>
</div>
</div>
<div class="figure">
<div id="new-flow-of-information-ham-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information-ham.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-ham-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-ham&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-ham-caption" class="caption-frame">
<p>Figure: The HAM now sits between us and the traditional digital
computer.</p>
</div>
</div>
<!--include{_ai/includes/p-n-fairness.md}-->
<h2 id="a-question-of-trust">A Question of Trust</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/a-question-of-trust.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/a-question-of-trust.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In Baroness Onora O’Neill’s Reeith Lectures from 2002, she raises the
challenge of trust. There are many aspects to her arcuments, but one of
the key points she makes is that we cannot trust without the notion of
duty. O’Neill is bemoaning the substitution of duty with process. The
idea is that processes and transparency are supposed to hold us to
account by measuring outcomes. But these processes themselves overwhelm
decision makers and undermine their professional duty to deliver the
right outcome.</p>
<div class="figure">
<div id="a-question-of-trust-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/a-question-of-trust.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="a-question-of-trust-magnify" class="magnify"
onclick="magnifyFigure(&#39;a-question-of-trust&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="a-question-of-trust-caption" class="caption-frame">
<p>Figure: <a href="https://www.bbc.co.uk/programmes/p00gpzfq">A
Question of Trust by Onora O’Neil</a> which examines the nature of trust
and its role in society.</p>
</div>
</div>
<blockquote>
<p>Again Univesities are to treat each applicant fairly on the basis of
ability and promise, but they are supposed also to admit a socially more
representative intake.</p>
<p>There’s no guarantee that the process meets the target.</p>
<p>Onora O’Neill <em>A Question of Trust: Called to Account</em> Reith
Lectures 2002 <span class="citation" data-cites="ONeill-trust02">O’Neill
(2002)</span>]</p>
</blockquote>
<p>O’Neill is speaking in 2002, in the early days of the internet and
before social media. Much of her thoughts are even more relevant for
today than they were when she spoke. This is because the increased
availability of information and machine driven decision-making makes the
mistaken premise, that process is an adequate substitute for duty, more
apparently plausible. But this undermines what O’Neill calls
“intelligent accountability”, which is not accounting by the numbers,
but through professional education and institutional safeguards.</p>
<table>
<tr>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-0" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-0)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle">policy</text>
</svg>
</center>
</td>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-1" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-1)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle"><tspan x="100" y="90">data</tspan><tspan x="100" y="130">governance</tspan></text>
</svg>
</center>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-2" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-2)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle"><tspan x="100" y="90">accelerate</tspan><tspan x="100" y="130">science</tspan></text>
</svg>
</center>
</td>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-3" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-3)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle">AutoAI</text>
</svg>
</center>
</td>
</tr>
</table>
<p>Innovating to serve science and society requires a pipeline of
interventions. As well as advances in the technical capabilities of AI
technologies, engineering knowhow is required to safely deploy and
monitor those solutions in practice. Regulatory frameworks need to adapt
to ensure trustworthy use of these technologies. Aligning technology
development with public interests demands effective stakeholder
engagement to bring diverse voices and expertise into technology
design.</p>
<p>Building this pipeline will take coordination across research,
engineering, policy and practice. It also requires action to address the
digital divides that influence who benefits from AI advances. These
include digital divides within the socioeconomic strata that need to be
overcome – AI must not exacerbate existing equalities or create new
ones. In addressing these challenges, we can be hindered by divides that
exist between traditional academic disciplines. We need to develop
common understanding of the problems and a shared knowledge of possible
solutions.</p>
<h2 id="making-ai-equitable">Making AI equitable</h2>
<p>AI@Cam is a new flagship University mission that seeks to address
these challenges. It recognises that development of safe and effective
AI-enabled innovations requires this mix of expertise from across
research domains, businesses, policy-makers, civill society, and from
affected communities. AI@Cam is setting out a vision for AI-enabled
innovation that benefits science, citizens and society.</p>
<p>This vision will be achieved through leveraging the University’s
vibrant interdisciplinary research community. AI@Cam will form
partnerships between researchers, practitioners, and affected
communities that embed equity and inclusion. It will develop new
platforms for innovation and knowledge transfer. It will deliver
innovative interdisciplinary teaching and learning for students,
researchers, and professionals. It will build strong connections between
the University and national AI priorities.</p>
<p>The University operates as both an engine of AI-enabled innovation
and steward of those innovations.</p>
<p>AI is not a universal remedy. It is a set of tools, techniques and
practices that correctly deployed can be leveraged to deliver societal
benefit and mitigate social harm.</p>
<p>In that sense AI@Cam’s mission is close in spirit to that of
Panacea’s elder sister Hygeia. It is focussed on building and
maintaining the hygiene of a robust and equitable AI research
ecosystem.</p>
<h2 id="richard-feynmann-on-doubt">Richard Feynmann on Doubt</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/richard-feynmann-doubt.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/richard-feynmann-doubt.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<blockquote>
<p>One thing is I can live with is doubt, and uncertainty and not
knowing. I think it’s much more interesting to live with not knowing
than to have an answer that might be wrong.</p>
<p>Richard P. Feynmann in the <em>The Pleasure of Finding Things
Out</em> 1981.</p>
</blockquote>
<h2 id="artificial-intelligence">Artificial Intelligence</h2>
<p>One of the struggles of artificial intelligence is that the term
means different things to different people. Our intelligence is precious
to us, and the notion that it can be easily recreated is disturbing to
us. This leads to some dystopian notions of artificial intelligence,
such as the singularity.</p>
<p>Depending on whether this powerful technology is viewed as beneficent
or maleficent, it can be viewed either as a helpful assistant, in the
manner of Jeeves, or a tyrannical dictator.</p>
<!-- AI Fallacy -->
<p>The history of automation and technology is a history of us adapting
to technological change. The invention of the railways, and the need for
consistent national times to timetable our movements. The development of
the factory system in the mills of Derbyshire required workers to
operate and maintain the machines that replaced them.</p>
<p>Listening to modern to conversations about artificial intelligence, I
think the use of the term <em>intelligence</em> has given rise to an
idea that this technology will be the But amoung these different
assessments of artificial intelligence is buried an idea, one that will
be the first technology to adapt to us.</p>
<h1 id="the-structure-of-scientific-revolutions">The Structure of
Scientific Revolutions</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/the-structure-of-scientific-revolutions.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-structure-of-scientific-revolutions.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-structure-of-scientific-revolutions-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/the-structure-of-scientific-revolutionss.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-structure-of-scientific-revolutions-magnify"
class="magnify"
onclick="magnifyFigure(&#39;the-structure-of-scientific-revolutions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-structure-of-scientific-revolutions-caption"
class="caption-frame">
<p>Figure: <a
href="https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions">The
Structure of Scientific Revolutions by Thomas S. Kuhn</a> suggests
scientific paradigms are recorded in books.</p>
</div>
</div>
<h2 id="blakes-newton">Blake’s Newton</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_art/includes/blake-newton.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_art/includes/blake-newton.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="blake-newton-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/blake-newton.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="blake-newton-magnify" class="magnify"
onclick="magnifyFigure(&#39;blake-newton&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="blake-newton-caption" class="caption-frame">
<p>Figure: William Blake’s <em>Newton</em>.</p>
</div>
</div>
<h2 id="lunette-rehoboam-abijah">Lunette Rehoboam Abijah</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-lunette-rehoboam-abijah.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-lunette-rehoboam-abijah.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="lunette-rehoboam-abijah-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-lunette-rehoboam-abijah.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="lunette-rehoboam-abijah-magnify" class="magnify"
onclick="magnifyFigure(&#39;lunette-rehoboam-abijah&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lunette-rehoboam-abijah-caption" class="caption-frame">
<p>Figure: Lunette containing Rehoboam and Abijah.</p>
</div>
</div>
<div class="figure">
<div id="people-culture-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/people-culture.svg" width="80%" style=" ">
</object>
</div>
<div id="people-culture-magnify" class="magnify"
onclick="magnifyFigure(&#39;people-culture&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="people-culture-caption" class="caption-frame">
<p>Figure: People communicate through artifacts and culture.</p>
</div>
</div>
<div class="figure">
<div id="human-analogue-machine-2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor-ham.svg" width="40%" style=" ">
</object>
</div>
<div id="human-analogue-machine-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-analogue-machine-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-analogue-machine-2-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="elohim-creating-adam">Elohim Creating Adam</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_art/includes/blake-elohim-creating-adam.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_art/includes/blake-elohim-creating-adam.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="elohim-creating-adam-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/blake-elohim-creating-adam.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="elohim-creating-adam-magnify" class="magnify"
onclick="magnifyFigure(&#39;elohim-creating-adam&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="elohim-creating-adam-caption" class="caption-frame">
<p>Figure: William Blake’s <em>Elohim Creating Adam</em>.</p>
</div>
</div>
<h2 id="fall-and-expulsion-from-garden-of-eden">Fall and Expulsion from
Garden of Eden</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-fall-and-expulsion-from-garden-of-eden.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_art/includes/michelangelo-fall-and-expulsion-from-garden-of-eden.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="michelangelo-fall-and-expulsion-from-garden-of-eden-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-fall-and-expulsion-from-the-garden-of-eden.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="michelangelo-fall-and-expulsion-from-garden-of-eden-magnify"
class="magnify"
onclick="magnifyFigure(&#39;michelangelo-fall-and-expulsion-from-garden-of-eden&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="michelangelo-fall-and-expulsion-from-garden-of-eden-caption"
class="caption-frame">
<p>Figure: Photo of detail of the fall and expulsion from the Garden of
Eden.</p>
</div>
</div>
<div class="figure">
<div id="bandwidth-vs-complexity-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/bandwidth-vs-complexity.svg" width="80%" style=" ">
</object>
</div>
<div id="bandwidth-vs-complexity-magnify" class="magnify"
onclick="magnifyFigure(&#39;bandwidth-vs-complexity&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="bandwidth-vs-complexity-caption" class="caption-frame">
<p>Figure: Bandwidth vs Complexity.</p>
</div>
</div>
<!-- Conversation LLM -->
<!--




## Complexity in Action

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>



As an exercise in understanding complexity, watch the following video. You will see the basketball being bounced around, and the players moving. Your job is to count the passes of those dressed in white and ignore those of the individuals dressed in black.

<div class="figure">
<div class="figure-frame" id="monkey-business-figure">
<iframe width="600" height="450" src="https://www.youtube.com/embed/vJG698U2Mvo?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<div class="magnify" id="monkey-business-magnify" onclick="magnifyFigure('monkey-business')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="monkey-business-caption">
Figure: Daniel Simon's famous illusion "monkey business". Focus on the movement of the ball distracts the viewer from seeing other aspects of the image.
</div>
</div>

In a classic study @Simons-gorillas99 ask subjects to count the number of passes of the basketball between players on the team wearing white shirts. Fifty percent of the time, these subjects don't notice the gorilla moving across the scene.

The phenomenon of inattentional blindness is well known, e.g in their paper Simons and Charbris quote the Hungarian neurologist, Rezsö Bálint,

> It is a well-known phenomenon that we do not notice anything happening in our surroundings while being absorbed in the inspection of something; focusing our attention on a certain object may happen to such an extent that we cannot perceive other objects placed in the peripheral parts of our visual field, although the light rays they emit arrive completely at the visual sphere of the cerebral cortex.
>
> Rezsö Bálint 1907 (translated in Husain and Stein 1988, page 91)


When we combine the complexity of the world with our relatively low bandwidth for information, problems can arise. Our focus on what we perceive to be the most important problem can cause us to miss other (potentially vital) contextual information.

This phenomenon is known as selective attention or 'inattentional blindness'.

<div class="figure">
<div class="figure-frame" id="daniel-simons-monkey-business-figure">
<iframe width="600" height="450" src="https://www.youtube.com/embed/_oGAzq5wM_Q?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<div class="magnify" id="daniel-simons-monkey-business-magnify" onclick="magnifyFigure('daniel-simons-monkey-business')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="daniel-simons-monkey-business-caption">
Figure: For a longer talk on inattentional bias from Daniel Simons see this video.
</div>
</div>


-->
<!--include{_data-science/includes/data-selection-attention-bias.md}-->
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
<!-- Lecture 2 -->
<!--
Time scales, how when you expand or contract time signal becomes noise and noise becomes signal illustrate with Dirac delta and and stochastic processes in Fourier space, ito calculus. Latent force models.

Practical examples of what happens understochasticity:

0) Derive U = W + TS?? Go from microscopic to macroscopic. 

1) Kappenball --- world in between where interesting things happen,

2) Queue efficiency (M/M/1  1/(1-\rho))

3) Input to the system being in the form of bias and variance (or perhaps Brownian motion, wiener process)

(Latent force models being driven by this???? Latent force as high frequency information processing? Environment as slow?-->
<h2 id="laplaces-demon">Laplace’s Demon</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplaces-demon.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplaces-demon.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=1YQPAAAAQAAJ&amp;pg=PR17-IA2&amp;output=embed" width="700" height="500">
</iframe>
<div class="figure">
<div id="laplaces-demon-cropped-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/philosophicaless00lapliala_16_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="laplaces-demon-cropped-magnify" class="magnify"
onclick="magnifyFigure(&#39;laplaces-demon-cropped&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="laplaces-demon-cropped-caption" class="caption-frame">
<p>Figure: English translation of Laplace’s demon, taken from the
Philosophical Essay on probabilities <span class="citation"
data-cites="Laplace-essai14">Laplace (1814)</span> pg 3.</p>
</div>
</div>
<p>One way of viewing what Laplace is saying is that we can take “the
forces by which nature is animated” or our best
mathematical/computational abstraction of that which we would call the
<em>model</em> and combine it with the “respective situation of the
beings who compose it” which I would refer to as the <em>data</em> and
if we have an “intelligence sufficiently vast enough to submit these
data to analysis”, or sufficient <em>compute</em> then we would have a
system for which “nothing would be uncertain and the future, as the
past, would be present in its eyes”, or in other words we can make a
<em>prediction</em>. Or more succinctly put we have</p>
<center>
<span class="math display">\[
\text{model} + \text{data} \stackrel{\text{compute}}{\rightarrow}
\text{prediction}.\]</span>
</center>
<p>Laplace’s demon has been a recurring theme in science, we can also
find it in Stephen Hawking’s book <em>A Brief History of Time</em> <span
class="citation" data-cites="Hawking-history88">(<em>A brief history of
time</em>, 1988)</span>.</p>
<blockquote>
<p>If we do discover a theory of everything … it would be the ultimate
triumph of human reason-for then we would truly know the mind of God</p>
<p>Stephen Hawking in <em>A Brief History of Time</em> 1988</p>
</blockquote>
<p>But is it really that simple? Do we just need more and more accurate
models and more and more data?</p>
<h2 id="game-of-life">Game of Life</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/game-of-life.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/game-of-life.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><a href="https://en.wikipedia.org/wiki/John_Horton_Conway">John
Horton Conway</a> was a mathematician who developed a game known as the
Game of Life. He died in April 2020, but since he invented the game, he
was in effect ‘god’ for this game. But as we will see, just inventing
the rules doesn’t give you omniscience in the game.</p>
<p>The Game of Life is played on a grid of squares, or pixels. Each
pixel is either on or off. The game has no players, but a set of simple
rules that are followed at each turn the rules are.</p>
<h2 id="life-rules">Life Rules</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-rules.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-rules.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>John Conway’s game of life is a cellular automaton where the cells
obey three very simple rules. The cells live on a rectangular grid, so
that each cell has 8 possible neighbors.</p>
<div class="figure">
<div id="life-rules-loneliness-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>loneliness</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="life-rules-loneliness-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-rules-loneliness&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-rules-loneliness-caption" class="caption-frame">
<p>Figure: ‘Death’ through loneliness in Conway’s game of life. If a
cell is surrounded by less than three cells, it ‘dies’ through
loneliness.</p>
</div>
</div>
<p>The game proceeds in turns, and at each location in the grid is
either alive or dead. Each turn, a cell counts its neighbors. If there
are two or fewer neighbors, the cell ‘dies’ of ‘loneliness’.</p>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>overcrowding</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="life-rules-crowding-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-rules-crowding&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-rules-crowding-caption" class="caption-frame">
<p>Figure: ‘Death’ through overpopulation in Conway’s game of life. If a
cell is surrounded by more than three cells, it ‘dies’ through
loneliness.</p>
</div>
</div>
<p>If there are four or more neighbors, the cell ‘dies’ from
‘overcrowding’. If there are three neighbors, the cell persists, or if
it is currently dead, a new cell is born.</p>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>birth</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="life-rules-crowding-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-rules-crowding&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-rules-crowding-caption" class="caption-frame">
<p>Figure: Birth in Conway’s life. Any position surrounded by precisely
three live cells will give birth to a new cell at the next turn.</p>
</div>
</div>
<p>And that’s it. Those are the simple ‘physical laws’ for Conway’s
game.</p>
<p>The game leads to patterns emerging, some of these patterns are
static, but some oscillate in place, with varying periods. Others
oscillate, but when they complete their cycle they’ve translated to a
new location, in other words they move. In Life the former are known as
<a href="https://conwaylife.com/wiki/Oscillator">oscillators</a> and the
latter as <a
href="https://conwaylife.com/wiki/Spaceship">spaceships</a>.</p>
<h2 id="loafers-and-gliders">Loafers and Gliders</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-glider-loafer-conway.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-glider-loafer-conway.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>John Horton Conway, as the creator of the game of life, could be seen
somehow as the god of this small universe. He created the rules. The
rules are so simple that in many senses he, and we, are all-knowing in
this space. But despite our knowledge, this world can still ‘surprise’
us. From the simple rules, emergent patterns of behaviour arise. These
include static patterns that don’t change from one turn to the next.
They also include, oscillators, that pulse between different forms
across different periods of time. A particular form of oscillator is
known as a ‘spaceship’, this is one that moves across the board as the
game evolves. One of the simplest and earliest spaceships to be
discovered is known as the glider.</p>
<div class="figure">
<div id="glider-loafer-conway-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Glider (1969)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Glider.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="glider-loafer-conway-magnify" class="magnify"
onclick="magnifyFigure(&#39;glider-loafer-conway&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="glider-loafer-conway-caption" class="caption-frame">
<p>Figure: <em>Left</em> A Glider pattern discovered 1969 by Richard K.
Guy. <em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020). The glider is an oscillator that moves diagonally after
creation. From the simple rules of Life it’s not obvious that such an
object does exist, until you do the necessary computation.</p>
</div>
</div>
<p>The glider was ‘discovered’ in 1969 by Richard K. Guy. What do we
mean by discovered in this context? Well, as soon as the game of life is
defined, objects such as the glider do somehow exist, but the many
configurations of the game mean that it takes some time for us to see
one and know it exists. This means, that despite being the creator,
Conway, and despite the rules of the game being simple, and despite the
rules being deterministic, we are not ‘omniscient’ in any simplistic
sense. It requires computation to ‘discover’ what can exist in this
universe once it’s been defined.</p>
<div class="figure">
<div id="gosper-glider-gun-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Gosperglidergun.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gosper-glider-gun-magnify" class="magnify"
onclick="magnifyFigure(&#39;gosper-glider-gun&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gosper-glider-gun-caption" class="caption-frame">
<p>Figure: The Gosper glider gun is a configuration that creates
gliders. A new glider is released after every 30 turns.</p>
</div>
</div>
<p>These patterns had to be discovered, in the same way that a scientist
might discover a disease, or an explorer a new land. For example, the
Gosper glider gun was <a
href="https://conwaylife.com/wiki/Bill_Gosper">discovered by Bill Gosper
in 1970</a>. It is a pattern that creates a new glider every 30 turns of
the game.</p>
<p>Despite widespread interest in Life, some of its patterns were only
very recently discovered like the Loafer, discovered in 2013 by Josh
Ball. So, despite the game having existed for over forty years, and the
rules of the game being simple, there are emergent behaviors that are
unknown.</p>
<div class="figure">
<div id="the-loafer-spaceship-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Loafer (2013)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Loafer.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="the-loafer-spaceship-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-loafer-spaceship&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-loafer-spaceship-caption" class="caption-frame">
<p>Figure: <em>Left</em> A Loafer pattern discovered by Josh Ball in
2013. <em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020).</p>
</div>
</div>
<p>Once these patterns are discovered, they are combined (or engineered)
to create new Life patterns that do some remarkable things. For example,
there’s a life pattern that runs a Turing machine, or more remarkably
there’s a Life pattern that runs Life itself.</p>
<div class="figure">
<div id="life-in-life-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-in-life.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="life-in-life-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-in-life&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-in-life-caption" class="caption-frame">
<p>Figure: The Game of Life running in Life. The video is drawing out
recursively showing pixels that are being formed by filling cells with
moving spaceships. Each individual pixel in this game of life is made up
of <span class="math inline">\(2048 \times 2048\)</span> pixels called
an <a href="https://www.conwaylife.com/wiki/OTCA_metapixel">OTCA
metapixel</a>.</p>
</div>
</div>
<p>To find out more about the Game of Life you can watch this video by
Alan Zucconi or read his <a
href="https://www.alanzucconi.com/2020/10/13/conways-game-of-life/">associated
blog post</a>.</p>
<div class="figure">
<div id="intro-to-life-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/Kk2MH9O4pXY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="intro-to-life-magnify" class="magnify"
onclick="magnifyFigure(&#39;intro-to-life&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intro-to-life-caption" class="caption-frame">
<p>Figure: An introduction to the Game of Life by Alan Zucconi.</p>
</div>
</div>
<p>Contrast this with our situation where in ‘real life’ we don’t know
the simple rules of the game, the state space is larger, and emergent
behaviors (hurricanes, earthquakes, volcanos, climate change) have
direct consequences for our daily lives, and we understand why the
process of ‘understanding’ the physical world is so difficult. We also
see immediately how much easier we might expect the physical sciences to
be than the social sciences, where the emergent behaviors are contingent
on highly complex human interactions.</p>
<h2 id="emergent-behaviour">Emergent Behaviour</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/emergent-behaviour.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/emergent-behaviour.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="laplaces-gremlin">Laplace’s Gremlin</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplaces-gremlin.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/laplaces-gremlin.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<blockquote>
<p>The curve described by a simple molecule of air or vapor is regulated
in a manner just as certain as the planetary orbits; the only difference
between them is that which comes from our ignorance. Probability is
relative, in part to this ignorance, in part to our knowledge. We know
that of three or greater number of events a single one ought to occur;
but nothing induces us to believe that one of them will occur rather
than the others. In this state of indecision it is impossible for us to
announce their occurrence with certainty. It is, however, probable that
one of these events, chosen at will, will not occur because we see
several cases equally possible which exclude its occurrence, while only
a single one favors it.</p>
<p>— Pierre-Simon Laplace <span class="citation"
data-cites="Laplace-essai14">(Laplace, 1814)</span>, pg 5</p>
</blockquote>
<p>The representation of ignorance through probability is the true
message of Laplace, I refer to this message as “Laplace’s gremlin”,
because it is the gremlin of uncertainty that interferes with the demon
of determinism to mean that our predictions are not deterministic.</p>
<p>Our separation of the uncertainty into the data, the model and the
computation give us three domains in which our doubts can creep into our
ability to predict. Over the last three lectures we’ve introduced some
of the basic tools we can use to unpick this uncertainty. You’ve been
introduced to, (or have yow reviewed) <em>Bayes’ rule</em>. The rule,
which is a simple consequence of the product rule of probability, is the
foundation of how we update our beliefs in the presence of new
information.</p>
<p>The real point of Laplace’s essay was that we don’t have access to
all the data, we don’t have access to a complete physical understanding,
and as the example of the Game of Life shows, even if we did have access
to both (as we do for “Conway’s universe”) we still don’t have access to
all the compute that we need to make deterministic predictions. There is
uncertainty in the system which means we can’t make precise
predictions.</p>
<p>Gremlins are imaginary creatures used as an explanation of failure in
aircraft, causing crashes. In that sense the Gremlin represents the
uncertainty that a pilot felt about what might go wrong in a plane which
might be “theoretically sound” but in practice is poorly maintained or
exposed to conditions that take it beyond its design criteria. Laplace’s
gremlin is all the things that your model, data and ability to compute
don’t account for bringing about failures in your ability to predict.
Laplace’s gremlin is the uncertainty in the system.</p>
<div class="figure">
<div id="germlins-think-its-fun-to-hurt-you-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/gremlins-think-its-fun-to-hurt-you.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="germlins-think-its-fun-to-hurt-you-magnify" class="magnify"
onclick="magnifyFigure(&#39;germlins-think-its-fun-to-hurt-you&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="germlins-think-its-fun-to-hurt-you-caption"
class="caption-frame">
<p>Figure: Gremlins are seen as the cause of a number of challenges in
this World War II poster.</p>
</div>
</div>
<h2 id="lenox-globe">Lenox Globe</h2>
<div class="figure">
<div id="lenox-globe-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="lenox-globe-magnify" class="magnify"
onclick="magnifyFigure(&#39;lenox-globe&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lenox-globe-caption" class="caption-frame">
<p>Figure: <a
href="http://www.myoldmaps.com/renaissance-maps-1490-1800/314-the-lenox-globe/314-lenox.pdf">The
Lenox globe</a>, which dates from early 16th century, one of the
earliest known globes.</p>
</div>
</div>
<div class="figure">
<div id="lenox-globe-by-b-f-da-costa-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe-by-b-f-da-costa.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="lenox-globe-by-b-f-da-costa-magnify" class="magnify"
onclick="magnifyFigure(&#39;lenox-globe-by-b-f-da-costa&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lenox-globe-by-b-f-da-costa-caption" class="caption-frame">
<p>Figure: Drawing of the Lenox Globe by the historian for the Magazine
of American History in September 1879.</p>
</div>
</div>
<div class="figure">
<div id="lenox-globe-hic-sunt-dracones-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe-hic-sunt-dracones.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="lenox-globe-hic-sunt-dracones-magnify" class="magnify"
onclick="magnifyFigure(&#39;lenox-globe-hic-sunt-dracones&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lenox-globe-hic-sunt-dracones-caption" class="caption-frame">
<p>Figure: Detail from the Lenox globe located in the region of China,
“hic sunt dracones”</p>
</div>
</div>
<h2 id="weather">Weather</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/d-day-weather.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/d-day-weather.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>So what’s going on here? One analogy I like to use is with weather
forecasting. Historically, before the use of computer driven weather
forecasting, we used a process of interpolation to measure the
pressure.</p>
<div class="figure">
<div id="met-office-weather-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/1944-06-05_met-office-weather.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="met-office-weather-magnify" class="magnify"
onclick="magnifyFigure(&#39;met-office-weather&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="met-office-weather-caption" class="caption-frame">
<p>Figure: Forecast from UK Met Office on 5th June 1944. (detail from <a
href="https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day"
class="uri">https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day</a>)</p>
</div>
</div>
<p>This was problematic for German forces in the Second World War
because they had no ability to predict the weather when it was coming in
from across the UK. Conversely, the UK had a number of weather stations
in the UK, and some information (perhaps from spies or Enigma decrypts)
about weather on the mainland.</p>
<div class="figure">
<div id="dwd-weather-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/1944-06-05_dwd-weather.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="dwd-weather-magnify" class="magnify"
onclick="magnifyFigure(&#39;dwd-weather&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="dwd-weather-caption" class="caption-frame">
<p>Figure: Forecast from Deutscher Wetterdienst on 5th June 1944.
(detail from <a
href="https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day"
class="uri">https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day</a>).
Note the lack of measurements within the UK. THis is the direction that
weather was coming from so the locaiton of weather fronts (and
associated storms) was harder for the Deutscher Wetterdienst to predict
than the Met Office.</p>
</div>
</div>
<p>This meant that more accurate forecasts were possible for D-Day for
the Allies than for the defending forces. As a result, on the morning
that Eisenhower invated, Rommel was back in Germany attending his wife’s
50th birthday party.</p>
<p>Modern artificial intelligence solutions are using very large amounts
of data to build a landscape in which this interpolation can take place.
Tools like ChatGPT are allowing us to interpolate between different
human concepts. This is an amazing achievement, but it is also a
challenge.</p>
<!-- thermodynamics -->
<h2 id="boulton-and-watts-lap-engine">Boulton and Watt’s Lap Engine</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/lap-engine.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/lap-engine.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="lap-engine-boulton-watt-figure" class="figure-frame">
<table>
<tr>
<td width="60%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/lap-engine.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Lap Engine (1788)</em>
</center>
</td>
<td width="40%">
<center>
total energy <br> = <br> available energy <br> + <br> temperature <br>
<span class="math inline">\(\times\)</span> <br>entropy
</center>
</td>
</tr>
</table>
</div>
<div id="lap-engine-boulton-watt-magnify" class="magnify"
onclick="magnifyFigure(&#39;lap-engine-boulton-watt&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lap-engine-boulton-watt-caption" class="caption-frame">
<p>Figure: James Watt’s Lap Engine which incorporates many of his
innovations to the steam engine, making it more efficient.</p>
</div>
</div>
<!--THEORY of IGNORANCE-->
<h2 id="theory-of-ignorance">Theory of Ignorance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/theory-of-ignorance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/theory-of-ignorance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="russell-wiener-russell-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="russell-wiener-russell-magnify" class="magnify"
onclick="magnifyFigure(&#39;russell-wiener-russell&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="russell-wiener-russell-caption" class="caption-frame">
<p>Figure: Bertrand Russell (1872-1970), Albert Einstein (1879-1955),
Norbert Wiener, (1894-1964)</p>
</div>
</div>
<div class="figure">
<div id="brownian-motion-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="brownian-motion-magnify" class="magnify"
onclick="magnifyFigure(&#39;brownian-motion&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="brownian-motion-caption" class="caption-frame">
<p>Figure: Brownian motion of a large particle in a group of smaller
particles. The movement is known as a <em>Wiener process</em> after
Norbert Wiener.</p>
</div>
</div>
<div class="figure">
<div id="maxwell-boltzmann-gibbs-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>James Clerk Maxwell</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Ludwig Boltzmann</em>
</center>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<em>Josiah Willard Gibbs</em>
</center>
</td>
</tr>
</table>
</div>
<div id="maxwell-boltzmann-gibbs-magnify" class="magnify"
onclick="magnifyFigure(&#39;maxwell-boltzmann-gibbs&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="maxwell-boltzmann-gibbs-caption" class="caption-frame">
<p>Figure: James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906)
Josiah Willard Gibbs (1839-1903)</p>
</div>
</div>
<div class="figure">
<div id="pooh-rabbit-hoosh-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/E.-H.-Shepard_Two-ink-drawings-from-The-House-at-Pooh-Corner-I_.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="pooh-rabbit-hoosh-magnify" class="magnify"
onclick="magnifyFigure(&#39;pooh-rabbit-hoosh&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pooh-rabbit-hoosh-caption" class="caption-frame">
<p>Figure: Rabbit and Pooh watch the result of Pooh’s hooshing idea to
move Eeyore towards the shore.</p>
</div>
</div>
<blockquote>
<p>When you are a Bear of Very Little Brain, and you Think of Things,
you find sometimes that a Thing which seemed very Thingish inside you is
quite different when it gets out into the open and has other people
looking at it.</p>
<p>A.A. Milne as Winnie-the-Pooh in <em>The House at Pooh Corner</em>,
1928</p>
</blockquote>
<p>This comment from Pooh bear comes just as he’s tried to rescue his
donkey friend, Eeyore, from a river by dropping a large stone on him
from a bridge. Pooh’s idea had been to create a wave to push the donkey
to the shore, a process that Pooh’s rabbit friend calls “hooshing”.</p>
<p>Hooshing is a technique many children will have tried to retrieve a
ball from a river. It can work, so Pooh’s idea wasn’t a bad one, but the
challenge he faced was in its execution. Pooh aimed to the side of
Eeyore, unfortunately the stone fell directly on the stuffed donkey. But
where is Laplace’s demon in hooshing? Just as we can talk about Gliders
and Loafers in Conway’s Game of Life, we talk about stones and donkeys
in our Universe. Pooh’s prediction that he can hoosh the donkey with the
stone is not based on the Theory, it comes from observing the way
objects interact in the actual Universe. Pooh is like the mice in
Douglas Adams’s Earth. He is observing his environment. He looks for
patterns in that environment. Pooh then borrows the computation that the
Universe has already done for us. He has seen similar situations before,
perhaps he once used a stone to hoosh a ball. He is then generalising
from these previous circumstances to suggest that he can also hoosh the
donkey. Despite being a bear of little brain, like the mice on Adams’s
Earth, Pooh can answer questions about his universe by observing the
results of the Theory of Everything playing out around him.</p>
<h2 id="hydrodynamica">Hydrodynamica</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/daniel-bernoulli-hydrodynamica.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/daniel-bernoulli-hydrodynamica.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>When Laplace spoke of the curve of a simple molecule of air, he may
well have been thinking of Daniel Bernoulli (1700-1782). Daniel
Bernoulli was one name in a prodigious family. His father and brother
were both mathematicians. Daniel’s main work was known as
<em>Hydrodynamica</em>.</p>
<div class="figure">
<div id="hydrodynamica-danielis-bernoulli-figure" class="figure-frame">
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=3yRVAAAAcAAJ&amp;pg=PP7&amp;output=embed" width="700" height="500">
</iframe>
</div>
<div id="hydrodynamica-danielis-bernoulli-magnify" class="magnify"
onclick="magnifyFigure(&#39;hydrodynamica-danielis-bernoulli&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="hydrodynamica-danielis-bernoulli-caption"
class="caption-frame">
<p>Figure: Daniel Bernoulli’s <em>Hydrodynamica</em> published in 1738.
It was one of the first works to use the idea of conservation of energy.
It used Newton’s laws to predict the behaviour of gases.</p>
</div>
</div>
<p>Daniel Bernoulli described a kinetic theory of gases, but it wasn’t
until 170 years later when these ideas were verified after Einstein had
proposed a model of Brownian motion which was experimentally verified by
Jean Baptiste Perrin.</p>
<div class="figure">
<div id="-figure" class="figure-frame">
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=3yRVAAAAcAAJ&amp;pg=PA200&amp;output=embed" width="700" height="500">
</iframe>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: Daniel Bernoulli’s chapter on the kinetic theory of gases,
for a review on the context of this chapter see <span class="citation"
data-cites="Mikhailov:hydrodynamica05">Mikhailov (n.d.)</span>. For 1738
this is extraordinary thinking. The notion of kinetic theory of gases
wouldn’t become fully accepted in Physics until 1908 when a model of
Einstein’s was verified by Jean Baptiste Perrin.</p>
</div>
</div>
<h2 id="entropy-billiards">Entropy Billiards</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/entropy-billiards.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/entropy-billiards.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="entropy-billiards-js-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="multiball-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left ">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="multiball-entropy">
</output>
</div>
<div id="multiball-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="multiball-newball" style="text-align:right">
New Ball
</button>
<button id="multiball-pause" style="text-align:right">
Pause
</button>
<button id="multiball-skip" style="text-align:right">
Skip 1000s
</button>
<button id="multiball-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/ballworld.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/multiball.js"></script>
</div>
<div id="entropy-billiards-js-magnify" class="magnify"
onclick="magnifyFigure(&#39;entropy-billiards-js&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="entropy-billiards-js-caption" class="caption-frame">
<p>Figure: Bernoulli’s simple kinetic models of gases assume that the
molecules of air operate like billiard balls.</p>
</div>
</div>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.random.randn(<span class="dv">10000</span>, <span class="dv">1</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> [<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">200</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)<span class="op">*</span>np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">*</span>x)</span></code></pre></div>
<p>Another important figure for Cambridge was the first to derive the
probability distribution that results from small balls banging together
in this manner. In doing so, James Clerk Maxwell founded the field of
statistical physics.</p>
<div class="figure">
<div id="gaussian-histogram-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ml/gaussian-histogram.svg" width="80%" style=" ">
</object>
</div>
<div id="gaussian-histogram-magnify" class="magnify"
onclick="magnifyFigure(&#39;gaussian-histogram&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-histogram-caption" class="caption-frame">
<p>Figure: James Clerk Maxwell 1831-1879 Derived distribution of
velocities of particles in an ideal gas (elastic fluid).</p>
</div>
</div>
<div class="figure">
<div id="maxwell-boltzmann-gibbs-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="maxwell-boltzmann-gibbs-magnify" class="magnify"
onclick="magnifyFigure(&#39;maxwell-boltzmann-gibbs&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="maxwell-boltzmann-gibbs-caption" class="caption-frame">
<p>Figure: James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906)
Josiah Willard Gibbs (1839-1903)</p>
</div>
</div>
<p>Many of the ideas of early statistical physicists were rejected by a
cadre of physicists who didn’t believe in the notion of a molecule. The
stress of trying to have his ideas established caused Boltzmann to
commit suicide in 1906, only two years before the same ideas became
widely accepted.</p>
<div class="figure">
<div id="boltzmann-warmetheorie-figure" class="figure-frame">
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=Vuk5AQAAMAAJ&amp;pg=PA373&amp;output=embed" width="700" height="500">
</iframe>
</div>
<div id="boltzmann-warmetheorie-magnify" class="magnify"
onclick="magnifyFigure(&#39;boltzmann-warmetheorie&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="boltzmann-warmetheorie-caption" class="caption-frame">
<p>Figure: Boltzmann’s paper <span class="citation"
data-cites="Boltzmann-warmetheorie77">Boltzmann (n.d.)</span> which
introduced the relationship between entropy and probability. A
translation with notes is available in <span class="citation"
data-cites="Kim-translation15">Sharp and Matschinsky (2015)</span>.</p>
</div>
</div>
<p>The important point about the uncertainty being represented here is
that it is not genuine stochasticity, it is a lack of knowledge about
the system. The techniques proposed by Maxwell, Boltzmann and Gibbs
allow us to exactly represent the state of the system through a set of
parameters that represent the sufficient statistics of the physical
system. We know these values as the volume, temperature, and pressure.
The challenge for us, when approximating the physical world with the
techniques we will use is that we will have to sit somewhere between the
deterministic and purely stochastic worlds that these different
scientists described.</p>
<p>One ongoing characteristic of people who study probability and
uncertainty is the confidence with which they hold opinions about it.
Another leader of the Cavendish laboratory expressed his support of the
second law of thermodynamics (which can be proven through the work of
Gibbs/Boltzmann) with an emphatic statement at the beginning of his
book.</p>
<div class="figure">
<div id="eddington-book-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/arthur-stanley-eddington.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_7.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eddington-book-magnify" class="magnify"
onclick="magnifyFigure(&#39;eddington-book&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eddington-book-caption" class="caption-frame">
<p>Figure: Eddington’s book on the Nature of the Physical World <span
class="citation" data-cites="Eddington:nature29">(Eddington,
1929)</span></p>
</div>
</div>
<p>The same Eddington is also famous for dismissing the ideas of a young
Chandrasekhar who had come to Cambridge to study in the Cavendish lab.
Chandrasekhar demonstrated the limit at which a star would collapse
under its own weight to a singularity, but when he presented the work to
Eddington, he was dismissive suggesting that there “must be some natural
law that prevents this abomination from happening”.</p>
<div class="figure">
<div id="physical-world-chandra-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/ChandraNobel.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="physical-world-chandra-magnify" class="magnify"
onclick="magnifyFigure(&#39;physical-world-chandra&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="physical-world-chandra-caption" class="caption-frame">
<p>Figure: Chandrasekhar (1910-1995) derived the limit at which a star
collapses in on itself. Eddington’s confidence in the 2nd law may have
been what drove him to dismiss Chandrasekhar’s ideas, humiliating a
young scientist who would later receive a Nobel prize for the work.</p>
</div>
</div>
<div class="figure">
<div id="deepest-humiliation-eddington-cropped-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deepest-humiliation-eddington-cropped-magnify" class="magnify"
onclick="magnifyFigure(&#39;deepest-humiliation-eddington-cropped&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deepest-humiliation-eddington-cropped-caption"
class="caption-frame">
<p>Figure: Eddington makes his feelings about the primacy of the second
law clear. This primacy is perhaps because the second law can be
demonstrated mathematically, building on the work of Maxwell, Gibbs and
Boltzmann. <span class="citation"
data-cites="Eddington:nature29">Eddington (1929)</span></p>
</div>
</div>
<p>Presumably he meant that the creation of a black hole seemed to
transgress the second law of thermodynamics, although later Hawking was
able to show that blackholes do evaporate, but the time scales at which
this evaporation occurs is many orders of magnitude slower than other
processes in the universe.</p>
<h2 id="brownian-motion-and-wiener">Brownian Motion and Wiener</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/brownian-wiener.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/brownian-wiener.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Robert Brown was a botanist who was studying plant pollen in 1827
when he noticed a trembling motion of very small particles contained
within cavities within the pollen. He worked hard to eliminate the
potential source of the movement by exploring other materials where he
found it to be continuously present. Thus, the movement was not
associated, as he originally thought, with life.</p>
<p>In 1905 Albert Einstein produced the first mathematical explanation
of the phenomenon. This can be seen as our first model of a ‘curve of a
simple molecule of air’. To model the phenomenon Einstein introduced
stochasticity to a differential equation. The particles were being
peppered with high-speed water molecules, that was triggering the
motion. Einstein modelled this as a stochastic process.</p>
<div class="figure">
<div id="albert-einstein-photo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="albert-einstein-photo-magnify" class="magnify"
onclick="magnifyFigure(&#39;albert-einstein-photo&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="albert-einstein-photo-caption" class="caption-frame">
<p>Figure: Albert Einstein’s 1905 paper on Brownian motion introduced
stochastic differential equations which can be used to model the ‘curve
of a simple molecule of air’.</p>
</div>
</div>
<p>Norbert Wiener was a child prodigy, whose father had schooled him in
philosophy. He was keen to have his son work with the leading
philosophers of the age, so at the age of 18 Wiener arrived in Cambridge
(already with a PhD). He was despatched to study with Bertrand Russell
but Wiener and Russell didn’t get along. Wiener wasn’t persuaded by
Russell’s ideas for theories of knowledge through logic. He was more
aligned with Laplace and his desire for a theory of ignorance. In is
autobiography he relates it as the first thing he could see his father
was proud of (at around the age of 10 or 11) <span class="citation"
data-cites="Wiener-exprodigy53">(Wiener, 1953)</span>.</p>
<div class="figure">
<div id="russell-wiener-russell-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="85%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="russell-wiener-russell-magnify" class="magnify"
onclick="magnifyFigure(&#39;russell-wiener-russell&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="russell-wiener-russell-caption" class="caption-frame">
<p>Figure: Bertrand Russell (1872-1970), Albert Einstein (1879-1955),
Norbert Wiener, (1894-1964)</p>
</div>
</div>
<p>But Russell (despite also not getting along well with Wiener)
introduced Wiener to Einstein’s works, and Wiener also met G. H. Hardy.
He left Cambridge for Göttingen where he studied with Hilbert. He
developed the underlying mathematics for proving the existence of the
solutions to Einstein’s equation, which are now known as Wiener
processes.</p>
<div class="figure">
<div id="brownian-motion-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="brownian-motion-magnify" class="magnify"
onclick="magnifyFigure(&#39;brownian-motion&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="brownian-motion-caption" class="caption-frame">
<p>Figure: Brownian motion of a large particle in a group of smaller
particles. The movement is known as a <em>Wiener process</em> after
Norbert Wiener.</p>
</div>
</div>
<div class="figure">
<div id="norbert-wiener-yellow-peril-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/wiener-yellow-peril.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="norbert-wiener-yellow-peril-magnify" class="magnify"
onclick="magnifyFigure(&#39;norbert-wiener-yellow-peril&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="norbert-wiener-yellow-peril-caption" class="caption-frame">
<p>Figure: Norbert Wiener (1894 - 1964). Founder of cybernetics and the
information era. He used Gibbs’s ideas to develop a “theory of
ignorance” that he deployed in early communication. On the right is
Wiener’s wartime report that used stochastic processes in forecasting
with applications in radar control (image from <span class="citation"
data-cites="Coales-yellow14">Coales and Kane (2014)</span>).</p>
</div>
</div>
<p>Wiener himself used the processes in his work. He was focused on
mathematical theories of communication. Between the world wars he was
based at Massachusetts Institute of Technology where the burgeoning
theory of electrical engineering was emerging, with a particular focus
on communication lines. Winer developed theories of communication that
used Gibbs’s entropy to encode information. He also used the ideas
behind the Wiener process for developing tracking methods for radar
systems in the second world war. These processes are what we know of now
as Gaussian processes (<span class="citation"
data-cites="Wiener:yellow49">Wiener (1949)</span>).</p>
<h2 id="kappenball">Kappenball</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/kappenball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/kappenball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="kappen-ball-figure" class="figure-frame">
<div>
<div style="width:900px;text-align:center;display:inline">
<span style="float:left;">Score:
<output id="kappenball-score">
</output>
</span> <span style="float:right;">Energy:
<output id="kappenball-energy">
</output>
</span>
<div style="clear: both;">

</div>
</div>
<canvas id="kappenball-canvas" width="900" height="500" style="border:1px solid black;display:inline;text-align:center ">
</canvas>
<div>
<input type="range" min="0" max="100" value="0" class="slider" id="kappenball-stochasticity" style="width:900px;"/>
</div>
<div>
<button id="kappenball-newball" style="text-align:right">
New Ball
</button>
<button id="kappenball-pause" style="text-align:right">
Pause
</button>
</div>
<output id="kappenball-count">
</output>
<script src="https://inverseprobability.com/talks/scripts//ballworld/kappenball.js"></script>
</div>
</div>
<div id="kappen-ball-magnify" class="magnify"
onclick="magnifyFigure(&#39;kappen-ball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kappen-ball-caption" class="caption-frame">
<p>Figure: Kappen Ball</p>
</div>
</div>
<p>If you want to complete a task, should you do it now or should you
put it off until tomorrow? Despite being told to not delay tasks, many
of us are deadline driven. Why is this?</p>
<p>Kappenball is a simple game that illustrates that this behaviour can
be optimal. It is inspired by an example in stochastic optimal control
by <a href="https://www.snn.ru.nl/~bertk/">Bert Kappen</a>. The game is
as follows: you need to place a falling balloon into one of two holes,
but if the balloon misses the holes it will pop on pins placed in the
ground. In ‘deterministic mode’, the balloon falls straight towards the
ground and the game is easy. You simply choose which hole to place the
ball in, and you can start to place it there as soon as the ball appears
at the top of the screen. The game becomes more interesting as you
increase the uncertainty. In Kappenball, the uncertainty takes the form
of the balloon being blown left and right as it falls. This movement
means that it is not sensible to decide early on which hole to place the
balloon in. A better strategy is to wait and see which hole the ball
falls towards. You can then place it in that hole using less energy than
in deterministic mode. Sometimes, the ball even falls into the hole on
its own, and you don’t have to expend any energy, but it requires some
skill to judge when you need to intervene. For this system Bert Kappen
has shown mathematically that the best solution is to wait until the
ball is close to the hole before you push it in. In other words, you
should be deadline driven.</p>
<p>In fact, it seems here uncertainty is a good thing, because on
average you’ll get the ball into the hole with less energy (by playing
intelligently, and being deadline driven!) than you do with
`deterministic mode’. It requires some skill to do this, more than the
deterministic system, but by using your resources intelligently you can
get more out of the system. However, if the uncertainty increases too
much then regardless of your skill, you can’t control the ball at
all.</p>
<p>This simple game explains many of the behaviours we exhibit in real
life. If a system is completely deterministic, then we can make a
decision early on and be sure that the ball will ‘drop in the hole’.
However, if there is uncertainty in a system, it can make sense to delay
our decision making until we’ve seen how events ‘pan out’. Be careful
though, as we also see that when the uncertainty is large, if you don’t
have the resources or the skill to be deadline-driven the uncertainty
can overwhelm you and events can quickly move beyond our control.</p>
<h2 id="bayesian-inference-by-rejection-sampling">Bayesian Inference by
Rejection Sampling</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One view of Bayesian inference is to assume we are given a mechanism
for generating samples, where we assume that mechanism is representing
an accurate view on the way we believe the world works.</p>
<p>This mechanism is known as our <em>prior</em> belief.</p>
<p>We combine our prior belief with our observations of the real world
by discarding all those prior samples that are inconsistent with our
observations. The <em>likelihood</em> defines mathematically what we
mean by inconsistent with the observations. The higher the noise level
in the likelihood, the looser the notion of consistent.</p>
<p>The samples that remain are samples from the <em>posterior</em>.</p>
<p>This approach to Bayesian inference is closely related to two
sampling techniques known as <em>rejection sampling</em> and
<em>importance sampling</em>. It is realized in practice in an approach
known as <em>approximate Bayesian computation</em> (ABC) or
likelihood-free inference.</p>
<p>In practice, the algorithm is often too slow to be practical, because
most samples will be inconsistent with the observations and as a result
the mechanism must be operated many times to obtain a few posterior
samples.</p>
<p>However, in the Gaussian process case, when the likelihood also
assumes Gaussian noise, we can operate this mechanism mathematically,
and obtain the posterior density <em>analytically</em>. This is the
benefit of Gaussian processes.</p>
<p>First, we will load in two python functions for computing the
covariance function.</p>
<p>Next, we sample from a multivariate normal density (a multivariate
Gaussian), using the covariance function as the covariance matrix.</p>
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gp-rejection-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-rejection-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-rejection-samples-caption" class="caption-frame">
<p>Figure: One view of Bayesian inference is we have a machine for
generating samples (the <em>prior</em>), and we discard all samples
inconsistent with our data, leaving the samples of interest (the
<em>posterior</em>). This is a rejection sampling view of Bayesian
inference. The Gaussian process allows us to do this analytically by
multiplying the <em>prior</em> by the <em>likelihood</em>.</p>
</div>
</div>
<h2 id="maxwells-demon">Maxwell’s Demon</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/maxwells-demon-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/maxwells-demon-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="maxwells-demon-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//physics/maxwells-demon.svg" width="100%" style=" ">
</object>
</div>
<div id="maxwells-demon-magnify" class="magnify"
onclick="magnifyFigure(&#39;maxwells-demon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="maxwells-demon-caption" class="caption-frame">
<p>Figure: Maxwell’s demon opens and closes a door which allows fast
particles to pass from left to right and slow particles to pass from
right to left. This makes the left hand side colder than the right.</p>
</div>
</div>
<div class="figure">
<div id="maxwells-demon-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="maxwell-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="maxwell-entropy">
</output>
</div>
<div id="maxwell-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="maxwell-newball" style="text-align:right">
New Ball
</button>
<button id="maxwell-pause" style="text-align:right">
Pause
</button>
<button id="maxwell-skip" style="text-align:right">
Skip 1000s
</button>
<button id="maxwell-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://inverseprobability.com/talks/scripts//ballworld/maxwell.js"></script>
</div>
<div id="maxwells-demon-magnify" class="magnify"
onclick="magnifyFigure(&#39;maxwells-demon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="maxwells-demon-caption" class="caption-frame">
<p>Figure: Maxwell’s Demon. The demon decides balls are either cold
(blue) or hot (red) according to their velocity. Balls are allowed to
pass the green membrane from right to left only if they are cold, and
from left to right, only if they are hot.</p>
</div>
</div>
<h2 id="conclusions">Conclusions</h2>
<p>The probabilistic modelling community has evolved in an era where the
assumption was that ambiguous conclusions are best shared with a
(trained) professional through probabilities. Recent advances in
generative AI offer the possibility of machines that have a better
understanding of human subjective ambiguities and therefore machines
that can summarise information in a way that can be interogated rather
than just through a series of numbers.</p>
<!-- lecture 3 -->
<center>
A chance for us to acknowledge our ignorance and to rediscover
interdisplinary science.
</center>
<h2 id="deepface">DeepFace</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deep-face-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-face&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-face-caption" class="caption-frame">
<p>Figure: The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.</p>
</div>
</div>
<p>The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span> consists
of layers that deal with <em>translation</em> invariances, known as
convolutional layers. These layers are followed by three
locally-connected layers and two fully-connected layers. Color
illustrates feature maps produced at each layer. The neural network
includes more than 120 million parameters, where more than 95% come from
the local and fully connected layers.</p>
<h3 id="deep-learning-as-pinball">Deep Learning as Pinball</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-pinball-caption" class="caption-frame">
<p>Figure: Deep learning models are composition of simple functions. We
can think of a pinball machine as an analogy. Each layer of pins
corresponds to one of the layers of functions in the model. Input data
is represented by the location of the ball from left to right when it is
dropped in from the top. Output class comes from the position of the
ball as it leaves the pins at the bottom.</p>
</div>
</div>
<p>Sometimes deep learning models are described as being like the brain,
or too complex to understand, but one analogy I find useful to help the
gist of these models is to think of them as being similar to early pin
ball machines.</p>
<p>In a deep neural network, we input a number (or numbers), whereas in
pinball, we input a ball.</p>
<p>Think of the location of the ball on the left-right axis as a single
number. Our simple pinball machine can only take one number at a time.
As the ball falls through the machine, each layer of pins can be thought
of as a different layer of ‘neurons’. Each layer acts to move the ball
from left to right.</p>
<p>In a pinball machine, when the ball gets to the bottom it might fall
into a hole defining a score, in a neural network, that is equivalent to
the decision: a classification of the input object.</p>
<p>An image has more than one number associated with it, so it is like
playing pinball in a <em>hyper-space</em>.</p>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-initialization-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-initialization&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-initialization-caption" class="caption-frame">
<p>Figure: At initialization, the pins, which represent the parameters
of the function, aren’t in the right place to bring the balls to the
correct decisions.</p>
</div>
</div>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-trained-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-trained&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-trained-caption" class="caption-frame">
<p>Figure: After learning the pins are now in the right place to bring
the balls to the correct decisions.</p>
</div>
</div>
<p>Learning involves moving all the pins to be in the correct position,
so that the ball ends up in the right place when it’s fallen through the
machine. But moving all these pins in hyperspace can be difficult.</p>
<p>In a hyper-space you have to put a lot of data through the machine
for to explore the positions of all the pins. Even when you feed many
millions of data points through the machine, there are likely to be
regions in the hyper-space where no ball has passed. When future test
data passes through the machine in a new route unusual things can
happen.</p>
<p><em>Adversarial examples</em> exploit this high dimensional space. If
you have access to the pinball machine, you can use gradient methods to
find a position for the ball in the hyper space where the image looks
like one thing, but will be classified as another.</p>
<p>Probabilistic methods explore more of the space by considering a
range of possible paths for the ball through the machine. This helps to
make them more data efficient and gives some robustness to adversarial
examples.</p>
<!--Connect supply chain as a "challenge" tot he abstraction of Schroedinger's bridge. Link to Optimal Transport (matching without the "physics"). Maxwell's demon.-->
<!--Control ability paper with Mauricio and Simo??)-->
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
<!--include{_gp/includes/what-is-a-gp.md} -->
<h2 id="deep-gaussian-processes">Deep Gaussian Processes</h2>
<ul>
<li><em>Deep Gaussian Processes and Variational Propagation of
Uncertainty</em> <span class="citation"
data-cites="Damianou:thesis2015">Damianou (2015)</span></li>
</ul>
<h2 id="structure-of-priors">Structure of Priors</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/mackay-bathwater.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/mackay-bathwater.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Even in the early days of Gaussian processes in machine learning, it
was understood that we were throwing something fundamental away. This is
perhaps captured best by David MacKay in his 1997 NeurIPS tutorial on
Gaussian processes, where he asked “Have we thrown out the baby with the
bathwater?”. The quote below is from his summarization paper.</p>
<blockquote>
<p>According to the hype of 1987, neural networks were meant to be
intelligent models which discovered features and patterns in data.
Gaussian processes in contrast are simply smoothing devices. How can
Gaussian processes possibly replace neural networks? What is going
on?</p>
<p><span class="citation" data-cites="MacKay:gpintroduction98">MacKay
(n.d.)</span></p>
</blockquote>
<h2 id="deep-neural-network">Deep Neural Network</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install daft</span></code></pre></div>
<div class="figure">
<div id="deep-neural-network-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn2.svg" width="70%" style=" ">
</object>
</div>
<div id="deep-neural-network-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-neural-network&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-neural-network-caption" class="caption-frame">
<p>Figure: A deep neural network. Input nodes are shown at the bottom.
Each hidden layer is the result of applying an affine transformation to
the previous layer and placing through an activation function.</p>
</div>
</div>
<p>Mathematically, each layer of a neural network is given through
computing the activation function, <span
class="math inline">\(\phi(\cdot)\)</span>, contingent on the previous
layer, or the inputs. In this way the activation functions, are composed
to generate more complex interactions than would be possible with any
single layer. <span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{
h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{
h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
<h2 id="overfitting">Overfitting</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/overfitting-low-rank.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/overfitting-low-rank.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One potential problem is that as the number of nodes in two adjacent
layers increases, the number of parameters in the affine transformation
between layers, <span class="math inline">\(\mathbf{W}\)</span>,
increases. If there are <span class="math inline">\(k_{i-1}\)</span>
nodes in one layer, and <span class="math inline">\(k_i\)</span> nodes
in the following, then that matrix contains <span
class="math inline">\(k_i k_{i-1}\)</span> parameters, when we have
layer widths in the 1000s that leads to millions of parameters.</p>
<p>One proposed solution is known as <em>dropout</em> where only a
sub-set of the neural network is trained at each iteration. An
alternative solution would be to reparameterize <span
class="math inline">\(\mathbf{W}\)</span> with its <em>singular value
decomposition</em>. <span class="math display">\[
  \mathbf{W}= \mathbf{U}\boldsymbol{ \Lambda}\mathbf{V}^\top
  \]</span> or <span class="math display">\[
  \mathbf{W}= \mathbf{U}\mathbf{V}^\top
  \]</span> where if <span class="math inline">\(\mathbf{W}\in
\Re^{k_1\times k_2}\)</span> then <span
class="math inline">\(\mathbf{U}\in \Re^{k_1\times q}\)</span> and <span
class="math inline">\(\mathbf{V}\in \Re^{k_2\times q}\)</span>, i.e. we
have a low rank matrix factorization for the weights.</p>
<div class="figure">
<div id="low-rank-mapping-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//wisuvt.svg" width="80%" style=" ">
</object>
</div>
<div id="low-rank-mapping-magnify" class="magnify"
onclick="magnifyFigure(&#39;low-rank-mapping&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="low-rank-mapping-caption" class="caption-frame">
<p>Figure: Pictorial representation of the low rank form of the matrix
<span class="math inline">\(\mathbf{W}\)</span>.</p>
</div>
</div>
<p>In practice there is evidence that deep models seek these low rank
solutions where we expect better generalisation. See e.g. <span
class="citation" data-cites="Arora-convergence19">Arora et al.
(2019)</span>;<span class="citation"
data-cites="Jacot-deeplinear21">Jacot et al. (2021)</span>.</p>
<h2 id="bottleneck-layers-in-deep-neural-networks">Bottleneck Layers in
Deep Neural Networks</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-nn-bottleneck-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck2.svg" width="70%" style=" ">
</object>
</div>
<div id="deep-nn-bottleneck-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-nn-bottleneck&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-nn-bottleneck-caption" class="caption-frame">
<p>Figure: Inserting the bottleneck layers introduces a new set of
variables.</p>
</div>
</div>
<p>Including the low rank decomposition of <span
class="math inline">\(\mathbf{W}\)</span> in the neural network, we
obtain a new mathematical form. Effectively, we are adding additional
<em>latent</em> layers, <span class="math inline">\(\mathbf{
z}\)</span>, in between each of the existing hidden layers. In a neural
network these are sometimes known as <em>bottleneck</em> layers. The
network can now be written mathematically as <span
class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{U}_1 \mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \mathbf{ h}_{1}\\
  \mathbf{ h}_{2} &amp;= \phi\left(\mathbf{U}_2 \mathbf{ z}_{2}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \mathbf{ h}_{2}\\
  \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{U}_3 \mathbf{ z}_{3}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4^\top\mathbf{ h}_{3}.
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \phi\left(\mathbf{U}_1
\mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \phi\left(\mathbf{U}_2
\mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4 ^\top \mathbf{ z}_{3}
\end{align}
\]</span></p>
<h2 id="cascade-of-gaussian-processes">Cascade of Gaussian
Processes</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/cascade-of-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/cascade-of-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Now if we replace each of these neural networks with a Gaussian
process. This is equivalent to taking the limit as the width of each
layer goes to infinity, while appropriately scaling down the
outputs.</p>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{ f}_1\left(\mathbf{ x}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{ f}_2\left(\mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{ f}_3\left(\mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ f}_4\left(\mathbf{ z}_{3}\right)
\end{align}
\]</span></p>
<h1 id="deep-learning">Deep Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<!-- No slide titles in this context -->
<p>Mathematically, a deep Gaussian process can be seen as a composite
<em>multivariate</em> function, <span class="math display">\[
  \mathbf{g}(\mathbf{ x})=\mathbf{ f}_5(\mathbf{ f}_4(\mathbf{
f}_3(\mathbf{ f}_2(\mathbf{ f}_1(\mathbf{ x}))))).
  \]</span> Or if we view it from the probabilistic perspective we can
see that a deep Gaussian process is specifying a factorization of the
joint density, the standard deep model takes the form of a Markov
chain.</p>
<p><span class="math display">\[
  p(\mathbf{ y}|\mathbf{ x})= p(\mathbf{ y}|\mathbf{ f}_5)p(\mathbf{
f}_5|\mathbf{ f}_4)p(\mathbf{ f}_4|\mathbf{ f}_3)p(\mathbf{
f}_3|\mathbf{ f}_2)p(\mathbf{ f}_2|\mathbf{ f}_1)p(\mathbf{
f}_1|\mathbf{ x})
  \]</span></p>
<div class="figure">
<div id="deep-markov-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov.svg" width="80%" style=" ">
</object>
</div>
<div id="deep-markov-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-markov&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-markov-caption" class="caption-frame">
<p>Figure: Probabilistically the deep Gaussian process can be
represented as a Markov chain. Indeed they can even be analyzed in this
way <span class="citation" data-cites="Dunlop:deep2017">(Dunlop et al.,
n.d.)</span>.</p>
</div>
</div>
<div class="figure">
<div id="deep-markov-vertical-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical.svg" width="7%" style=" ">
</object>
</div>
<div id="deep-markov-vertical-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-markov-vertical&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-markov-vertical-caption" class="caption-frame">
<p>Figure: More usually deep probabilistic models are written vertically
rather than horizontally as in the Markov chain.</p>
</div>
</div>
<h2 id="why-composition">Why Composition?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/process-composition.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/process-composition.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>If the result of composing many functions together is simply another
function, then why do we bother? The key point is that we can change the
class of functions we are modeling by composing in this manner. A
Gaussian process is specifying a prior over functions, and one with a
number of elegant properties. For example, the derivative process (if it
exists) of a Gaussian process is also Gaussian distributed. That makes
it easy to assimilate, for example, derivative observations. But that
also might raise some alarm bells. That implies that the <em>marginal
derivative distribution</em> is also Gaussian distributed. If that’s the
case, then it means that functions which occasionally exhibit very large
derivatives are hard to model with a Gaussian process. For example, a
function with jumps in.</p>
<p>A one off discontinuity is easy to model with a Gaussian process, or
even multiple discontinuities. They can be introduced in the mean
function, or independence can be forced between two covariance functions
that apply in different areas of the input space. But in these cases we
will need to specify the number of discontinuities and where they occur.
In otherwords we need to <em>parameterise</em> the discontinuities. If
we do not know the number of discontinuities and don’t wish to specify
where they occur, i.e. if we want a non-parametric representation of
discontinuities, then the standard Gaussian process doesn’t help.</p>
<h2 id="stochastic-process-composition">Stochastic Process
Composition</h2>
<p>The deep Gaussian process leads to <em>non-Gaussian</em> models, and
non-Gaussian characteristics in the covariance function. In effect, what
we are proposing is that we change the properties of the functions we
are considering by <em>composing stochastic processes</em>. This is an
approach to creating new stochastic processes from well known
processes.</p>
<p>Additionally, we are not constrained to the formalism of the chain.
For example, we can easily add single nodes emerging from some point in
the depth of the chain. This allows us to combine the benefits of the
graphical modelling formalism, but with a powerful framework for
relating one set of variables to another, that of Gaussian processes</p>
<div class="figure">
<div id="deep-markov-vertical-side-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical-side.svg" width="15%" style=" ">
</object>
</div>
<div id="deep-markov-vertical-side-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-markov-vertical-side&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-markov-vertical-side-caption" class="caption-frame">
<p>Figure: More generally we aren’t constrained by the Markov chain. We
can design structures that respect our belief about the underlying
conditional dependencies. Here we are adding a side note from the
chain.</p>
</div>
</div>
<h2 id="difficulty-for-probabilistic-approaches">Difficulty for
Probabilistic Approaches</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/non-linear-difficulty.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/non-linear-difficulty.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The challenge for composition of probabilistic models is that you
need to propagate a probability densities through non linear mappings.
This allows you to create broader classes of probability density.
Unfortunately it renders the resulting densities
<em>intractable</em>.</p>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-3d-plot.svg" width="60%" style=" ">
</object>
</div>
<div id="nonlinear-mapping-3d-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;nonlinear-mapping-3d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nonlinear-mapping-3d-plot-caption" class="caption-frame">
<p>Figure: A two dimensional grid mapped into three dimensions to form a
two dimensional manifold.</p>
</div>
</div>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-2d-plot.svg" width="60%" style=" ">
</object>
</div>
<div id="non-linear-mapping-2d-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;non-linear-mapping-2d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="non-linear-mapping-2d-plot-caption" class="caption-frame">
<p>Figure: A one dimensional line mapped into two dimensions by two
separate independent functions. Each point can be mapped exactly through
the mappings.</p>
</div>
</div>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
<div id="gaussian-through-nonlinear-magnify" class="magnify"
onclick="magnifyFigure(&#39;gaussian-through-nonlinear&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-through-nonlinear-caption" class="caption-frame">
<p>Figure: A Gaussian density over the input of a non linear function
leads to a very non Gaussian output. Here the output is multimodal.</p>
</div>
</div>
<h2 id="standard-variational-approach-fails">Standard Variational
Approach Fails</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gplvm/includes/variational-bayes-gplvm-long.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gplvm/includes/variational-bayes-gplvm-long.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li>Standard variational bound has the form: <span
class="math display">\[
\mathcal{L}= \left\langle\log p(\mathbf{
y}|\mathbf{Z})\right\rangle_{q(\mathbf{Z})} + \text{KL}\left(
q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
<p>The standard variational approach would require the expectation of
<span class="math inline">\(\log p(\mathbf{ y}|\mathbf{Z})\)</span>
under <span class="math inline">\(q(\mathbf{Z})\)</span>. <span
class="math display">\[
  \begin{align}
  \log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{
y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{
f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log
\det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}}
-\frac{n}{2}\log 2\pi
  \end{align}
  \]</span> But this is extremely difficult to compute because <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}\)</span> is
dependent on <span class="math inline">\(\mathbf{Z}\)</span> and it
appears in the inverse.</p>
<h2 id="variational-bayesian-gp-lvm">Variational Bayesian GP-LVM</h2>
<p>The alternative approach is to consider the collapsed variational
bound (used for low rank (sparse is a misnomer) Gaussian process
approximations. <span class="math display">\[
    p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{
f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{
u}
  \]</span> <span class="math display">\[
    p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int
\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
  \]</span> <span class="math display">\[
      \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq
\int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
  \]</span></p>
<p>To integrate across <span class="math inline">\(\mathbf{Z}\)</span>
we apply the lower bound to the inner integral. <span
class="math display">\[
    \begin{align}
    \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp;
\left\langle\sum_{i=1}^n\log  c_i\right\rangle_{q(\mathbf{Z})}\\ &amp;
+\left\langle\log\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)\right\rangle_{q(\mathbf{Z})}\\&amp;
+ \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
    \end{align}
  \]</span> * Which is analytically tractable for Gaussian <span
class="math inline">\(q(\mathbf{Z})\)</span> and some covariance
functions.</p>
<ul>
<li><p>Need expectations under <span
class="math inline">\(q(\mathbf{Z})\)</span> of: <span
class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{
u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i,
\mathbf{ u}}\right]
\]</span> and <span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{
u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log
2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f},
\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{
u}\right)^2
\]</span></p></li>
<li><p>This requires the expectations <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{
u}}\right\rangle_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{
u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{
f}}\right\rangle_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance
functions <span class="citation"
data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or
through sampling <span class="citation"
data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015;
Salimbeni and Deisenroth, 2017)</span>.</p></li>
</ul>
<p>Variational approximations aren’t the only approach to approximate
inference. The original work on deep Gaussian processes made use of MAP
approximations <span class="citation"
data-cites="Lawrence:hgplvm07">(Lawrence and Moore, 2007)</span>, which
couldn’t propagate the uncertainty through the model at the data points
but sustain uncertainty elsewhere. Since the variational approximation
was proposed researchers have also considered sampling approaches <span
class="citation" data-cites="Havasi:deepgp18">(Havasi et al.,
2018)</span> and expectation propagation <span class="citation"
data-cites="Bui:deep16">(Bui et al., 2016)</span>.</p>
<div class="figure">
<div id="neural-network-uncertainty-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepgp/neural-network-uncertainty.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="neural-network-uncertainty-magnify" class="magnify"
onclick="magnifyFigure(&#39;neural-network-uncertainty&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="neural-network-uncertainty-caption" class="caption-frame">
<p>Figure: Even the latest work on Bayesian neural networks has severe
problems handling uncertainty. In this example, <span class="citation"
data-cites="Izmailov:subspace19">(Izmailov et al., 2019)</span>, methods
even fail to interpolate through the data correctly or provide well
calibrated error bars in regions where data is observed.</p>
</div>
</div>
<p>The argument in the deep learning revolution is that deep
architectures allow us to develop an abstraction of the feature set
through model composition. Composing Gaussian processes is analytically
intractable. To form deep Gaussian processes we use a variational
approach to stack the models.</p>
<h2 id="stacked-pca">Stacked PCA</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-pca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-pca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="stack-pca-sample-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-4.svg" width="20%" style=" ">
</object>
</div>
<div id="stack-pca-sample-magnify" class="magnify"
onclick="magnifyFigure(&#39;stack-pca-sample&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="stack-pca-sample-caption" class="caption-frame">
<p>Figure: Composition of linear functions just leads to a new linear
function. Here you see the result of multiple affine transformations
applied to a square in two dimensions.</p>
</div>
</div>
<p>Stacking a series of linear functions simply leads to a new linear
function. The use of multiple linear function merely changes the
covariance of the resulting Gaussian. If <span class="math display">\[
\mathbf{Z}\sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right)
\]</span> and the <span class="math inline">\(i\)</span>th hidden layer
is a multivariate linear transformation defined by <span
class="math inline">\(\mathbf{W}_i\)</span>, <span
class="math display">\[
\mathbf{Y}= \mathbf{Z}\mathbf{W}_1 \mathbf{W}_2 \dots \mathbf{W}_\ell
\]</span> then the rules of multivariate Gaussians tell us that <span
class="math display">\[
\mathbf{Y}\sim \mathcal{N}\left(\mathbf{0},\mathbf{W}_\ell\dots
\mathbf{W}_1 \mathbf{W}^\top_1 \dots \mathbf{W}^\top_\ell\right).
\]</span> So the model can be replaced by one where we set <span
class="math inline">\(\mathbf{V}= \mathbf{W}_\ell\dots \mathbf{W}_2
\mathbf{W}_1\)</span>. So is such a model trivial? The answer is that it
depends. There are two cases in which such a model remaisn interesting.
Firstly, if we make intermediate observations stemming from the chain.
So, for example, if we decide that, <span class="math display">\[
\mathbf{Z}_i = \mathbf{W}_i \mathbf{Z}_{i-1}
\]</span> and set <span class="math inline">\(\mathbf{Z}_{0} =
\mathbf{X}\sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right)\)</span>,
then the matrices <span class="math inline">\(\mathbf{W}\)</span>
inter-relate a series of jointly Gaussian observations in an interesting
way, stacking the full data matrix to give <span class="math display">\[
\mathbf{Z}= \begin{bmatrix}
\mathbf{Z}_0 \\
\mathbf{Z}_1 \\
\vdots \\
\mathbf{Z}_\ell
\end{bmatrix}
\]</span> we can obtain <span class="math display">\[\mathbf{Z}\sim
\mathcal{N}\left(\mathbf{0},\begin{bmatrix}
\mathbf{I}&amp; \mathbf{W}^\top_1 &amp;
\mathbf{W}_1^\top\mathbf{W}_2^\top &amp; \dots &amp; \mathbf{V}^\top \\
\mathbf{W}_1 &amp; \mathbf{W}_1 \mathbf{W}_1^\top &amp; \mathbf{W}_1
\mathbf{W}_1^\top \mathbf{W}_2^\top &amp; \dots &amp; \mathbf{W}_1
\mathbf{V}^\top \\
\mathbf{W}_2 \mathbf{W}_1 &amp; \mathbf{W}_2 \mathbf{W}_1
\mathbf{W}_1^\top &amp; \mathbf{W}_2 \mathbf{W}_1 \mathbf{W}_1^\top
\mathbf{W}_2^\top &amp; \dots &amp; \mathbf{W}_2 \mathbf{W}_1
\mathbf{V}^\top \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{V}&amp; \mathbf{V}\mathbf{W}_1^\top  &amp;
\mathbf{V}\mathbf{W}_1^\top \mathbf{W}_2^\top&amp; \dots &amp;
\mathbf{V}\mathbf{V}^\top
\end{bmatrix}\right)\]</span> which is a highly structured Gaussian
covariance with hierarchical dependencies between the variables <span
class="math inline">\(\mathbf{Z}_i\)</span>.</p>
<h2 id="stacked-gp">Stacked GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="stack-gp-sample-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-4.svg" width="20%" style=" ">
</object>
</div>
<div id="stack-gp-sample-magnify" class="magnify"
onclick="magnifyFigure(&#39;stack-gp-sample&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="stack-gp-sample-caption" class="caption-frame">
<p>Figure: Stacking Gaussian process models leads to non linear mappings
at each stage. Here we are mapping from two dimensions to two dimensions
in each layer.</p>
</div>
</div>
<p>Note that once the box has folded over on itself, it cannot be
unfolded. So a feature that is generated near the top of the model
cannot be removed further down the model.</p>
<p>This folding over effect happens in low dimensions. In higher
dimensions it is less common.</p>
<p>Observation of this effect at a talk in Cambridge was one of the
things that caused David Duvenaud (and collaborators) to consider the
behavior of deeper Gaussian process models <span class="citation"
data-cites="Duvenaud:pathologies14">(Duvenaud et al., 2014)</span>.</p>
<p>Such folding over in the latent spaces necessarily forces the density
to be non-Gaussian. Indeed, since folding-over is avoided as we increase
the dimensionality of the latent spaces, such processes become more
Gaussian. If we take the limit of the latent space dimensionality as it
tends to infinity, the entire deep Gaussian process returns to a
standard Gaussian process, with a covariance function given as a deep
kernel (such as those described by <span class="citation"
data-cites="Cho:deep09">Cho and Saul (2009)</span>).</p>
<p>Further analysis of these deep networks has been conducted by <span
class="citation" data-cites="Dunlop:deep2017">Dunlop et al.
(n.d.)</span>, who use analysis of the deep network’s stationary density
(treating it as a Markov chain across layers), to explore the nature of
the implied process prior for a deep GP.</p>
<p>Both of these works, however, make constraining assumptions on the
form of the Gaussian process prior at each layer (e.g. same covariance
at each layer). In practice, the form of this covariance can be learnt
and the densities described by the deep GP are more general than those
mentioned in either of these papers.</p>
<h2 id="stacked-gps-video-by-david-duvenaud">Stacked GPs (video by David
Duvenaud)</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-pathologies.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-pathologies.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="visualization-deep-gp-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/XhIvygQYFFQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="visualization-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;visualization-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="visualization-deep-gp-caption" class="caption-frame">
<p>Figure: Visualization of mapping of a two dimensional space through a
deep Gaussian process.</p>
</div>
</div>
<p>David Duvenaud also created a YouTube video to help visualize what
happens as you drop through the layers of a deep GP.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian
Process Framework in Python</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Gaussian processes are a flexible tool for non-parametric analysis
with uncertainty. The GPy software was started in Sheffield to provide a
easy to use interface to GPs. One which allowed the user to focus on the
modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing
Gaussian process models in Python. It is designed for teaching and
modelling. We welcome contributions which can be made through the GitHub
repository <a href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian
process models in python. This allows GPs to be combined with a wide
variety of software libraries.</p>
<p>The software itself is available on <a
href="https://github.com/SheffieldML/GPy">GitHub</a> and the team
welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language,
i.e., you specify the model rather than the algorithm. As well as a
large range of covariance functions the software allows for non-Gaussian
likelihoods, multivariate outputs, dimensionality reduction and
approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a
href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<p>This notebook depends on PyDeepGP. This library can be installed via
pip.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>SheffieldML<span class="op">/</span>PyDeepGP.git</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install mlai</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Late bind setup methods to DeepGP object</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> initialize</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> staged_optimize</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> posterior_sample</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> visualize</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> visualize_pinball</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.initialize<span class="op">=</span>initialize</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.staged_optimize<span class="op">=</span>staged_optimize</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.posterior_sample<span class="op">=</span>posterior_sample</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.visualize<span class="op">=</span>visualize</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.visualize_pinball<span class="op">=</span>visualize_pinball</span></code></pre></div>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression
modelling. The data consists of the pace of Olympic Gold Medal Marathon
winners for the Olympics from 1896 to present. Let’s load in the data
and plot.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pods</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1896.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in that
year the Olympics was in St Louis, USA. Organizational problems and
challenges with dust kicked up by the cars following the race meant that
participants got lost, and only very few participants completed. More
recent years see more consistently quick marathons.</p>
<h2 id="alan-turing">Alan Turing</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="turing-run-times-magnify" class="magnify"
onclick="magnifyFigure(&#39;turing-run-times&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="turing-run-times-caption" class="caption-frame">
<p>Figure: Alan Turing, in 1946 he was only 11 minutes slower than the
winner of the 1948 games. Would he have won a hypothetical games held in
1946? Source:
<a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank">Alan
Turing Internet Scrapbook</a>.</p>
</div>
</div>
<p>If we had to summarise the objectives of machine learning in one
word, a very good candidate for that word would be
<em>generalization</em>. What is generalization? From a human
perspective it might be summarised as the ability to take lessons
learned in one domain and apply them to another domain. If we accept the
definition given in the first session for machine learning, <span
class="math display">\[
\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow}
\text{prediction}
\]</span> then we see that without a model we can’t generalise: we only
have data. Data is fine for answering very specific questions, like “Who
won the Olympic Marathon in 2012?”, because we have that answer stored,
however, we are not given the answer to many other questions. For
example, Alan Turing was a formidable marathon runner, in 1946 he ran a
time 2 hours 46 minutes (just under four minutes per kilometer, faster
than I and most of the other <a
href="http://www.parkrun.org.uk/sheffieldhallam/">Endcliffe Park Run</a>
runners can do 5 km). What is the probability he would have won an
Olympics if one had been held in 1946?</p>
<p>To answer this question we need to generalize, but before we
formalize the concept of generalization let’s introduce some formal
representation of what it means to generalize in machine learning.</p>
<h2 id="gaussian-process-fit">Gaussian Process Fit</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/olympic-marathon-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/olympic-marathon-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Our first objective will be to perform a Gaussian process fit to the
data, we’ll do this using the <a
href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<p>The first command sets up the model, then
<code>m_full.optimize()</code> optimizes the parameters of the
covariance function and the noise level of the model. Once the fit is
complete, we’ll try creating some test points, and computing the output
of the GP model in terms of the mean and standard deviation of the
posterior functions between 1870 and 2030. We plot the mean function and
the standard deviation at 200 locations. We can obtain the predictions
using <code>y_mean, y_var = m_full.predict(xt)</code></p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.linspace(<span class="dv">1870</span>,<span class="dv">2030</span>,<span class="dv">200</span>)[:,np.newaxis]</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>yt_mean, yt_var <span class="op">=</span> m_full.predict(xt)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>yt_sd<span class="op">=</span>np.sqrt(yt_var)</span></code></pre></div>
<p>Now we plot the results using the helper function in
<code>mlai.plot</code>.</p>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/olympic-marathon-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Olympic Marathon data. The error
bars are too large, perhaps due to the outlier from 1904.</p>
</div>
</div>
<h2 id="fit-quality">Fit Quality</h2>
<p>In the fit we see that the error bars (coming mainly from the noise
variance) are quite large. This is likely due to the outlier point in
1904, ignoring that point we can see that a tighter fit is obtained. To
see this make a version of the model, <code>m_clean</code>, where that
point is removed.</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>x_clean<span class="op">=</span>np.vstack((x[<span class="dv">0</span>:<span class="dv">2</span>, :], x[<span class="dv">3</span>:, :]))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>y_clean<span class="op">=</span>np.vstack((yhat[<span class="dv">0</span>:<span class="dv">2</span>, :], yhat[<span class="dv">3</span>:, :]))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>m_clean <span class="op">=</span> GPy.models.GPRegression(x_clean,y_clean)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_clean.optimize()</span></code></pre></div>
<h2 id="deep-gp-fit">Deep GP Fit</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/olympic-marathon-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/olympic-marathon-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Let’s see if a deep Gaussian process can help here. We will construct
a deep Gaussian process with one hidden layer (i.e. one Gaussian process
feeding into another).</p>
<p>Build a Deep GP with an additional hidden layer (one dimensional) to
fit the model.</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>hidden <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP([y.shape[<span class="dv">1</span>],hidden,x.shape[<span class="dv">1</span>]],Y<span class="op">=</span>yhat, X<span class="op">=</span>x, inits<span class="op">=</span>[<span class="st">&#39;PCA&#39;</span>,<span class="st">&#39;PCA&#39;</span>], </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>[GPy.kern.RBF(hidden,ARD<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                           GPy.kern.RBF(x.shape[<span class="dv">1</span>],ARD<span class="op">=</span><span class="va">True</span>)], <span class="co"># the kernels for each layer</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">50</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Call the initalization</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>m.initialize()</span></code></pre></div>
<p>Now optimize the model.</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    layer.likelihood.variance.constrain_positive(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">True</span>,max_iters<span class="op">=</span><span class="dv">10000</span>)</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>m.staged_optimize(messages<span class="op">=</span>(<span class="va">True</span>,<span class="va">True</span>,<span class="va">True</span>))</span></code></pre></div>
<h2 id="olympic-marathon-data-deep-gp">Olympic Marathon Data Deep
GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp.svg" width="100%" style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-caption" class="caption-frame">
<p>Figure: Deep GP fit to the Olympic marathon data. Error bars now
change as the prediction evolves.</p>
</div>
</div>
<h2 id="olympic-marathon-data-deep-gp-1">Olympic Marathon Data Deep
GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-samples.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-samples-caption"
class="caption-frame">
<p>Figure: Point samples run through the deep Gaussian process show the
distribution of output locations.</p>
</div>
</div>
<h2 id="fitted-gp-for-each-layer">Fitted GP for each layer</h2>
<p>Now we explore the GPs the model has used to fit each layer. First of
all, we look at the hidden layer.</p>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-0.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-layer-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-layer-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-layer-0-caption"
class="caption-frame">
<p>Figure: The mapping from input to the latent layer is broadly, with
some flattening as time goes on. Variance is high across the input
range.</p>
</div>
</div>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-1.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-layer-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-layer-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-layer-1-caption"
class="caption-frame">
<p>Figure: The mapping from the latent layer to the output layer.</p>
</div>
</div>
<h2 id="olympic-marathon-pinball-plot">Olympic Marathon Pinball
Plot</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-pinball.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-pinball-caption"
class="caption-frame">
<p>Figure: A pinball plot shows the movement of the ‘ball’ as it passes
through each layer of the Gaussian processes. Mean directions of
movement are shown by lines. Shading gives one standard deviation of
movement position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. There is
some grouping of later points towards the right in the first layer,
which also injects a large amount of uncertainty. Due to flattening of
the curve in the second layer towards the right the uncertainty is
reduced in final output.</p>
</div>
</div>
<p>The pinball plot shows the flow of any input ball through the deep
Gaussian process. In a pinball plot a series of vertical parallel lines
would indicate a purely linear function. For the olypmic marathon data
we can see the first layer begins to shift from input towards the right.
Note it also does so with some uncertainty (indicated by the shaded
backgrounds). The second layer has less uncertainty, but bunches the
inputs more strongly to the right. This input layer of uncertainty,
followed by a layer that pushes inputs to the right is what gives the
heteroschedastic noise.</p>
<h2 id="gene-expression-example">Gene Expression Example</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/della-gatta-gene-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/della-gatta-gene-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We now consider an example in gene expression. Gene expression is the
measurement of mRNA levels expressed in cells. These mRNA levels show
which genes are ‘switched on’ and producing data. In the example we will
use a Gaussian process to determine whether a given gene is active, or
we are merely observing a noise response.</p>
<h2 id="della-gatta-gene-data">Della Gatta Gene Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/della-gatta-gene-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/della-gatta-gene-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li>Given given expression levels in the form of a time series from
<span class="citation" data-cites="DellaGatta:direct08">Della Gatta et
al. (2008)</span>.</li>
</ul>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.della_gatta_TRP63_gene_expression(data_set<span class="op">=</span><span class="st">&#39;della_gatta&#39;</span>,gene_number<span class="op">=</span><span class="dv">937</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-data-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-data-caption" class="caption-frame">
<p>Figure: Gene expression levels over time for a gene from data
provided by <span class="citation"
data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>. We
would like to understand whether there is signal in the data, or we are
only observing noise.</p>
</div>
</div>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene
<span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and
Lawrence (2011)</span>.</li>
</ul>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Freddie Kalaitzis
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/freddie-kalaitzis.jpg" clip-path="url(#clip4)"/>
</svg>
</div>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="a-simple-approach-to-ranking-magnify" class="magnify"
onclick="magnifyFigure(&#39;a-simple-approach-to-ranking&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="a-simple-approach-to-ranking-caption" class="caption-frame">
<p>Figure: The example is taken from the paper “A Simple Approach to
Ranking Differentially Expressed Gene Expression Time Courses through
Gaussian Process Regression.” <span class="citation"
data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence
(2011)</span>.</p>
</div>
</div>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180"
class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
<p>Our first objective will be to perform a Gaussian process fit to the
data, we’ll do this using the <a
href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>m_full.kern.lengthscale<span class="op">=</span><span class="dv">50</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<p>Initialize the length scale parameter (which here actually represents
a <em>time scale</em> of the covariance function) to a reasonable value.
Default would be 1, but here we set it to 50 minutes, given points are
arriving across zero to 250 minutes.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">20</span>,<span class="dv">260</span>,<span class="dv">200</span>)[:,np.newaxis]</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>yt_mean, yt_var <span class="op">=</span> m_full.predict(xt)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>yt_sd<span class="op">=</span>np.sqrt(yt_var)</span></code></pre></div>
<p>Now we plot the results using the helper function in
<code>mlai.plot</code>.</p>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-gp-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the time
scale parameter initialized to 50 minutes.</p>
</div>
</div>
<p>Now we try a model initialized with a longer length scale.</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>m_full2 <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>m_full2.kern.lengthscale<span class="op">=</span><span class="dv">2000</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full2.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp2-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-gp2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-gp2-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the time
scale parameter initialized to 2000 minutes.</p>
</div>
</div>
<p>Now we try a model initialized with a lower noise.</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>m_full3 <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>m_full3.kern.lengthscale<span class="op">=</span><span class="dv">20</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>m_full3.likelihood.variance<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full3.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp3-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-gp3&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-gp3-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the
noise initialized low (standard deviation 0.1) and the time scale
parameter initialized to 20 minutes.</p>
</div>
</div>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
<div id="gp-multiple-optima000-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-multiple-optima000&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-multiple-optima000-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<!--

<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima001.svg" width="" style=" "></object>-->
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">1</span>,x.shape[<span class="dv">1</span>]]</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i)]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>yhat, X<span class="op">=</span>x, </span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>                  inits<span class="op">=</span>inits, </span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels, <span class="co"># the kernels for each layer</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">20</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>m.initialize()</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>m.staged_optimize()</span></code></pre></div>
<h2 id="della-gatta-gene-data-deep-gp">Della Gatta Gene Data Deep
GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/della-gatta-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/della-gatta-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="della-gatta-gene-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-caption" class="caption-frame">
<p>Figure: Deep Gaussian process fit to the Della Gatta gene expression
data.</p>
</div>
</div>
<h2 id="della-gatta-gene-data-deep-gp-1">Della Gatta Gene Data Deep
GP</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-samples.svg" width style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-samples-caption"
class="caption-frame">
<p>Figure: Deep Gaussian process samples fitted to the Della Gatta gene
expression data.</p>
</div>
</div>
<h2 id="della-gatta-gene-data-latent-1">Della Gatta Gene Data Latent
1</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-0.svg" width="50%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-layer-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-layer-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-layer-0-caption"
class="caption-frame">
<p>Figure: Gaussian process mapping from input to latent layer for the
della Gatta gene expression data.</p>
</div>
</div>
<h2 id="della-gatta-gene-data-latent-2">Della Gatta Gene Data Latent
2</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-1.svg" width="50%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-layer-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-layer-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-layer-1-caption"
class="caption-frame">
<p>Figure: Gaussian process mapping from latent to output layer for the
della Gatta gene expression data.</p>
</div>
</div>
<h2 id="tp53-gene-pinball-plot">TP53 Gene Pinball Plot</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-pinball-caption"
class="caption-frame">
<p>Figure: A pinball plot shows the movement of the ‘ball’ as it passes
through each layer of the Gaussian processes. Mean directions of
movement are shown by lines. Shading gives one standard deviation of
movement position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. Pinball
plot of the della Gatta gene expression data.</p>
</div>
</div>
<h2 id="step-function">Step Function</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/step-function-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/step-function-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Next we consider a simple step function data set.</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>num_low<span class="op">=</span><span class="dv">25</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>num_high<span class="op">=</span><span class="dv">25</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>gap <span class="op">=</span> <span class="op">-</span><span class="fl">.1</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>noise<span class="op">=</span><span class="fl">0.0001</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.vstack((np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span>gap<span class="op">/</span><span class="fl">2.0</span>, num_low)[:, np.newaxis],</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>              np.linspace(gap<span class="op">/</span><span class="fl">2.0</span>, <span class="dv">1</span>, num_high)[:, np.newaxis]))</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.vstack((np.zeros((num_low, <span class="dv">1</span>)), np.ones((num_high,<span class="dv">1</span>))))</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y<span class="op">-</span>offset)<span class="op">/</span>scale</span></code></pre></div>
<h2 id="step-function-data">Step Function Data</h2>
<div class="figure">
<div id="step-function-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/step-function.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-data-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-data-caption" class="caption-frame">
<p>Figure: Simulation study of step function data artificially
generated. Here there is a small overlap between the two lines.</p>
</div>
</div>
<h2 id="step-function-data-gp">Step Function Data GP</h2>
<p>We can fit a Gaussian process to the step function data using
<code>GPy</code> as follows.</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<p>Where <code>GPy.models.GPRegression()</code> gives us a standard GP
regression model with exponentiated quadratic covariance function.</p>
<p>The model is optimized using <code>m_full.optimize()</code> which
calls an L-BGFS gradient based solver in python.</p>
<div class="figure">
<div id="step-function-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/step-function-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the step function data. Note the
large error bars and the over-smoothing of the discontinuity. Error bars
are shown at two standard deviations.</p>
</div>
</div>
<p>The resulting fit to the step function data shows some challenges. In
particular, the over smoothing at the discontinuity. If we know how many
discontinuities there are, we can parameterize them in the step
function. But by doing this, we form a semi-parametric model. The
parameters indicate how many discontinuities are, and where they are.
They can be optimized as part of the model fit. But if new, unforeseen,
discontinuities arise when the model is being deployed in practice,
these won’t be accounted for in the predictions.</p>
<h2 id="step-function-data-deep-gp">Step Function Data Deep GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/step-function-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/step-function-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>First we initialize a deep Gaussian process with three latent layers
(four layers total). Within each layer we create a GP with an
exponentiated quadratic covariance (<code>GPy.kern.RBF</code>).</p>
<p>At each layer we use 20 inducing points for the variational
approximation.</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>,x.shape[<span class="dv">1</span>]]</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i)]</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>yhat, X<span class="op">=</span>x, </span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>                  inits<span class="op">=</span>inits, </span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels, <span class="co"># the kernels for each layer</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">20</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p>Once the model is constructed we initialize the parameters, and
perform the staged optimization which starts by optimizing variational
parameters with a low noise and proceeds to optimize the whole
model.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>m.initialize()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>m.staged_optimize()</span></code></pre></div>
<p>We plot the output of the deep Gaussian process fitted to the step
data as follows.</p>
<p>The deep Gaussian process does a much better job of fitting the data.
It handles the discontinuity easily, and error bars drop to smaller
values in the regions of data.</p>
<div class="figure">
<div id="step-function-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-caption" class="caption-frame">
<p>Figure: Deep Gaussian process fit to the step function data.</p>
</div>
</div>
<h2 id="step-function-data-deep-gp-1">Step Function Data Deep GP</h2>
<p>The samples of the model can be plotted with the helper function from
<code>mlai.plot</code>, <code>model_sample</code></p>
<p>The samples from the model show that the error bars, which are
informative for Gaussian outputs, are less informative for this model.
They make clear that the data points lie, in output mainly at 0 or 1, or
occasionally in between.</p>
<div class="figure">
<div id="step-function-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-samples-caption" class="caption-frame">
<p>Figure: Samples from the deep Gaussian process model for the step
function fit.</p>
</div>
</div>
<p>The visualize code allows us to inspect the intermediate layers in
the deep GP model to understand how it has reconstructed the step
function.</p>
<div class="figure">
<div id="step-function-deep-gp-mappings-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-2.svg" width="60%" style=" ">
</object>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-3.svg" width="60%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-mappings-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp-mappings&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-mappings-caption" class="caption-frame">
<p>Figure: From top to bottom, the Gaussian process mapping function
that makes up each layer of the resulting deep Gaussian process.</p>
</div>
</div>
<p>A pinball plot can be created for the resulting model to understand
how the input is being translated to the output across the different
layers.</p>
<div class="figure">
<div id="step-function-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-pinball-caption" class="caption-frame">
<p>Figure: Pinball plot of the deep GP fitted to the step function data.
Each layer of the model pushes the ‘ball’ towards the left or right,
saturating at 1 and 0. This causes the final density to be be peaked at
0 and 1. Transitions occur driven by the uncertainty of the mapping in
each layer.</p>
</div>
</div>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.mcycle()</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>scale<span class="op">=</span>np.sqrt(y.var())</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>offset<span class="op">=</span>y.mean()</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<h2 id="motorcycle-helmet-data">Motorcycle Helmet Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/motorcycle-helmet-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/motorcycle-helmet-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="motorcycle-helment-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/motorcycle-helmet.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helment-data-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helment-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helment-data-caption" class="caption-frame">
<p>Figure: Motorcycle helmet data. The data consists of acceleration
readings on a motorcycle helmet undergoing a collision. The data
exhibits heteroschedastic (time varying) noise levles and
non-stationarity.</p>
</div>
</div>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<h2 id="motorcycle-helmet-data-gp">Motorcycle Helmet Data GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/motorcycle-helmet-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/motorcycle-helmet-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="motorcycle-helmet-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/motorcycle-helmet-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the motorcycle helmet accelerometer
data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-deep-gp">Motorcycle Helmet Data Deep
GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/motorcycle-helmet-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/motorcycle-helmet-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span></code></pre></div>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">1</span>, x.shape[<span class="dv">1</span>]]</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i)]</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>yhat, X<span class="op">=</span>x, </span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>                  inits<span class="op">=</span>inits, </span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels, <span class="co"># the kernels for each layer</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">20</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>m.initialize()</span></code></pre></div>
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>m.staged_optimize(iters<span class="op">=</span>(<span class="dv">1000</span>,<span class="dv">1000</span>,<span class="dv">10000</span>), messages<span class="op">=</span>(<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>))</span></code></pre></div>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-caption" class="caption-frame">
<p>Figure: Deep Gaussian process fit to the motorcycle helmet
accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-deep-gp-1">Motorcycle Helmet Data Deep
GP</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-samples-caption"
class="caption-frame">
<p>Figure: Samples from the deep Gaussian process as fitted to the
motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-latent-1">Motorcycle Helmet Data Latent
1</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-layer-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-layer-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-layer-0-caption"
class="caption-frame">
<p>Figure: Mappings from the input to the latent layer for the
motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-latent-2">Motorcycle Helmet Data Latent
2</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-layer-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-layer-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-layer-1-caption"
class="caption-frame">
<p>Figure: Mappings from the latent layer to the output layer for the
motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-pinball-plot">Motorcycle Helmet Pinball
Plot</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-pinball-caption"
class="caption-frame">
<p>Figure: Pinball plot for the mapping from input to output layer for
the motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="robot-wireless-data">Robot Wireless Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/robot-wireless-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/robot-wireless-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The robot wireless data is taken from an experiment run by Brian
Ferris at University of Washington. It consists of the measurements of
WiFi access point signal strengths as Brian walked in a loop. It was
published at IJCAI in 2007 <span class="citation"
data-cites="Ferris:wifi07">(Ferris et al., 2007)</span>.</p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>pods.datasets.robot_wireless()</span></code></pre></div>
<p>The ground truth is recorded in the data, the actual loop is given in
the plot below.</p>
<h2 id="robot-wireless-ground-truth">Robot Wireless Ground Truth</h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
<div id="robot-wireless-ground-truth-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-ground-truth&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-ground-truth-caption" class="caption-frame">
<p>Figure: Ground truth movement for the position taken while recording
the multivariate time-course of wireless access point signal
strengths.</p>
</div>
</div>
<p>We will ignore this ground truth in making our predictions, but see
if the model can recover something similar in one of the latent
layers.</p>
<h2 id="robot-wifi-data">Robot WiFi Data</h2>
<p>One challenge with the data is that the signal strength ‘drops out’.
This is because the device only tracks a limited number of wifi access
points, when one of the access points falls outside the track, the value
disappears (in the plot below it reads -0.5). The data is missing, but
it is not missing at random because the implication is that the wireless
access point must be weak to have dropped from the list of those that
are tracked.</p>
<div class="figure">
<div id="robot-wireless-data-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-dim-1.svg" width="60%" style=" ">
</object>
</div>
<div id="robot-wireless-data-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-data-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-data-dim-1-caption" class="caption-frame">
<p>Figure: Output dimension 1 from the robot wireless data. This plot
shows signal strength changing over time.</p>
</div>
</div>
<h2 id="gaussian-process-fit-to-robot-wireless-data">Gaussian Process
Fit to Robot Wireless Data</h2>
<p>Perform a Gaussian process fit on the data using GPy.</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<h2 id="robot-wifi-data-gp">Robot WiFi Data GP</h2>
<div class="figure">
<div id="robot-wireless-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/robot-wireless-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
<div id="robot-wireless-gp-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-gp-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-gp-dim-1-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Robot Wireless dimension 1.</p>
</div>
</div>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">2</span>, x.shape[<span class="dv">1</span>]]</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i, ARD<span class="op">=</span><span class="va">True</span>)]</span></code></pre></div>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>y, X<span class="op">=</span>x, inits<span class="op">=</span>inits, </span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels,</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">50</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>m.initialize()</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>m.staged_optimize(messages<span class="op">=</span>(<span class="va">True</span>,<span class="va">True</span>,<span class="va">True</span>))</span></code></pre></div>
<h2 id="robot-wifi-data-deep-gp">Robot WiFi Data Deep GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/robot-wireless-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/robot-wireless-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="robot-wireless-deep-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
<div id="robot-wireless-deep-gp-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-deep-gp-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-deep-gp-dim-1-caption" class="caption-frame">
<p>Figure: Fit of the deep Gaussian process to dimension 1 of the robot
wireless data.</p>
</div>
</div>
<h2 id="robot-wifi-data-deep-gp-1">Robot WiFi Data Deep GP</h2>
<div class="figure">
<div id="robot-wireless-deep-gp-samples-dim-1-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-samples-dim-1.svg" width="80%" style=" ">
</object>
</div>
<div id="robot-wireless-deep-gp-samples-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-deep-gp-samples-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-deep-gp-samples-dim-1-caption"
class="caption-frame">
<p>Figure: Samples from the deep Gaussian process fit to dimension 1 of
the robot wireless data.</p>
</div>
</div>
<h2 id="robot-wifi-data-latent-space">Robot WiFi Data Latent Space</h2>
<div class="figure">
<div id="robot-wireless-latent-space-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-latent-space.svg" width="60%" style=" ">
</object>
</div>
<div id="robot-wireless-latent-space-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-latent-space&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-latent-space-caption" class="caption-frame">
<p>Figure: Inferred two dimensional latent space from the model for the
robot wireless data.</p>
</div>
</div>
<h2 id="high-five-motion-capture-data">‘High Five’ Motion Capture
Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/high-five-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/high-five-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Motion capture data from the CMU motion capture data base <span
class="citation" data-cites="CMU-mocap03">(CMU Motion Capture Lab,
2003)</span>. It contains two subjects approaching each other and
executing a ‘high five’. The subjects are number 10 and 11 and their
motion numbers are 21.</p>
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.cmu_mocap_high_five()</span></code></pre></div>
<p>The data dictionary contains the keys ‘Y1’ and ‘Y2’, which represent
the motions of the two different subjects. Their skeleton files are
included in the keys ‘skel1’ and ‘skel2’.</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&#39;Y1&#39;</span>].shape</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&#39;Y2&#39;</span>].shape</span></code></pre></div>
<p>The data was used in the hierarchical GP-LVM paper <span
class="citation" data-cites="Lawrence:hgplvm07">(Lawrence and Moore,
2007)</span> in an experiment that was also recreated in the Deep
Gaussian process paper <span class="citation"
data-cites="Damianou:deepgp13">(Damianou and Lawrence, 2013)</span>.</p>
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data[<span class="st">&#39;citation&#39;</span>])</span></code></pre></div>
<p>And extra information about the data is included, as standard, under
the keys <code>info</code> and <code>details</code>.</p>
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data[<span class="st">&#39;info&#39;</span>])</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data[<span class="st">&#39;details&#39;</span>])</span></code></pre></div>
<h2 id="shared-lvm">Shared LVM</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/high-five-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/high-five-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="shared-latent-variable-model-graph-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//shared.svg" width="60%" style=" ">
</object>
</div>
<div id="shared-latent-variable-model-graph-magnify" class="magnify"
onclick="magnifyFigure(&#39;shared-latent-variable-model-graph&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="shared-latent-variable-model-graph-caption"
class="caption-frame">
<p>Figure: Shared latent variable model structure. Here two related data
sets are brought together with a set of latent variables that are
partially shared and partially specific to one of the data sets.</p>
</div>
</div>
<div class="figure">
<div id="deep-gp-high-five-figure" class="figure-frame">
<p><img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deep-gp-high-five2.png" width="80%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
</div>
<div id="deep-gp-high-five-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-gp-high-five&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-gp-high-five-caption" class="caption-frame">
<p>Figure: Latent spaces of the ‘high five’ data. The structure of the
model is automatically learnt. One of the latent spaces is coordinating
how the two figures walk together, the other latent spaces contain
latent variables that are specific to each of the figures
separately.</p>
</div>
</div>
<h2 id="subsample-of-the-mnist-data">Subsample of the MNIST Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We will look at a sub-sample of the MNIST digit data set.</p>
<p>First load in the MNIST data set from scikit learn. This can take a
little while because it’s large to download.</p>
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> fetch_openml(<span class="st">&#39;mnist_784&#39;</span>)</span></code></pre></div>
<p>Sub-sample the dataset to make the training faster.</p>
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>]</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>N_per_digit <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> []</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> []</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> digits:</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> mnist[<span class="st">&#39;data&#39;</span>][mnist[<span class="st">&#39;target&#39;</span>]<span class="op">==</span><span class="bu">str</span>(d)]</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    Y.append(imgs.loc[np.random.permutation(imgs.index)[:N_per_digit]])</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    labels.append(np.ones(N_per_digit)<span class="op">*</span>d)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.vstack(Y).astype(np.float64)</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.hstack(labels)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>Y <span class="op">/=</span> <span class="dv">255</span></span></code></pre></div>
<h2 id="fitting-a-deep-gp-to-a-the-mnist-digits-subsample">Fitting a
Deep GP to a the MNIST Digits Subsample</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/mnist-digits-subsample-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/mnist-digits-subsample-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Zhenwen Dai
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/zhenwen-dai.jpg" clip-path="url(#clip5)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip6">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/andreas-damianou.png" clip-path="url(#clip6)"/>
</svg>
</div>
<p>We now look at the deep Gaussian processes’ capacity to perform
unsupervised learning.</p>
<h2 id="fit-a-deep-gp">Fit a Deep GP</h2>
<p>We’re going to fit a Deep Gaussian process model to the MNIST data
with two hidden layers. Each of the two Gaussian processes (one from the
first hidden layer to the second, one from the second hidden layer to
the data) has an exponentiated quadratic covariance.</p>
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>num_latent <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>num_hidden_2 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP([Y.shape[<span class="dv">1</span>],num_hidden_2,num_latent],</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>                  Y,</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>[GPy.kern.RBF(num_hidden_2,ARD<span class="op">=</span><span class="va">True</span>), </span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>                           GPy.kern.RBF(num_latent,ARD<span class="op">=</span><span class="va">False</span>)], </span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">50</span>, back_constraint<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>                  encoder_dims<span class="op">=</span>[[<span class="dv">200</span>],[<span class="dv">200</span>]])</span></code></pre></div>
<h2 id="initialization">Initialization</h2>
<p>Just like deep neural networks, there are some tricks to
intitializing these models. The tricks we use here include some early
training of the model with model parameters constrained. This gives the
variational inducing parameters some scope to tighten the bound for the
case where the noise variance is small and the variances of the Gaussian
processes are around 1.</p>
<div class="sourceCode" id="cb56"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>m.obslayer.likelihood.variance[:] <span class="op">=</span> Y.var()<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    layer.kern.variance.fix(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    layer.likelihood.variance.fix(warning<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p>We now we optimize for a hundred iterations with the constrained
model.</p>
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">False</span>,max_iters<span class="op">=</span><span class="dv">100</span>)</span></code></pre></div>
<p>Now we remove the fixed constraint on the kernel variance parameters,
but keep the noise output constrained, and run for a further 100
iterations.</p>
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    layer.kern.variance.constrain_positive(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">False</span>,max_iters<span class="op">=</span><span class="dv">100</span>)</span></code></pre></div>
<p>Finally we unconstrain the layer likelihoods and allow the full model
to be trained for 1000 iterations.</p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    layer.likelihood.variance.constrain_positive(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">True</span>,max_iters<span class="op">=</span><span class="dv">10000</span>)</span></code></pre></div>
<h2 id="visualize-the-latent-space-of-the-top-layer">Visualize the
latent space of the top layer</h2>
<p>Now the model is trained, let’s plot the mean of the posterior
distributions in the top latent layer of the model.</p>
<div class="figure">
<div id="mnist-digits-subsample-latent-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-latent.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-latent-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-latent&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-latent-caption" class="caption-frame">
<p>Figure: Latent space for the deep Gaussian process learned through
unsupervised learning and fitted to a subset of the MNIST digits
subsample.</p>
</div>
</div>
<h2 id="visualize-the-latent-space-of-the-intermediate-layer">Visualize
the latent space of the intermediate layer</h2>
<p>We can also visualize dimensions of the intermediate layer. First the
lengthscale of those dimensions is given by</p>
<div class="sourceCode" id="cb60"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>m.obslayer.kern.lengthscale</span></code></pre></div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-1-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-2-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-3-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-4-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<h2 id="generate-from-model">Generate From Model</h2>
<p>Now we can take a look at a sample from the model, by drawing a
Gaussian random sample in the latent space and propagating it through
the model.</p>
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>t<span class="op">=</span>np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, rows<span class="op">*</span>cols)[:, <span class="va">None</span>]</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>,lengthscale<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> kern.K(t, t)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.multivariate_normal(np.zeros(rows<span class="op">*</span>cols), cov, num_latent).T</span></code></pre></div>
<div class="figure">
<div id="digit-samples-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/digit-samples-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="digit-samples-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;digit-samples-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="digit-samples-deep-gp-caption" class="caption-frame">
<p>Figure: These digits are produced by taking a tour of the two
dimensional latent space (as described by a Gaussian process sample) and
mapping the tour into the data space. We visualize the mean of the
mapping in the images.</p>
</div>
</div>
<h2 id="deep-health">Deep Health</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_health/includes/deep-health-model.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/deep-health-model.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-health-model-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deep-health.svg" width="70%" style=" ">
</object>
</div>
<div id="deep-health-model-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-health-model&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-health-model-caption" class="caption-frame">
<p>Figure: The deep health model uses different layers of abstraction in
the deep Gaussian process to represent information about diagnostics and
treatment to model interelationships between a patients different data
modalities.</p>
</div>
</div>
<p>From a machine learning perspective, we’d like to be able to
interrelate all the different modalities that are informative about the
state of the disease. For deep health, the notion is that the state of
the disease is appearing at the more abstract levels, as we descend the
model, we express relationships between the more abstract concept, that
sits within the physician’s mind, and the data we can measure.</p>
<h2 id="conclusions-1">Conclusions</h2>
<p>The probabilistic modelling community has evolved in an era where the
assumption was that ambiguous conclusions are best shared with a
(trained) professional through probabilities. Recent advances in
generative AI offer the possibility of machines that have a better
understanding of human subjective ambiguities and therefore machines
that can summarise information in a way that can be interogated rather
than just through a series of numbers.</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>book: <a
href="https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248">The
Atomic Human</a></li>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Hawking-history88" class="csl-entry" role="listitem">
A brief history of time, 1988. Bantam Dell Publishing Group, London.
</div>
<div id="ref-Ananthanarayanan-cat09" class="csl-entry" role="listitem">
Ananthanarayanan, R., Esser, S.K., Simon, H.D., Modha, D.S., 2009. The
cat is out of the bag: Cortical simulations with <span
class="math inline">\(10^9\)</span> neurons, <span
class="math inline">\(10^{13}\)</span> synapses, in: Proceedings of the
Conference on High Performance Computing Networking, Storage and
Analysis - SC ’09. <a
href="https://doi.org/10.1145/1654059.1654124">https://doi.org/10.1145/1654059.1654124</a>
</div>
<div id="ref-Arora-convergence19" class="csl-entry" role="listitem">
Arora, S., Cohen, N., Golowich, N., Hu, W., 2019. <a
href="https://openreview.net/forum?id=SkMQg3C5K7">A convergence analysis
of gradient descent for deep linear neural networks</a>, in:
International Conference on Learning Representations.
</div>
<div id="ref-Boltzmann-warmetheorie77" class="csl-entry"
role="listitem">
Boltzmann, L., n.d. Über die <span>B</span>eziehung zwischen dem zweiten
<span>H</span>auptsatze der mechanischen <span>W</span>armetheorie und
der <span>W</span>ahrscheinlichkeitsrechnung, respective den
<span>S</span>ätzen über das wärmegleichgewicht. Sitzungberichte der
Kaiserlichen Akademie der Wissenschaften. Mathematisch-Naturwissen
Classe. Abt. II LXXVI, 373–435.
</div>
<div id="ref-Bui:deep16" class="csl-entry" role="listitem">
Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., Turner, R.,
2016. <a href="http://proceedings.mlr.press/v48/bui16.html">Deep
<span>G</span>aussian processes for regression using approximate
expectation propagation</a>, in: Balcan, M.F., Weinberger, K.Q. (Eds.),
Proceedings of the 33rd International Conference on Machine Learning,
Proceedings of Machine Learning Research. PMLR, New York, New York, USA,
pp. 1472–1481.
</div>
<div id="ref-Cabrera-realworld23" class="csl-entry" role="listitem">
Cabrera, C., Paleyes, A., Thodoroff, P., Lawrence, N.D., 2023. <a
href="https://arxiv.org/abs/2302.04810">Real-world machine learning
systems: A survey from a data-oriented architecture perspective</a>.
</div>
<div id="ref-Cho:deep09" class="csl-entry" role="listitem">
Cho, Y., Saul, L.K., 2009. <a
href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel
methods for deep learning</a>, in: Bengio, Y., Schuurmans, D., Lafferty,
J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural
Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.
</div>
<div id="ref-CMU-mocap03" class="csl-entry" role="listitem">
CMU Motion Capture Lab, 2003. The <span>CMU</span> mocap database.
</div>
<div id="ref-Coales-yellow14" class="csl-entry" role="listitem">
Coales, J.F., Kane, S.J., 2014. The <span>“yellow peril”</span> and
after. IEEE Control Systems Magazine 34, 65–69. <a
href="https://doi.org/10.1109/MCS.2013.2287387">https://doi.org/10.1109/MCS.2013.2287387</a>
</div>
<div id="ref-Damianou:thesis2015" class="csl-entry" role="listitem">
Damianou, A., 2015. Deep <span>G</span>aussian processes and variational
propagation of uncertainty (PhD thesis). University of Sheffield.
</div>
<div id="ref-Damianou:deepgp13" class="csl-entry" role="listitem">
Damianou, A., Lawrence, N.D., 2013. Deep <span>G</span>aussian
processes. pp. 207–215.
</div>
<div id="ref-Damianou:variational15" class="csl-entry" role="listitem">
Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference
for latent variables and uncertain inputs in <span>G</span>aussian
processes. Journal of Machine Learning Research 17.
</div>
<div id="ref-DellaGatta:direct08" class="csl-entry" role="listitem">
Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D.,
Missero, C., Bernardo, D. di, 2008. Direct targets of the TRP63
transcription factor revealed by a combination of gene expression
profiling and reverse engineering. Genome Research 18, 939–948. <a
href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a>
</div>
<div id="ref-Dunlop:deep2017" class="csl-entry" role="listitem">
Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. <a
href="http://jmlr.org/papers/v19/18-015.html">How deep are deep
<span>G</span>aussian processes?</a> Journal of Machine Learning
Research 19, 1–46.
</div>
<div id="ref-Duvenaud:pathologies14" class="csl-entry" role="listitem">
Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding
pathologies in very deep networks.
</div>
<div id="ref-Eddington:nature29" class="csl-entry" role="listitem">
Eddington, A.S., 1929. The nature of the physical world. Dent (London).
<a
href="https://doi.org/10.2307/2180099">https://doi.org/10.2307/2180099</a>
</div>
<div id="ref-Ferris:wifi07" class="csl-entry" role="listitem">
Ferris, B.D., Fox, D., Lawrence, N.D., 2007. <span>WiFi-SLAM</span>
using <span>G</span>aussian process latent variable models, in: Veloso,
M.M. (Ed.), Proceedings of the 20th International Joint Conference on
Artificial Intelligence (IJCAI 2007). pp. 2480–2485.
</div>
<div id="ref-Havasi:deepgp18" class="csl-entry" role="listitem">
Havasi, M., Hernández-Lobato, J.M., Murillo-Fuentes, J.J., 2018. <a
href="http://papers.nips.cc/paper/7979-inference-in-deep-gaussian-processes-using-stochastic-gradient-hamiltonian-monte-carlo.pdf">Inference
in deep <span>G</span>aussian processes using stochastic gradient
<span>H</span>amiltonian <span>M</span>onte <span>C</span>arlo</a>, in:
Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,
Garnett, R. (Eds.), Advances in Neural Information Processing Systems
31. Curran Associates, Inc., pp. 7506–7516.
</div>
<div id="ref-Heider:interpersonal58" class="csl-entry" role="listitem">
Heider, F., 1958. The psychology of interpersonal relations. John Wiley.
</div>
<div id="ref-Heider-experimental44" class="csl-entry" role="listitem">
Heider, F., Simmel, M., 1944. An experimental study of apparent
behavior. The American Journal of Psychology 57, 243–259. <a
href="https://doi.org/10.2307/1416950">https://doi.org/10.2307/1416950</a>
</div>
<div id="ref-Joseph-origins21" class="csl-entry" role="listitem">
Henrich, J., Muthukrishna, M., 2021. The origins and psychology of human
cooperation. Annual Review of Psychology 72, 207–240. <a
href="https://doi.org/10.1146/annurev-psych-081920-042106">https://doi.org/10.1146/annurev-psych-081920-042106</a>
</div>
<div id="ref-Huang-inner22" class="csl-entry" role="listitem">
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng,
A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T.,
Brown, N., Luu, L., Levine, S., Hausman, K., ichter, brian, 2023. <a
href="https://proceedings.mlr.press/v205/huang23c.html">Inner monologue:
Embodied reasoning through planning with language models</a>, in: Liu,
K., Kulic, D., Ichnowski, J. (Eds.), Proceedings of the 6th Conference
on Robot Learning, Proceedings of Machine Learning Research. PMLR, pp.
1769–1782.
</div>
<div id="ref-Izmailov:subspace19" class="csl-entry" role="listitem">
Izmailov, P., Maddox, W.J., Kirichenko, P., Garipov, T., Vetrov, D.P.,
Wilson, A.G., 2019. <a href="http://arxiv.org/abs/1907.07504">Subspace
inference for bayesian deep learning</a>. CoRR abs/1907.07504.
</div>
<div id="ref-Jacot-deeplinear21" class="csl-entry" role="listitem">
Jacot, A., Ged, F., Gabriel, F., Şimşek, B., Hongler, C., 2021. <a
href="https://arxiv.org/abs/2106.15933">Deep linear networks dynamics:
Low-rank biases induced by initialization scale and L2
regularization</a>.
</div>
<div id="ref-Kalaitzis:simple11" class="csl-entry" role="listitem">
Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking
differentially expressed gene expression time courses through
<span>Gaussian</span> process regression. BMC Bioinformatics 12. <a
href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a>
</div>
<div id="ref-Laplace-essai14" class="csl-entry" role="listitem">
Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.
Courcier, Paris.
</div>
<div id="ref-Lawrence-atomic24" class="csl-entry" role="listitem">
Lawrence, N.D., 2024. The atomic human: Understanding ourselves in the
age of AI. Allen Lane.
</div>
<div id="ref-Lawrence:embodiment17" class="csl-entry" role="listitem">
Lawrence, N.D., 2017. <a href="https://arxiv.org/abs/1705.07996">Living
together: Mind and machine intelligence</a>. arXiv.
</div>
<div id="ref-Lawrence:hgplvm07" class="csl-entry" role="listitem">
Lawrence, N.D., Moore, A.J., 2007. Hierarchical <span>G</span>aussian
process latent variable models. pp. 481–488.
</div>
<div id="ref-MacKay:gpintroduction98" class="csl-entry" role="listitem">
MacKay, D.J.C., n.d. Introduction to <span>G</span>aussian processes.
pp. 133–166.
</div>
<div id="ref-Mackay-behind91" class="csl-entry" role="listitem">
MacKay, D.M., 1991. Behind the eye. Basil Blackwell.
</div>
<div id="ref-Mikhailov:hydrodynamica05" class="csl-entry"
role="listitem">
Mikhailov, G.K., n.d. Daniel bernoulli, hydrodynamica (1738).
</div>
<div id="ref-ONeill-trust02" class="csl-entry" role="listitem">
O’Neill, O., 2002. A question of trust. Cambridge University Press.
</div>
<div id="ref-Reed-information98" class="csl-entry" role="listitem">
Reed, C., Durlach, N.I., 1998. Note on information transfer rates in
human communication. Presence Teleoperators &amp; Virtual Environments
7, 509–518. <a
href="https://doi.org/10.1162/105474698565893">https://doi.org/10.1162/105474698565893</a>
</div>
<div id="ref-Salimbeni:doubly2017" class="csl-entry" role="listitem">
Salimbeni, H., Deisenroth, M., 2017. <a
href="http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf">Doubly
stochastic variational inference for deep <span>G</span>aussian
processes</a>, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H.,
Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural
Information Processing Systems 30. Curran Associates, Inc., pp.
4591–4602.
</div>
<div id="ref-Kim-translation15" class="csl-entry" role="listitem">
Sharp, K., Matschinsky, F., 2015. Translation of <span>L</span>udwig
<span>B</span>oltzmann’s paper <span>“on the relationship between the
second fundamental theorem of the mechanical theory of heat and
probability calculations regarding the conditions for thermal
equilibrium.”</span> Entropy 17, 1971–2009. <a
href="https://doi.org/10.3390/e17041971">https://doi.org/10.3390/e17041971</a>
</div>
<div id="ref-Susskind-future15" class="csl-entry" role="listitem">
Susskind, R.E., Susskind, D., 2015. The future of the professions: How
technology will transform the work of human experts. Oxford University
Press.
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Admiralty-gunnery45" class="csl-entry" role="listitem">
The Admiralty, 1945. <a href="https://www.maritime.org/doc/br224/">The
gunnery pocket book, b.r. 224/45</a>.
</div>
<div id="ref-Thompson-juries89" class="csl-entry" role="listitem">
Thompson, W.C., 1989. <a href="http://www.jstor.org/stable/1191906">Are
juries competent to evaluate statistical evidence?</a> Law and
Contemporary Problems 52, 9–41.
</div>
<div id="ref-Wiener-exprodigy53" class="csl-entry" role="listitem">
Wiener, N., 1953. Ex-prodigy: My childhood and youth. mitp, Cambridge,
MA.
</div>
<div id="ref-Wiener:yellow49" class="csl-entry" role="listitem">
Wiener, N., 1949. The extrapolation, interpolation and smoothing of
stationary time series with engineering applications. wiley.
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>the challenge of understanding what information pertains
to is known as knowledge representation.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The quote is reported in the <em>Manchester
Guardian</em> on 29th June 1892. See also <a
href="https://www.york.ac.uk/depts/maths/histstat/lies.htm"
class="uri">https://www.york.ac.uk/depts/maths/histstat/lies.htm</a>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Although Twain attributes Disraeli in this way there’s
<a
href="https://en.wikipedia.org/wiki/Lies,_damned_lies,_and_statistics">no
record of him having said this.</a>.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>monographs<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>monographs<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

