{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Atomic Human\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge\n",
    "\n",
    "### 2024-03-12"
   ],
   "id": "4361cb9c-d298-48a3-b728-5c7c35494d48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: A vital perspective is missing from the discussions we’re\n",
    "having about Artificial Intelligence: what does it mean for our\n",
    "identity?\n",
    "\n",
    "Our fascination with AI stems from the perceived uniqueness of human\n",
    "intelligence. We believe it’s what differentiates us. Fears of AI not\n",
    "only concern how it invades our digital lives, but also the implied\n",
    "threat of an intelligence that displaces us from our position at the\n",
    "centre of the world.\n",
    "\n",
    "Atomism, proposed by Democritus, suggested it was impossible to continue\n",
    "dividing matter down into ever smaller components: eventually we reach a\n",
    "point where a cut cannot be made (the Greek for uncuttable is ‘atom’).\n",
    "In the same way, by slicing away at the facets of human intelligence\n",
    "that can be replaced by machines, AI uncovers what is left: an\n",
    "indivisible core that is the essence of humanity.\n",
    "\n",
    "By contrasting our own (evolved, locked-in, embodied) intelligence with\n",
    "the capabilities of machine intelligence through history, The Atomic\n",
    "Human reveals the technical origins, capabilities and limitations of AI\n",
    "systems, and how they should be wielded. Not just by the experts, but\n",
    "ordinary people. Either AI is a tool for us, or we become a tool of AI.\n",
    "Understanding this will enable us to choose the future we want.\n",
    "\n",
    "This talk is based on Neil’s forthcoming book to be published with Allen\n",
    "Lane in June 2024. Machine learning solutions, in particular those based\n",
    "on deep learning methods, form an underpinning of the current revolution\n",
    "in “artificial intelligence” that has dominated popular press headlines\n",
    "and is having a significant influence on the wider tech agenda.\n",
    "\n",
    "In this talk I will give an overview of where we are now with machine\n",
    "learning solutions, and what challenges we face both in the near and far\n",
    "future. These include practical application of existing algorithms in\n",
    "the face of the need to explain decision making, mechanisms for\n",
    "improving the quality and availability of data, dealing with large\n",
    "unstructured datasets."
   ],
   "id": "30fc5c45-b0e2-4ec7-b2bd-957d9bf1c0ee"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ],
   "id": "d63527af-24cb-4cdc-9099-ab54534bbae6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ],
   "id": "37451010-b861-493d-8008-b32d93d3ac30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Atomic Human\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-human.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-human.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//books/the-atomic-human.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>[The Atomic Human](https://www.amazon.co.uk/dp/B0CGZHBSLL)\n",
    "(Lawrence, 2024) due for release in June 2024.</i>"
   ],
   "id": "5af77b51-912f-433d-97c0-820c752fd26f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Henry Ford’s Faster Horse\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/henry-ford-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/henry-ford-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/1925_Ford_Model_T_touring.jpg\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>A 1925 Ford Model T built at Henry Ford’s Highland Park Plant\n",
    "in Dearborn, Michigan. This example now resides in Australia, owned by\n",
    "the founder of FordModelT.net. From\n",
    "<https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg></i>\n",
    "\n",
    "It’s said that Henry Ford’s customers wanted a “a faster horse”. If\n",
    "Henry Ford was selling us artificial intelligence today, what would the\n",
    "customer call for, “a smarter human”? That’s certainly the picture of\n",
    "machine intelligence we find in science fiction narratives, but the\n",
    "reality of what we’ve developed is much more mundane.\n",
    "\n",
    "Car engines produce prodigious power from petrol. Machine intelligences\n",
    "deliver decisions derived from data. In both cases the scale of\n",
    "consumption enables a speed of operation that is far beyond the\n",
    "capabilities of their natural counterparts. Unfettered energy\n",
    "consumption has consequences in the form of climate change. Does\n",
    "unbridled data consumption also have consequences for us?\n",
    "\n",
    "If we devolve decision making to machines, we depend on those machines\n",
    "to accommodate our needs. If we don’t understand how those machines\n",
    "operate, we lose control over our destiny. Our mistake has been to see\n",
    "machine intelligence as a reflection of our intelligence. We cannot\n",
    "understand the smarter human without understanding the human. To\n",
    "understand the machine, we need to better understand ourselves."
   ],
   "id": "b1e51736-675e-411d-ac09-407941074998"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embodiment Factors\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg\" class=\"\" width=\"15%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//human.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "bits/min\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "billions\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "2,000\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "billion <br>calculations/s\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "~100\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "a billion\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "embodiment\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "20 minutes\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "5 billion years\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Embodiment factors are the ratio between our ability to\n",
    "compute and our ability to communicate. Relative to the machine we are\n",
    "also locked in. In the table we represent embodiment as the length of\n",
    "time it would take to communicate one second’s worth of computation. For\n",
    "computers it is a matter of minutes, but for a human, it is a matter of\n",
    "thousands of millions of years. See also “Living Together: Mind and\n",
    "Machine Intelligence” Lawrence (2017)</i>\n",
    "\n",
    "There is a fundamental limit placed on our intelligence based on our\n",
    "ability to communicate. Claude Shannon founded the field of information\n",
    "theory. The clever part of this theory is it allows us to separate our\n",
    "measurement of information from what the information pertains to.[1]\n",
    "\n",
    "Shannon measured information in bits. One bit of information is the\n",
    "amount of information I pass to you when I give you the result of a coin\n",
    "toss. Shannon was also interested in the amount of information in the\n",
    "English language. He estimated that on average a word in the English\n",
    "language contains 12 bits of information.\n",
    "\n",
    "Given typical speaking rates, that gives us an estimate of our ability\n",
    "to communicate of around 100 bits per second (Reed and Durlach, 1998).\n",
    "Computers on the other hand can communicate much more rapidly. Current\n",
    "wired network speeds are around a billion bits per second, ten million\n",
    "times faster.\n",
    "\n",
    "When it comes to compute though, our best estimates indicate our\n",
    "computers are slower. A typical modern computer can process make around\n",
    "100 billion floating-point operations per second, each floating-point\n",
    "operation involves a 64 bit number. So the computer is processing around\n",
    "6,400 billion bits per second.\n",
    "\n",
    "It’s difficult to get similar estimates for humans, but by some\n",
    "estimates the amount of compute we would require to *simulate* a human\n",
    "brain is equivalent to that in the UK’s fastest computer\n",
    "(Ananthanarayanan et al., 2009), the MET office machine in Exeter, which\n",
    "in 2018 ranked as the 11th fastest computer in the world. That machine\n",
    "simulates the world’s weather each morning, and then simulates the\n",
    "world’s climate in the afternoon. It is a 16-petaflop machine,\n",
    "processing around 1,000 *trillion* bits per second.\n",
    "\n",
    "[1] the challenge of understanding what information pertains to is known\n",
    "as knowledge representation."
   ],
   "id": "d20a8c3f-1a03-4ae6-adee-1b504b4ed25e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revolution\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/cuneiform.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/cuneiform.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Arguably the information revolution we are experiencing is unprecedented\n",
    "in history. But changes in the way we share information have a long\n",
    "history. Over 5,000 years ago in the city of Uruk, on the banks of the\n",
    "Euphrates, communities which relied on the water to irrigate their corps\n",
    "developed an approach to recording transactions in clay. Eventually the\n",
    "system of recording system became sophisticated enough that their oral\n",
    "histories could be recorded in the form of the first epic: Gilgamesh.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//cuneiform/chicago-cuneiform-stone.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Chicago Stone, side 2, recording sale of a number of fields,\n",
    "probably from Isin, Early Dynastic Period, c. 2600 BC, black basalt</i>\n",
    "\n",
    "It was initially develoepd for people as a recordd of who owed what to\n",
    "whom, expanding individuals’ capacity to remember. But over a five\n",
    "hundred year period writing evolved to become a tool for literature as\n",
    "well. More pithily put, writing was invented by accountants not poets\n",
    "(see e.g. [this piece by Tim\n",
    "Harford](https://www.bbc.co.uk/news/business-39870485)).\n",
    "\n",
    "In some respects today’s revolution is different, because it involves\n",
    "also the creation of stories as well as their curation. But in some\n",
    "fundamental ways we can see what we have produced as another tool for us\n",
    "in the information revolution.\n",
    "\n",
    "<!-- Faster horse -->\n",
    "<!-- Embodiment Factors -->"
   ],
   "id": "1327e85f-0549-4fef-b899-9322bcb6b669"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information and Embodiment\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-celsius.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-celsius.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ClaudeShannon_MFO3807.jpg\" style=\"width:40%\">\n",
    "\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "*Claude Shannon*\n",
    "\n",
    "</center>\n",
    "\n",
    "Figure: <i>Claude Shannon (1916-2001)</i>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg\" class=\"\" width=\"15%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//human.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "bits/min\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "billions\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "2,000\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "billion <br>calculations/s\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "~100\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "a billion\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "embodiment\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "20 minutes\n",
    "\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "\n",
    "5 billion years\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Embodiment factors are the ratio between our ability to\n",
    "compute and our ability to communicate. Relative to the machine we are\n",
    "also locked in. In the table we represent embodiment as the length of\n",
    "time it would take to communicate one second’s worth of computation. For\n",
    "computers it is a matter of minutes, but for a human, it is a matter of\n",
    "thousands of millions of years.</i>\n",
    "\n",
    "<!-- Information Triangle -->"
   ],
   "id": "0135fe95-47ab-473e-a821-9fa28b1d07a7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Flow of Information\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Classically the field of statistics focused on mediating the\n",
    "relationship between the machine and the human. Our limited bandwidth of\n",
    "communication means we tend to over-interpret the limited information\n",
    "that we are given, in the extreme we assign motives and desires to\n",
    "inanimate objects (a process known as anthropomorphizing). Much of\n",
    "mathematical statistics was developed to help temper this tendency and\n",
    "understand when we are valid in drawing conclusions from data.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information003.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The trinity of human, data, and computer, and highlights the\n",
    "modern phenomenon. The communication channel between computer and data\n",
    "now has an extremely high bandwidth. The channel between human and\n",
    "computer and the channel between data and human is narrow. New direction\n",
    "of information flow, information is reaching us mediated by the\n",
    "computer. The focus on classical statistics reflected the importance of\n",
    "the direct communication between human and data. The modern challenges\n",
    "of data science emerge when that relationship is being mediated by the\n",
    "machine.</i>\n",
    "\n",
    "Data science brings new challenges. In particular, there is a very large\n",
    "bandwidth connection between the machine and data. This means that our\n",
    "relationship with data is now commonly being mediated by the machine.\n",
    "Whether this is in the acquisition of new data, which now happens by\n",
    "happenstance rather than with purpose, or the interpretation of that\n",
    "data where we are increasingly relying on machines to summarize what the\n",
    "data contains. This is leading to the emerging field of data science,\n",
    "which must not only deal with the same challenges that mathematical\n",
    "statistics faced in tempering our tendency to over interpret data but\n",
    "must also deal with the possibility that the machine has either\n",
    "inadvertently or maliciously misrepresented the underlying data."
   ],
   "id": "234e6e4e-3631-49d9-98f9-1d000b265097"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Societal Effects\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_data-science/includes/societal-effects.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/societal-effects.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "We have already seen the effects of this changed dynamic in biology and\n",
    "computational biology. Improved sensorics have led to the new domains of\n",
    "transcriptomics, epigenomics, and ‘rich phenomics’ as well as\n",
    "considerably augmenting our capabilities in genomics.\n",
    "\n",
    "Biologists have had to become data-savvy, they require a rich\n",
    "understanding of the available data resources and need to assimilate\n",
    "existing data sets in their hypothesis generation as well as their\n",
    "experimental design. Modern biology has become a far more quantitative\n",
    "science, but the quantitativeness has required new methods developed in\n",
    "the domains of *computational biology* and *bioinformatics*.\n",
    "\n",
    "There is also great promise for personalized health, but in health the\n",
    "wide data-sharing that has underpinned success in the computational\n",
    "biology community is much harder to carry out.\n",
    "\n",
    "We can expect to see these phenomena reflected in wider society.\n",
    "Particularly as we make use of more automated decision making based only\n",
    "on data. This is leading to a requirement to better understand our own\n",
    "subjective biases to ensure that the human to computer interface allows\n",
    "domain experts to assimilate data driven conclusions in a well\n",
    "calibrated manner. This is particularly important where medical\n",
    "treatments are being prescribed. It also offers potential for different\n",
    "kinds of medical intervention. More subtle interventions are possible\n",
    "when the digital environment is able to respond to users in an bespoke\n",
    "manner. This has particular implications for treatment of mental health\n",
    "conditions.\n",
    "\n",
    "The main phenomenon we see across the board is the shift in dynamic from\n",
    "the direct pathway between human and data, as traditionally mediated by\n",
    "classical statistics, to a new flow of information via the computer.\n",
    "This change of dynamics gives us the modern and emerging domain of *data\n",
    "science*, where the interactions between human and data are mediated by\n",
    "the machine.\n",
    "\n",
    "<!-- AI Fallacy -->"
   ],
   "id": "663b645c-69a1-4a0d-9ccb-b34093899784"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Great AI Fallacy\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/the-great-ai-fallacy.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/the-great-ai-fallacy.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "There is a lot of variation in the use of the term artificial\n",
    "intelligence. I’m sometimes asked to define it, but depending on whether\n",
    "you’re speaking to a member of the public, a fellow machine learning\n",
    "researcher, or someone from the business community, the sense of the\n",
    "term differs.\n",
    "\n",
    "However, underlying its use I’ve detected one disturbing trend. A trend\n",
    "I’m beginining to think of as “The Great AI Fallacy”.\n",
    "\n",
    "The fallacy is associated with an implicit promise that is embedded in\n",
    "many statements about Artificial Intelligence. Artificial Intelligence,\n",
    "as it currently exists, is merely a form of automated decision making.\n",
    "The implicit promise of Artificial Intelligence is that it will be the\n",
    "first wave of automation where the machine adapts to the human, rather\n",
    "than the human adapting to the machine.\n",
    "\n",
    "How else can we explain the suspension of sensible business judgment\n",
    "that is accompanying the hype surrounding AI?\n",
    "\n",
    "This fallacy is particularly pernicious because there are serious\n",
    "benefits to society in deploying this new wave of data-driven automated\n",
    "decision making. But the AI Fallacy is causing us to suspend our\n",
    "calibrated skepticism that is needed to deploy these systems safely and\n",
    "efficiently.\n",
    "\n",
    "The problem is compounded because many of the techniques that we’re\n",
    "speaking of were originally developed in academic laboratories in\n",
    "isolation from real-world deployment.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/Jeeves_in_the_Springtime_01.jpg\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>We seem to have fallen for a perspective on AI that suggests\n",
    "it will adapt to our schedule, rather in the manner of a 1930s\n",
    "manservant.</i>\n",
    "\n",
    "<!-- Mathematical Statistics -->"
   ],
   "id": "18be7655-4807-4558-b92b-75ccd1b95a9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lies and Damned Lies\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_data-science/includes/lies-damned-lies.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/lies-damned-lies.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "> There are three types of lies: lies, damned lies and statistics\n",
    ">\n",
    "> Arthur Balfour 1848-1930\n",
    "\n",
    "Arthur Balfour was quoting the lawyer James Munro[1] when he said that\n",
    "there three types of lies: lies, damned lies and statistics in 1892.\n",
    "This is 20 years before the first academic department of applied\n",
    "statistics was founded at UCL. If Balfour were alive today, it is likely\n",
    "that he’d rephrase his quote:\n",
    "\n",
    "> There are three types of lies, lies damned lies and *big data*.\n",
    "\n",
    "Why? Because the challenges of understanding and interpreting big data\n",
    "today are similar to those that Balfour (who was a Conservative\n",
    "politician and statesman and would later become Prime Minister) faced in\n",
    "governing an empire through statistics in the latter part of the 19th\n",
    "century.\n",
    "\n",
    "The quote lies, damned lies and statistics was also credited to Benjamin\n",
    "Disraeli by Mark Twain in Twain’s autobiography.[2] It characterizes the\n",
    "idea that statistic can be made to prove anything. But Disraeli died in\n",
    "1881 and Mark Twain died in 1910. The important breakthrough in\n",
    "overcoming our tendency to over-interpet data came with the\n",
    "formalization of the field through the development of *mathematical\n",
    "statistics*.\n",
    "\n",
    "Data has an elusive quality, it promises so much but can deliver little,\n",
    "it can mislead and misrepresent. To harness it, it must be tamed. In\n",
    "Balfour and Disraeli’s time during the second half of the 19th century,\n",
    "numbers and data were being accumulated, the social sciences were being\n",
    "developed. There was a large-scale collection of data for the purposes\n",
    "of government.\n",
    "\n",
    "The modern ‘big data era’ is on the verge of delivering the same sense\n",
    "of frustration that Balfour experienced, the early promise of big data\n",
    "as a panacea is evolving to demands for delivery. For me, personally,\n",
    "peak-hype coincided with an email I received inviting collaboration on a\n",
    "project to deploy “*Big Data* and *Internet of Things* in an *Industry\n",
    "4.0* environment”. Further questioning revealed that the actual project\n",
    "was optimization of the efficiency of a manufacturing production line, a\n",
    "far more tangible and *realizable* goal.\n",
    "\n",
    "The antidote to this verbiage is found in increasing awareness. When\n",
    "dealing with data the first trap to avoid is the games of buzzword bingo\n",
    "that we are wont to play. The first goal is to quantify what challenges\n",
    "can be addressed and what techniques are required. Behind the hype\n",
    "fundamentals are changing. The phenomenon is about the increasing access\n",
    "we have to data. The way customers’ information is recorded and\n",
    "processes are codified and digitized with little overhead. Internet of\n",
    "things is about the increasing number of cheap sensors that can be\n",
    "easily interconnected through our modern network structures. But\n",
    "businesses are about making money, and these phenomena need to be recast\n",
    "in those terms before their value can be realized.\n",
    "\n",
    "For more thoughts on the challenges that statistics brings see Chapter 8\n",
    "of Lawrence (2024).\n",
    "\n",
    "[1] The quote is reported in the *Manchester Guardian* on 29th June\n",
    "1892. See also <https://www.york.ac.uk/depts/maths/histstat/lies.htm>.\n",
    "\n",
    "[2] Although Twain attributes Disraeli in this way there’s [no record of\n",
    "him having said\n",
    "this.](https://en.wikipedia.org/wiki/Lies,_damned_lies,_and_statistics)."
   ],
   "id": "bd073519-d4af-4f13-b20f-997dee9b56bb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Mathematical* Statistics\n",
    "\n",
    "[Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) (1857-1936),\n",
    "[Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) (1890-1962)\n",
    "and others considered the question of what conclusions can truly be\n",
    "drawn from data. Their mathematical studies act as a restraint on our\n",
    "tendency to over-interpret and see patterns where there are none. They\n",
    "introduced concepts such as randomized control trials that form a\n",
    "mainstay of our decision making today, from government, to clinicians to\n",
    "large scale A/B testing that determines the nature of the web interfaces\n",
    "we interact with on social media and shopping.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//Portrait_of_Karl_Pearson.jpg\" style=\"width:30%\">\n",
    "\n",
    "Figure: <i>Karl Pearson (1857-1936), one of the founders of Mathematical\n",
    "Statistics.</i>\n",
    "\n",
    "Their movement did the most to put statistics to rights, to eradicate\n",
    "the ‘damned lies’. It was known as [‘mathematical\n",
    "statistics’](https://en.wikipedia.org/wiki/Mathematical_statistics).\n",
    "Today I believe we should look to the emerging field of *data science*\n",
    "to provide the same role. Data science is an amalgam of statistics, data\n",
    "mining, computer systems, databases, computation, machine learning and\n",
    "artificial intelligence. Spread across these fields are the tools we\n",
    "need to realize data’s potential. For many businesses this might be\n",
    "thought of as the challenge of ‘converting bits into atoms’. Bits: the\n",
    "data stored on computer, atoms: the physical manifestation of what we\n",
    "do; the transfer of goods, the delivery of service. From fungible to\n",
    "tangible. When solving a challenge through data there are a series of\n",
    "obstacles that need to be addressed.\n",
    "\n",
    "Firstly, data awareness: what data you have and where its stored.\n",
    "Sometimes this includes changing your conception of what data is and how\n",
    "it can be obtained. From automated production lines to apps on employee\n",
    "smart phones. Often data is locked away: manual logbooks, confidential\n",
    "data, personal data. For increasing awareness an internal audit can\n",
    "help. The website [data.gov.uk](https://data.gov.uk/) hosts data made\n",
    "available by the UK government. To create this website the government’s\n",
    "departments went through an audit of what data they each hold and what\n",
    "data they could make available. Similarly, within private businesses\n",
    "this type of audit could be useful for understanding their internal\n",
    "digital landscape: after all the key to any successful campaign is a\n",
    "good map.\n",
    "\n",
    "Secondly, availability. How well are the data sources interconnected?\n",
    "How well curated are they? The curse of Disraeli was associated with\n",
    "unreliable data and *unreliable statistics*. The misrepresentations this\n",
    "leads to are worse than the absence of data as they give a false sense\n",
    "of confidence to decision making. Understanding how to avoid these\n",
    "pitfalls involves an improved sense of data and its value, one that\n",
    "needs to permeate the organization.\n",
    "\n",
    "The final challenge is analysis, the accumulation of the necessary\n",
    "expertise to digest what the data tells us. Data requires\n",
    "interpretation, and interpretation requires experience. Analysis is\n",
    "providing a bottleneck due to a skill shortage, a skill shortage made\n",
    "more acute by the fact that, ideally, analysis should be carried out by\n",
    "individuals not only skilled in data science but also equipped with the\n",
    "domain knowledge to understand the implications in a given application,\n",
    "and to see opportunities for improvements in efficiency."
   ],
   "id": "33c9cf79-7dfe-49fc-9c52-6102b83feb02"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‘Mathematical Data Science’\n",
    "\n",
    "As a term ‘big data’ promises much and delivers little, to get true\n",
    "value from data, it needs to be curated and evaluated. The three stages\n",
    "of awareness, availability and analysis provide a broad framework\n",
    "through which organizations should be assessing the potential in the\n",
    "data they hold. Hand waving about big data solutions will not do, it\n",
    "will only lead to self-deception. The castles we build on our data\n",
    "landscapes must be based on firm foundations, process and scientific\n",
    "analysis. If we do things right, those are the foundations that will be\n",
    "provided by the new field of data science.\n",
    "\n",
    "Today the statement “There are three types of lies: lies, damned lies\n",
    "and ‘big data’” may be more apt. We are revisiting many of the mistakes\n",
    "made in interpreting data from the 19th century. Big data is laid down\n",
    "by happenstance, rather than actively collected with a particular\n",
    "question in mind. That means it needs to be treated with care when\n",
    "conclusions are being drawn. For data science to succeed it needs the\n",
    "same form of rigor that Pearson and Fisher brought to statistics, a\n",
    "“mathematical data science” is needed.\n",
    "\n",
    "You can also check my blog post on [Lies, Damned Lies and Big\n",
    "Data](http://inverseprobability.com/2016/11/19/lies-damned-lies-big-data).\n",
    "\n",
    "<!-- Conversation -->"
   ],
   "id": "91f04c14-7afb-469b-9b45-629a124e245f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Communication\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "For human conversation to work, we require an internal model of who we\n",
    "are speaking to. We model each other, and combine our sense of who they\n",
    "are, who they think we are, and what has been said. This is our approach\n",
    "to dealing with the limited bandwidth connection we have. Empathy and\n",
    "understanding of intent. Mental dispositional concepts are used to\n",
    "augment our limited communication bandwidth.\n",
    "\n",
    "Fritz Heider referred to the important point of a conversation as being\n",
    "that they are happenings that are “*psychologically represented* in each\n",
    "of the participants” (his emphasis) (Heider, 1958)."
   ],
   "id": "41428518-718b-4775-bd23-589c4ad09726"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandwidth Constrained Conversations"
   ],
   "id": "dda88a96-1c47-4d95-92c2-005cead16f1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "ec9d401d-74fc-47e8-9c33-3e3c203b823b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "b56d253b-60e6-4bf3-86f7-e53808043ba0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('anne-bob-conversation{sample:0>3}.svg', \n",
    "                            'https://inverseprobability.com/talks/../slides/diagrams/',  sample=IntSlider(0, 0, 7, 1))"
   ],
   "id": "9ab91899-f7d5-4486-820a-1eb6fffca00c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation006.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Conversation relies on internal models of other\n",
    "individuals.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation007.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Misunderstanding of context and who we are talking to leads\n",
    "to arguments.</i>\n",
    "\n",
    "Embodiment factors imply that, in our communication between humans, what\n",
    "is *not* said is, perhaps, more important than what is said. To\n",
    "communicate with each other we need to have a model of who each of us\n",
    "are.\n",
    "\n",
    "To aid this, in society, we are required to perform roles. Whether as a\n",
    "parent, a teacher, an employee or a boss. Each of these roles requires\n",
    "that we conform to certain standards of behaviour to facilitate\n",
    "communication between ourselves.\n",
    "\n",
    "Control of self is vitally important to these communications.\n",
    "\n",
    "The high availability of data available to humans undermines\n",
    "human-to-human communication channels by providing new routes to\n",
    "undermining our control of self.\n",
    "\n",
    "<!-- Fritz Heider -->"
   ],
   "id": "ce0b9a54-0baf-441a-a32f-0e6787f543b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heider and Simmel (1944)\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/heider-simmel.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/heider-simmel.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "1e82a361-fe85-4846-aa6b-608c2b46ad5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('8FIEZXMUM2I')"
   ],
   "id": "9fce38e5-abc6-475c-9ab4-0671ec44b206"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Fritz Heider and Marianne Simmel’s video of shapes from\n",
    "Heider and Simmel (1944).</i>\n",
    "\n",
    "[Fritz Heider](https://en.wikipedia.org/wiki/Fritz_Heider) and [Marianne\n",
    "Simmel](https://en.wikipedia.org/wiki/Marianne_Simmel)’s experiments\n",
    "with animated shapes from 1944 (Heider and Simmel, 1944). Our\n",
    "interpretation of these objects as showing motives and even emotion is a\n",
    "combination of our desire for narrative, a need for understanding of\n",
    "each other, and our ability to empathize. At one level, these are\n",
    "crudely drawn objects, but in another way, the animator has communicated\n",
    "a story through simple facets such as their relative motions, their\n",
    "sizes and their actions. We apply our psychological representations to\n",
    "these faceless shapes to interpret their actions.\n",
    "\n",
    "See also a recent review paper on Human Cooperation by Henrich and\n",
    "Muthukrishna (2021)."
   ],
   "id": "40d78246-9e4d-4322-b322-04beaeed71a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Six Word Novel\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/baby-shoes.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/baby-shoes.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//Classic_baby_shoes.jpg\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Consider the six-word novel, apocryphally credited to Ernest\n",
    "Hemingway, “For sale: baby shoes, never worn”. To understand what that\n",
    "means to a human, you need a great deal of additional context. Context\n",
    "that is not directly accessible to a machine that has not got both the\n",
    "evolved and contextual understanding of our own condition to realize\n",
    "both the implication of the advert and what that implication means\n",
    "emotionally to the previous owner.</i>\n",
    "\n",
    "But this is a very different kind of intelligence than ours. A computer\n",
    "cannot understand the depth of the Ernest Hemingway’s apocryphal\n",
    "six-word novel: “For Sale, Baby Shoes, Never worn”, because it isn’t\n",
    "equipped with that ability to model the complexity of humanity that\n",
    "underlies that statement.\n",
    "\n",
    "<!-- Conversation LLM -->"
   ],
   "id": "aa24c9b1-756e-4403-b398-9e9b3ed11dcc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Conversations\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-computer.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-computer.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "b1b0af59-821f-4fbe-924f-328738b2d753"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "903fa321-b48e-4319-b09a-d29a435b5d9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "c5225bc7-cccb-42a1-ad2d-08430ae3b13d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('anne-bob-conversation{sample:0>3}.svg', \n",
    "                            'https://inverseprobability.com/talks/../slides/diagrams/',  sample=IntSlider(0, 0, 7, 1))"
   ],
   "id": "d99b0cf4-8cab-4609-958b-e4f388adc280"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation006.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Conversation relies on internal models of other\n",
    "individuals.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation007.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Misunderstanding of context and who we are talking to leads\n",
    "to arguments.</i>\n",
    "\n",
    "Similarly, we find it difficult to comprehend how computers are making\n",
    "decisions. Because they do so with more data than we can possibly\n",
    "imagine.\n",
    "\n",
    "In many respects, this is not a problem, it’s a good thing. Computers\n",
    "and us are good at different things. But when we interact with a\n",
    "computer, when it acts in a different way to us, we need to remember\n",
    "why.\n",
    "\n",
    "Just as the first step to getting along with other humans is\n",
    "understanding other humans, so it needs to be with getting along with\n",
    "our computers.\n",
    "\n",
    "Embodiment factors explain why, at the same time, computers are so\n",
    "impressive in simulating our weather, but so poor at predicting our\n",
    "moods. Our complexity is greater than that of our weather, and each of\n",
    "us is tuned to read and respond to one another.\n",
    "\n",
    "Their intelligence is different. It is based on very large quantities of\n",
    "data that we cannot absorb. Our computers don’t have a complex internal\n",
    "model of who we are. They don’t understand the human condition. They are\n",
    "not tuned to respond to us as we are to each other.\n",
    "\n",
    "Embodiment factors encapsulate a profound thing about the nature of\n",
    "humans. Our locked in intelligence means that we are striving to\n",
    "communicate, so we put a lot of thought into what we’re communicating\n",
    "with. And if we’re communicating with something complex, we naturally\n",
    "anthropomorphize them.\n",
    "\n",
    "We give our dogs, our cats, and our cars human motivations. We do the\n",
    "same with our computers. We anthropomorphize them. We assume that they\n",
    "have the same objectives as us and the same constraints. They don’t.\n",
    "\n",
    "This means, that when we worry about artificial intelligence, we worry\n",
    "about the wrong things. We fear computers that behave like more powerful\n",
    "versions of ourselves that will struggle to outcompete us.\n",
    "\n",
    "In reality, the challenge is that our computers cannot be human enough.\n",
    "They cannot understand us with the depth we understand one another. They\n",
    "drop below our cognitive radar and operate outside our mental models.\n",
    "\n",
    "The real danger is that computers don’t anthropomorphize. They’ll make\n",
    "decisions in isolation from us without our supervision because they\n",
    "can’t communicate truly and deeply with us."
   ],
   "id": "734b9c9a-d4c6-4e6b-a043-41fdfde70c3a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Conversations\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/anne-probability-conversation.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The focus so far has been on reducing uncertainty to a few\n",
    "representative values and sharing numbers with human beings. We forget\n",
    "that most people can be confused by basic probabilities for example the\n",
    "prosecutor’s fallacy.</i>\n",
    "\n",
    "In practice we know that probabilities can be very unintuitive, for\n",
    "example in court there is a fallacy known as the “prosecutor’s fallacy”\n",
    "that confuses conditional probabilities. This can cause problems in jury\n",
    "trials (Thompson, 1989)."
   ],
   "id": "bc0a4047-d8cb-481f-a29d-54d055f76e57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networked Interactions\n",
    "\n",
    "Our modern society intertwines the machine with human interactions. The\n",
    "key question is who has control over these interfaces between humans and\n",
    "machines.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/human-computers-interacting.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Humans and computers interacting should be a major focus of\n",
    "our research and engineering efforts.</i>\n",
    "\n",
    "So the real challenge that we face for society is understanding which\n",
    "systemic interventions will encourage the right interactions between the\n",
    "humans and the machine at all of these interfaces.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/human-culture-interacting.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Humans use culture, facts and ‘artefacts’ to communicate.</i>"
   ],
   "id": "eda69696-f180-4dbd-b490-ae3637a6922b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number Theatre\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_data-science/includes/number-data-theatre.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/number-data-theatre.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Unfortunately, we don’t always have time to wait for this process to\n",
    "converge to an answer we can all rely on before a decision is required.\n",
    "\n",
    "Not only can we be misled by data before a decision is made, but\n",
    "sometimes we can be misled by data to justify the making of a decision.\n",
    "David Spiegelhalter refers to the phenomenon of “Number Theatre” in a\n",
    "conversation with Andrew Marr from May 2020 on the presentation of data."
   ],
   "id": "c31876c4-d761-4b2a-81ad-0300cd328d45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('9388XmWIHXg')"
   ],
   "id": "8670d9f2-d42a-470b-a103-0faa0cbe90d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Professor Sir David Spiegelhalter on Andrew Marr on 10th May\n",
    "2020 speaking about some of the challengers around data, data\n",
    "presentation, and decision making in a pandemic. David mentions number\n",
    "theatre at 9 minutes 10 seconds.</i>\n",
    "\n",
    "<!--includebbcvideo{p08csg28}-->"
   ],
   "id": "881ab4c7-e076-41b9-a1e2-a567e5620abc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Theatre\n",
    "\n",
    "Data Theatre exploits data inattention bias to present a particular view\n",
    "on events that may misrepresents through selective presentation.\n",
    "Statisticians are one of the few groups that are trained with a\n",
    "sufficient degree of data skepticism. But it can also be combatted\n",
    "through ensuring there are domain experts present, and that they can\n",
    "speak freely.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//business/data-theatre001.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The phenomenon of number theatre or *data theatre* was\n",
    "described by David Spiegelhalter and is nicely summarized by Martin\n",
    "Robbins in this sub-stack article\n",
    "<https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards>.</i>\n",
    "\n",
    "<!--\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "::: {.cell .markdown}\n",
    "\n",
    "## Complexity in Action \n",
    "\n",
    "<div style=\"text-align:right\"><span class=\"editsection-bracket\" style=\"\">[</span><span class=\"editsection\" style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">]</span></div>\n",
    "\n",
    "As an exercise in understanding complexity, watch the following video. You will see the basketball being bounced around, and the players moving. Your job is to count the passes of those dressed in white and ignore those of the individuals dressed in black.\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.cell .code}\n",
    "\n",
    "```{.python}\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('vJG698U2Mvo')"
   ],
   "id": "2148ecae-dad1-414a-af3c-1b9bb19eeba6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Daniel Simon’s famous illusion “monkey business”. Focus on\n",
    "the movement of the ball distracts the viewer from seeing other aspects\n",
    "of the image.</i>\n",
    "\n",
    "In a classic study Simons and Chabris (1999) ask subjects to count the\n",
    "number of passes of the basketball between players on the team wearing\n",
    "white shirts. Fifty percent of the time, these subjects don’t notice the\n",
    "gorilla moving across the scene.\n",
    "\n",
    "The phenomenon of inattentional blindness is well known, e.g in their\n",
    "paper Simons and Charbris quote the Hungarian neurologist, Rezsö Bálint,\n",
    "\n",
    "> It is a well-known phenomenon that we do not notice anything happening\n",
    "> in our surroundings while being absorbed in the inspection of\n",
    "> something; focusing our attention on a certain object may happen to\n",
    "> such an extent that we cannot perceive other objects placed in the\n",
    "> peripheral parts of our visual field, although the light rays they\n",
    "> emit arrive completely at the visual sphere of the cerebral cortex.\n",
    ">\n",
    "> Rezsö Bálint 1907 (translated in Husain and Stein 1988, page 91)\n",
    "\n",
    "When we combine the complexity of the world with our relatively low\n",
    "bandwidth for information, problems can arise. Our focus on what we\n",
    "perceive to be the most important problem can cause us to miss other\n",
    "(potentially vital) contextual information.\n",
    "\n",
    "This phenomenon is known as selective attention or ‘inattentional\n",
    "blindness’."
   ],
   "id": "8d171d0d-9dd0-41eb-8db2-e97f8eef6afc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('_oGAzq5wM_Q')"
   ],
   "id": "edec5b67-2e12-4a72-b8d9-5ed9f5520f3d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>For a longer talk on inattentional bias from Daniel Simons\n",
    "see this video.</i>\n",
    "\n",
    "–\\>\n",
    "\n",
    "    ```{=html}\n",
    "    <!--include{_data-science/includes/data-selection-attention-bias.md}-->"
   ],
   "id": "2bbf7006-dcec-4c16-8c36-99d978c92da2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Conversations\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-llm.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-llm.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/anne-llm-conversation.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The focus so far has been on reducing uncertainty to a few\n",
    "representative values and sharing numbers with human beings. We forget\n",
    "that most people can be confused by basic probabilities for example the\n",
    "prosecutor’s fallacy.</i>"
   ],
   "id": "77fae0a8-bb78-4814-acd6-42604521260f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('0sJjdxn5kcI')"
   ],
   "id": "06e0312a-fd88-4b8c-9826-bcd021c67ee9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>The Inner Monologue paper suggests using LLMs for robotic\n",
    "planning (Huang et al., 2023).</i>\n",
    "\n",
    "By interacting directly with machines that have an understanding of\n",
    "human cultural context, it should be possible to share the nature of\n",
    "uncertainty in the same way humans do. See for example the paper [Inner\n",
    "Monologue: Embodied Reasoning through\n",
    "Planning](https://innermonologue.github.io/) Huang et al. (2023)."
   ],
   "id": "c5719b3d-3919-42e0-b962-1cd68c7b3897"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MONIAC\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "[The MONIAC](https://en.wikipedia.org/wiki/MONIAC) was an analogue\n",
    "computer designed to simulate the UK economy. Analogue comptuers work\n",
    "through analogy, the analogy in the MONIAC is that both money and water\n",
    "flow. The MONIAC exploits this through a system of tanks, pipes, valves\n",
    "and floats that represent the flow of money through the UK economy.\n",
    "Water flowed from the treasury tank at the top of the model to other\n",
    "tanks representing government spending, such as health and education.\n",
    "The machine was initially designed for teaching support but was also\n",
    "found to be a useful economic simulator. Several were built and today\n",
    "you can see the original at Leeds Business School, there is also one in\n",
    "the London Science Museum and one [in the Unisversity of Cambridge’s\n",
    "economics\n",
    "faculty](https://www.econ.cam.ac.uk/economics-alumni/drip-down-economics-phillips-machine).\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Bill Phillips and his MONIAC (completed in 1949). The machine\n",
    "is an analogue computer designed to simulate the workings of the UK\n",
    "economy.</i>"
   ],
   "id": "fceaeb50-1246-4ea0-8782-e42f78a47474"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donald MacKay\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/donald-mackay-brain.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/donald-mackay-brain.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//people/DonaldMacKay1952.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Donald M. MacKay (1922-1987), a physicist who was an early\n",
    "member of the cybernetics community and member of the Ratio Club.</i>\n",
    "\n",
    "Donald MacKay was a physicist who worked on naval gun targetting during\n",
    "the second world war. The challenge with gun targetting for ships is\n",
    "that both the target and the gun platform are moving. The challenge was\n",
    "tackled using analogue computers, for example in the US the [Mark I fire\n",
    "control\n",
    "computer](https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer)\n",
    "which was a mechanical computer. MacKay worked on radar systems for gun\n",
    "laying, here the velocity and distance of the target could be assessed\n",
    "through radar and an mechanical electrical analogue computer."
   ],
   "id": "672a545d-bce3-4fed-9cf2-d19aa29c8289"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire Control Systems\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/fire-control-systems.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/fire-control-systems.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Naval gunnery systems deal with targeting guns while taking into account\n",
    "movement of ships. The Royal Navy’s Gunnery Pocket Book (The Admiralty,\n",
    "1945) gives details of one system for gun laying.\n",
    "\n",
    "Like many challenges we face today, in the second world war, fire\n",
    "control was handled by a hybrid system of humans and computers. This\n",
    "means deploying human beings for the tasks that they can manage, and\n",
    "machines for the tasks that are better performed by a machine. This\n",
    "leads to a division of labour between the machine and the human that can\n",
    "still be found in our modern digital ecosystems.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/low-angle-fire-control-team.jpg\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The fire control computer set at the centre of a system of\n",
    "observation and tracking (The Admiralty, 1945).</i>\n",
    "\n",
    "As analogue computers, fire control computers from the second world war\n",
    "would contain components that directly represented the different\n",
    "variables that were important in the problem to be solved, such as the\n",
    "inclination between two ships.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/the-measurement-of-inclination.jpg\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Measuring inclination between two ships (The Admiralty,\n",
    "1945). Sophisticated fire control computers allowed the ship to continue\n",
    "to fire while under maneuvers.</i>\n",
    "\n",
    "The fire control systems were electro-mechanical analogue computers that\n",
    "represented the “state variables” of interest, such as inclination and\n",
    "ship speed with gears and cams within the machine.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/typical-modern-fire-control-table.jpg\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>A second world war gun computer’s control table (The\n",
    "Admiralty, 1945).</i>\n",
    "\n",
    "For more details on fire control computers, you can watch a 1953 film on\n",
    "the the US the [Mark IA fire control\n",
    "computer](https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer)\n",
    "from Periscope Film."
   ],
   "id": "59693e85-652e-43bc-ba4c-a3fa51e66835"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('gwf5mAlI7Ug')"
   ],
   "id": "abc36329-b43f-47d3-a46d-094bbb1cf5d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>U.S. Navy training film MN-6783a. Basic Mechanisms of Fire\n",
    "Control Computers. Mechanical Computer Instructional Film 27794 (1953)\n",
    "for the Mk 1A Fire Control Computer.</i>"
   ],
   "id": "670d4535-edb8-498f-9c6d-e969fabde267"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the Eye\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_books/includes/behind-the-eye.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/behind-the-eye.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//books/behind-the-eye.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>[Behind the\n",
    "Eye](https://www.amazon.co.uk/Behind-Eye-Gifford-Lectures-MACKAY/dp/0631173323)\n",
    "(MacKay, 1991) summarises MacKay’s Gifford Lectures, where MacKay uses\n",
    "the operation of the eye as a window on the operation of the brain.</i>\n",
    "\n",
    "Donald MacKay was at King’s College for his PhD. He was just down the\n",
    "road from Bill Phillips at LSE who was building the MONIAC. He was part\n",
    "of the Ratio Club. A group of early career scientists who were\n",
    "interested in communication and control in animals and humans, or more\n",
    "specifically they were interested in computers and brains. The were part\n",
    "of an international movement known as cybernetics.\n",
    "\n",
    "Donald MacKay wrote of the influence that his own work on radar had on\n",
    "his interest in the brain.\n",
    "\n",
    "> … during the war I had worked on the theory of automated and\n",
    "> electronic computing and on the theory of information, all of which\n",
    "> are highly relevant to such things as automatic pilots and automatic\n",
    "> gun direction. I found myself grappling with problems in the design of\n",
    "> artificial sense organs for naval gun-directors and with the\n",
    "> principles on which electronic circuits could be used to simulate\n",
    "> situations in the external world so as to provide goal-directed\n",
    "> guidance for ships, aircraft, missiles and the like.\n",
    "\n",
    "> Later in the 1940’s, when I was doing my Ph.D. work, there was much\n",
    "> talk of the brain as a computer and of the early digital computers\n",
    "> that were just making the headlines as “electronic brains.” As an\n",
    "> analogue computer man I felt strongly convinced that the brain,\n",
    "> whatever it was, was not a digital computer. I didn’t think it was an\n",
    "> analogue computer either in the conventional sense.\n",
    "\n",
    "> But this naturally rubbed under my skin the question: well, if it is\n",
    "> not either of these, what kind of system is it? Is there any way of\n",
    "> following through the kind of analysis that is appropriate to their\n",
    "> artificial automata so as to understand better the kind of system the\n",
    "> human brain is? That was the beginning of my slippery slope into brain\n",
    "> research.\n",
    ">\n",
    "> *Behind the Eye* pg 40. Edited version of the 1986 Gifford Lectures\n",
    "> given by Donald M. MacKay and edited by Valerie MacKay\n",
    "\n",
    "Importantly, MacKay distinguishes between the *analogue* computer and\n",
    "the *digital* computer. As he mentions, his experience was with analogue\n",
    "machines. An analogue machine is *literally* an analogue. The radar\n",
    "systems that Wiener and MacKay both worked on were made up of electronic\n",
    "components such as resistors, capacitors, inductors and/or mechanical\n",
    "components such as cams and gears. Together these components could\n",
    "represent a physical system, such as an anti-aircraft gun and a plane.\n",
    "The design of the analogue computer required the engineer to simulate\n",
    "the real world in analogue electronics, using dualities that exist\n",
    "between e.g. mechanical circuits (mass, spring, damper) and electronic\n",
    "circuits (inductor, resistor, capacitor). The analogy between mass and a\n",
    "damper, between spring and a resistor and between capacitor and a damper\n",
    "works because the underlying mathematics is approximated with the same\n",
    "linear system: a second order differential equation. This mathematical\n",
    "analogy allowed the designer to map from the real world, through\n",
    "mathematics, to a virtual world where the components reflected the real\n",
    "world through analogy."
   ],
   "id": "60a158b1-764d-484f-9733-a5ea7c906135"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Analogue Machine\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/human-analogue-machines-short.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/human-analogue-machines-short.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The machine learning systems we have built today that can reconstruct\n",
    "human text, or human classification of images, necessarily must have\n",
    "some aspects to them that are analagous to our understanding. As MacKay\n",
    "suggests the brain is neither a digital or an analogue computer, and the\n",
    "same can be said of the modern neural network systems that are being\n",
    "tagged as “artificial intelligence”.\n",
    "\n",
    "I believe a better term for them is “human-analogue machines”, because\n",
    "what we have built is not a system that can make intelligent decisions\n",
    "from first principles (a rational approach) but one that observes how\n",
    "humans have made decisions through our data and reconstructs that\n",
    "process. Machine learning is more empiricist than rational, but now we n\n",
    "empirical approach that distils our evolved intelligence.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/human-analogue-machine.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>The human analogue machine creates a feature space which is\n",
    "analagous to that we use to reason, one way of doing this is to have a\n",
    "machine attempt to compress all human generated text in an\n",
    "auto-regressive manner.</i>\n",
    "\n",
    "The perils of developing this capability include counterfeit people, a\n",
    "notion that the philosopher [Daniel Dennett has described in *The\n",
    "Atlantic*](https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/).\n",
    "This is where computers can represent themselves as human and fool\n",
    "people into doing things on that basis."
   ],
   "id": "c1663ae6-df18-4b42-9c8e-9527f5d03bb1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intellectual Debt\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ai/includes/intellectual-debt-blog-post.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/intellectual-debt-blog-post.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ai/2020-02-12-intellectual-debt.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>Jonathan Zittrain’s term to describe the challenges of\n",
    "explanation that come with AI is Intellectual Debt.</i>\n",
    "\n",
    "In the context of machine learning and complex systems, Jonathan\n",
    "Zittrain has coined the term [“Intellectual\n",
    "Debt”](https://medium.com/berkman-klein-center/from-technical-debt-to-intellectual-debt-in-ai-e05ac56a502c)\n",
    "to describe the challenge of understanding what you’ve created. In [the\n",
    "ML@CL group we’ve been foucssing on developing the notion of a\n",
    "*data-oriented\n",
    "architecture*](https://mlatcl.github.io/projects/data-oriented-architectures-for-ai-based-systems.html)\n",
    "to deal with intellectual debt (Cabrera et al., 2023).\n",
    "\n",
    "Zittrain points out the challenge around the lack of interpretability of\n",
    "individual ML models as the origin of intellectual debt. In machine\n",
    "learning I refer to work in this area as fairness, interpretability and\n",
    "transparency or FIT models. To an extent I agree with Zittrain, but if\n",
    "we understand the context and purpose of the decision making, I believe\n",
    "this is readily put right by the correct monitoring and retraining\n",
    "regime around the model. A concept I refer to as “progression testing”.\n",
    "Indeed, the best teams do this at the moment, and their failure to do it\n",
    "feels more of a matter of technical debt rather than intellectual,\n",
    "because arguably it is a maintenance task rather than an explanation\n",
    "task. After all, we have good statistical tools for interpreting\n",
    "individual models and decisions when we have the context. We can\n",
    "linearise around the operating point, we can perform counterfactual\n",
    "tests on the model. We can build empirical validation sets that explore\n",
    "fairness or accuracy of the model.\n",
    "\n",
    "But if we can avoid the pitfalls of counterfeit people, this also offers\n",
    "us an opportunity to *psychologically represent* (Heider, 1958) the\n",
    "machine in a manner where humans can communicate without special\n",
    "training. This in turn offers the opportunity to overcome the challenge\n",
    "of *intellectual debt*.\n",
    "\n",
    "Despite the lack of interpretability of machine learning models, they\n",
    "allow us access to what the machine is doing in a way that bypasses many\n",
    "of the traditional techniques developed in statistics. But understanding\n",
    "this new route for access is a major new challenge."
   ],
   "id": "45628952-88f7-4f77-b7b9-2a5fb14f00be"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAM\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information004.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The trinity of human, data, and computer, and highlights the\n",
    "modern phenomenon. The communication channel between computer and data\n",
    "now has an extremely high bandwidth. The channel between human and\n",
    "computer and the channel between data and human is narrow. New direction\n",
    "of information flow, information is reaching us mediated by the\n",
    "computer. The focus on classical statistics reflected the importance of\n",
    "the direct communication between human and data. The modern challenges\n",
    "of data science emerge when that relationship is being mediated by the\n",
    "machine.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information-ham.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The HAM now sits between us and the traditional digital\n",
    "computer.</i>"
   ],
   "id": "fdbc1d08-6d9b-417c-81ed-8bbbf69d2524"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Richard Feynmann on Doubt\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_physics/includes/richard-feynmann-doubt.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/richard-feynmann-doubt.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "> One thing is I can live with is doubt, and uncertainty and not\n",
    "> knowing. I think it’s much more interesting to live with not knowing\n",
    "> than to have an answer that might be wrong.\n",
    ">\n",
    "> Richard P. Feynmann in the *The Pleasure of Finding Things Out* 1981."
   ],
   "id": "91984908-09b2-4398-9f83-17edb333d16e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrodynamica\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_physics/includes/daniel-bernoulli-hydrodynamica.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/daniel-bernoulli-hydrodynamica.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "When Laplace spoke of the curve of a simple molecule of air, he may well\n",
    "have been thinking of Daniel Bernoulli (1700-1782). Daniel Bernoulli was\n",
    "one name in a prodigious family. His father and brother were both\n",
    "mathematicians. Daniel’s main work was known as *Hydrodynamica*."
   ],
   "id": "e2878f41-5870-4845-badc-1388bc01f64b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='3yRVAAAAcAAJ', page='PP7')"
   ],
   "id": "2535127a-cf2b-40f1-80d9-3ff5982aae4c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Daniel Bernoulli’s *Hydrodynamica* published in 1738. It was\n",
    "one of the first works to use the idea of conservation of energy. It\n",
    "used Newton’s laws to predict the behaviour of gases.</i>\n",
    "\n",
    "Daniel Bernoulli described a kinetic theory of gases, but it wasn’t\n",
    "until 170 years later when these ideas were verified after Einstein had\n",
    "proposed a model of Brownian motion which was experimentally verified by\n",
    "Jean Baptiste Perrin."
   ],
   "id": "2269ff09-476a-4d2e-836b-21a68a39fbb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='3yRVAAAAcAAJ', page='PA200')"
   ],
   "id": "8838ed5b-4724-4a4f-93de-51b446323e0b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Daniel Bernoulli’s chapter on the kinetic theory of gases,\n",
    "for a review on the context of this chapter see Mikhailov (n.d.). For\n",
    "1738 this is extraordinary thinking. The notion of kinetic theory of\n",
    "gases wouldn’t become fully accepted in Physics until 1908 when a model\n",
    "of Einstein’s was verified by Jean Baptiste Perrin.</i>"
   ],
   "id": "5ab61c37-bd15-4b10-8af3-5b2323bb6e4c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Billiards\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_physics/includes/entropy-billiards.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/entropy-billiards.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<canvas id=\"multiball-canvas\" width=\"700\" height=\"500\" style=\"border:1px solid black;display:inline;text-align:left \">\n",
    "</canvas>\n",
    "\n",
    "Entropy:\n",
    "\n",
    "<output id=\"multiball-entropy\">\n",
    "</output>\n",
    "\n",
    "<button id=\"multiball-newball\" style=\"text-align:right\">\n",
    "\n",
    "New Ball\n",
    "\n",
    "</button>\n",
    "<button id=\"multiball-pause\" style=\"text-align:right\">\n",
    "\n",
    "Pause\n",
    "\n",
    "</button>\n",
    "<button id=\"multiball-skip\" style=\"text-align:right\">\n",
    "\n",
    "Skip 1000s\n",
    "\n",
    "</button>\n",
    "<button id=\"multiball-histogram\" style=\"text-align:right\">\n",
    "\n",
    "Histogram\n",
    "\n",
    "</button>\n",
    "\n",
    "<script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "<script src=\"https://inverseprobability.com/talks/scripts//ballworld/ballworld.js\"></script>\n",
    "<script src=\"https://inverseprobability.com/talks/scripts//ballworld/multiball.js\"></script>\n",
    "\n",
    "Figure: <i>Bernoulli’s simple kinetic models of gases assume that the\n",
    "molecules of air operate like billiard balls.</i>"
   ],
   "id": "b881b597-e605-480f-981c-fdf4708841cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "id": "a2b5ef26-a079-4a26-926b-62aa8d8071ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.randn(10000, 1)\n",
    "xlim = [-4, 4]\n",
    "x = np.linspace(xlim[0], xlim[1], 200)\n",
    "y = 1/np.sqrt(2*np.pi)*np.exp(-0.5*x*x)"
   ],
   "id": "45367bea-8406-4ef0-a7d3-b8091fb0df55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "6b05568a-3ff3-496b-b02d-a6a49edd15a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x, y, 'r', linewidth=3)\n",
    "ax.hist(p, 100, density=True)\n",
    "ax.set_xlim(xlim)\n",
    "\n",
    "mlai.write_figure('gaussian-histogram.svg', directory='./ml')"
   ],
   "id": "9648f4ba-d968-4984-a877-ff5047800336"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important figure for Cambridge was the first to derive the\n",
    "probability distribution that results from small balls banging together\n",
    "in this manner. In doing so, James Clerk Maxwell founded the field of\n",
    "statistical physics.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ml/gaussian-histogram.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>James Clerk Maxwell 1831-1879 Derived distribution of\n",
    "velocities of particles in an ideal gas (elastic fluid).</i>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906)\n",
    "Josiah Willard Gibbs (1839-1903)</i>\n",
    "\n",
    "Many of the ideas of early statistical physicists were rejected by a\n",
    "cadre of physicists who didn’t believe in the notion of a molecule. The\n",
    "stress of trying to have his ideas established caused Boltzmann to\n",
    "commit suicide in 1906, only two years before the same ideas became\n",
    "widely accepted."
   ],
   "id": "5cd27be0-bf93-4d88-9f98-6eba97540870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='Vuk5AQAAMAAJ', page='PA373')"
   ],
   "id": "54abd45a-9809-4ea1-be9f-a9bafff7eac8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Boltzmann’s paper Boltzmann (n.d.) which introduced the\n",
    "relationship between entropy and probability. A translation with notes\n",
    "is available in Sharp and Matschinsky (2015).</i>\n",
    "\n",
    "The important point about the uncertainty being represented here is that\n",
    "it is not genuine stochasticity, it is a lack of knowledge about the\n",
    "system. The techniques proposed by Maxwell, Boltzmann and Gibbs allow us\n",
    "to exactly represent the state of the system through a set of parameters\n",
    "that represent the sufficient statistics of the physical system. We know\n",
    "these values as the volume, temperature, and pressure. The challenge for\n",
    "us, when approximating the physical world with the techniques we will\n",
    "use is that we will have to sit somewhere between the deterministic and\n",
    "purely stochastic worlds that these different scientists described.\n",
    "\n",
    "One ongoing characteristic of people who study probability and\n",
    "uncertainty is the confidence with which they hold opinions about it.\n",
    "Another leader of the Cavendish laboratory expressed his support of the\n",
    "second law of thermodynamics (which can be proven through the work of\n",
    "Gibbs/Boltzmann) with an emphatic statement at the beginning of his\n",
    "book.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"49%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/arthur-stanley-eddington.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"49%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_7.png\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Eddington’s book on the Nature of the Physical World\n",
    "(Eddington, 1929)</i>\n",
    "\n",
    "The same Eddington is also famous for dismissing the ideas of a young\n",
    "Chandrasekhar who had come to Cambridge to study in the Cavendish lab.\n",
    "Chandrasekhar demonstrated the limit at which a star would collapse\n",
    "under its own weight to a singularity, but when he presented the work to\n",
    "Eddington, he was dismissive suggesting that there “must be some natural\n",
    "law that prevents this abomination from happening”.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"49%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100.png\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"49%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/ChandraNobel.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Chandrasekhar (1910-1995) derived the limit at which a star\n",
    "collapses in on itself. Eddington’s confidence in the 2nd law may have\n",
    "been what drove him to dismiss Chandrasekhar’s ideas, humiliating a\n",
    "young scientist who would later receive a Nobel prize for the work.</i>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100_cropped.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Eddington makes his feelings about the primacy of the second\n",
    "law clear. This primacy is perhaps because the second law can be\n",
    "demonstrated mathematically, building on the work of Maxwell, Gibbs and\n",
    "Boltzmann. Eddington (1929)</i>\n",
    "\n",
    "Presumably he meant that the creation of a black hole seemed to\n",
    "transgress the second law of thermodynamics, although later Hawking was\n",
    "able to show that blackholes do evaporate, but the time scales at which\n",
    "this evaporation occurs is many orders of magnitude slower than other\n",
    "processes in the universe."
   ],
   "id": "043b4a3d-612c-4525-abde-c04c770c6d28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Motion and Wiener\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_physics/includes/brownian-wiener.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/brownian-wiener.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Robert Brown was a botanist who was studying plant pollen in 1827 when\n",
    "he noticed a trembling motion of very small particles contained within\n",
    "cavities within the pollen. He worked hard to eliminate the potential\n",
    "source of the movement by exploring other materials where he found it to\n",
    "be continuously present. Thus, the movement was not associated, as he\n",
    "originally thought, with life.\n",
    "\n",
    "In 1905 Albert Einstein produced the first mathematical explanation of\n",
    "the phenomenon. This can be seen as our first model of a ‘curve of a\n",
    "simple molecule of air’. To model the phenomenon Einstein introduced\n",
    "stochasticity to a differential equation. The particles were being\n",
    "peppered with high-speed water molecules, that was triggering the\n",
    "motion. Einstein modelled this as a stochastic process.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Albert Einstein’s 1905 paper on Brownian motion introduced\n",
    "stochastic differential equations which can be used to model the ‘curve\n",
    "of a simple molecule of air’.</i>\n",
    "\n",
    "Norbert Wiener was a child prodigy, whose father had schooled him in\n",
    "philosophy. He was keen to have his son work with the leading\n",
    "philosophers of the age, so at the age of 18 Wiener arrived in Cambridge\n",
    "(already with a PhD). He was despatched to study with Bertrand Russell\n",
    "but Wiener and Russell didn’t get along. Wiener wasn’t persuaded by\n",
    "Russell’s ideas for theories of knowledge through logic. He was more\n",
    "aligned with Laplace and his desire for a theory of ignorance. In is\n",
    "autobiography he relates it as the first thing he could see his father\n",
    "was proud of (at around the age of 10 or 11) (Wiener, 1953).\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg\" style=\"width:100%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg\" style=\"width:85%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg\" style=\"width:100%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Bertrand Russell (1872-1970), Albert Einstein (1879-1955),\n",
    "Norbert Wiener, (1894-1964)</i>\n",
    "\n",
    "But Russell (despite also not getting along well with Wiener) introduced\n",
    "Wiener to Einstein’s works, and Wiener also met G. H. Hardy. He left\n",
    "Cambridge for Göttingen where he studied with Hilbert. He developed the\n",
    "underlying mathematics for proving the existence of the solutions to\n",
    "Einstein’s equation, which are now known as Wiener processes.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Brownian motion of a large particle in a group of smaller\n",
    "particles. The movement is known as a *Wiener process* after Norbert\n",
    "Wiener.</i>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//books/wiener-yellow-peril.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Norbert Wiener (1894 - 1964). Founder of cybernetics and the\n",
    "information era. He used Gibbs’s ideas to develop a “theory of\n",
    "ignorance” that he deployed in early communication. On the right is\n",
    "Wiener’s wartime report that used stochastic processes in forecasting\n",
    "with applications in radar control (image from Coales and Kane\n",
    "(2014)).</i>\n",
    "\n",
    "Wiener himself used the processes in his work. He was focused on\n",
    "mathematical theories of communication. Between the world wars he was\n",
    "based at Massachusetts Institute of Technology where the burgeoning\n",
    "theory of electrical engineering was emerging, with a particular focus\n",
    "on communication lines. Winer developed theories of communication that\n",
    "used Gibbs’s entropy to encode information. He also used the ideas\n",
    "behind the Wiener process for developing tracking methods for radar\n",
    "systems in the second world war. These processes are what we know of now\n",
    "as Gaussian processes (Wiener (1949)).\n",
    "\n",
    "<!-- Lecture 2 -->"
   ],
   "id": "252550d0-6021-4e4b-ab3d-6356bcdd9a57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kappenball\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_physics/includes/kappenball.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/kappenball.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<span style=\"float:left;\">Score:\n",
    "\n",
    "<output id=\"kappenball-score\">\n",
    "</output>\n",
    "\n",
    "</span> <span style=\"float:right;\">Energy:\n",
    "\n",
    "<output id=\"kappenball-energy\">\n",
    "</output>\n",
    "\n",
    "</span>\n",
    "\n",
    "<canvas id=\"kappenball-canvas\" width=\"900\" height=\"500\" style=\"border:1px solid black;display:inline;text-align:center \">\n",
    "</canvas>\n",
    "\n",
    "<input type=\"range\" min=\"0\" max=\"100\" value=\"0\" class=\"slider\" id=\"kappenball-stochasticity\" style=\"width:900px;\"/>\n",
    "\n",
    "<button id=\"kappenball-newball\" style=\"text-align:right\">\n",
    "\n",
    "New Ball\n",
    "\n",
    "</button>\n",
    "<button id=\"kappenball-pause\" style=\"text-align:right\">\n",
    "\n",
    "Pause\n",
    "\n",
    "</button>\n",
    "\n",
    "<output id=\"kappenball-count\">\n",
    "</output>\n",
    "<script src=\"https://inverseprobability.com/talks/scripts//ballworld/kappenball.js\"></script>\n",
    "\n",
    "Figure: <i>Kappen Ball</i>\n",
    "\n",
    "If you want to complete a task, should you do it now or should you put\n",
    "it off until tomorrow? Despite being told to not delay tasks, many of us\n",
    "are deadline driven. Why is this?\n",
    "\n",
    "Kappenball is a simple game that illustrates that this behaviour can be\n",
    "optimal. It is inspired by an example in stochastic optimal control by\n",
    "[Bert Kappen](https://www.snn.ru.nl/~bertk/). The game is as follows:\n",
    "you need to place a falling balloon into one of two holes, but if the\n",
    "balloon misses the holes it will pop on pins placed in the ground. In\n",
    "‘deterministic mode’, the balloon falls straight towards the ground and\n",
    "the game is easy. You simply choose which hole to place the ball in, and\n",
    "you can start to place it there as soon as the ball appears at the top\n",
    "of the screen. The game becomes more interesting as you increase the\n",
    "uncertainty. In Kappenball, the uncertainty takes the form of the\n",
    "balloon being blown left and right as it falls. This movement means that\n",
    "it is not sensible to decide early on which hole to place the balloon\n",
    "in. A better strategy is to wait and see which hole the ball falls\n",
    "towards. You can then place it in that hole using less energy than in\n",
    "deterministic mode. Sometimes, the ball even falls into the hole on its\n",
    "own, and you don’t have to expend any energy, but it requires some skill\n",
    "to judge when you need to intervene. For this system Bert Kappen has\n",
    "shown mathematically that the best solution is to wait until the ball is\n",
    "close to the hole before you push it in. In other words, you should be\n",
    "deadline driven.\n",
    "\n",
    "In fact, it seems here uncertainty is a good thing, because on average\n",
    "you’ll get the ball into the hole with less energy (by playing\n",
    "intelligently, and being deadline driven!) than you do with\n",
    "\\`deterministic mode’. It requires some skill to do this, more than the\n",
    "deterministic system, but by using your resources intelligently you can\n",
    "get more out of the system. However, if the uncertainty increases too\n",
    "much then regardless of your skill, you can’t control the ball at all.\n",
    "\n",
    "This simple game explains many of the behaviours we exhibit in real\n",
    "life. If a system is completely deterministic, then we can make a\n",
    "decision early on and be sure that the ball will ‘drop in the hole’.\n",
    "However, if there is uncertainty in a system, it can make sense to delay\n",
    "our decision making until we’ve seen how events ‘pan out’. Be careful\n",
    "though, as we also see that when the uncertainty is large, if you don’t\n",
    "have the resources or the skill to be deadline-driven the uncertainty\n",
    "can overwhelm you and events can quickly move beyond our control."
   ],
   "id": "776b1761-d442-43a7-be64-f3e607e3406b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game of Life\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_simulation/includes/game-of-life.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/game-of-life.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "[John Horton Conway](https://en.wikipedia.org/wiki/John_Horton_Conway)\n",
    "was a mathematician who developed a game known as the Game of Life. He\n",
    "died in April 2020, but since he invented the game, he was in effect\n",
    "‘god’ for this game. But as we will see, just inventing the rules\n",
    "doesn’t give you omniscience in the game.\n",
    "\n",
    "The Game of Life is played on a grid of squares, or pixels. Each pixel\n",
    "is either on or off. The game has no players, but a set of simple rules\n",
    "that are followed at each turn the rules are."
   ],
   "id": "bb4eb319-fdb1-4c07-8a75-23f0849c4885"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life Rules\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-rules.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-rules.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "John Conway’s game of life is a cellular automaton where the cells obey\n",
    "three very simple rules. The cells live on a rectangular grid, so that\n",
    "each cell has 8 possible neighbors.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-0.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"39%\">\n",
    "<center>\n",
    "\n",
    "*loneliness*\n",
    "\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-1.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg\" style=\"width:100%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>‘Death’ through loneliness in Conway’s game of life. If a\n",
    "cell is surrounded by less than three cells, it ‘dies’ through\n",
    "loneliness.</i>\n",
    "\n",
    "The game proceeds in turns, and at each location in the grid is either\n",
    "alive or dead. Each turn, a cell counts its neighbors. If there are two\n",
    "or fewer neighbors, the cell ‘dies’ of ‘loneliness’.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-0.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"39%\">\n",
    "<center>\n",
    "\n",
    "*overcrowding*\n",
    "\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-1.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg\" style=\"width:100%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>‘Death’ through overpopulation in Conway’s game of life. If a\n",
    "cell is surrounded by more than three cells, it ‘dies’ through\n",
    "loneliness.</i>\n",
    "\n",
    "If there are four or more neighbors, the cell ‘dies’ from\n",
    "‘overcrowding’. If there are three neighbors, the cell persists, or if\n",
    "it is currently dead, a new cell is born.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-0.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"39%\">\n",
    "<center>\n",
    "\n",
    "*birth*\n",
    "\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-1.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg\" style=\"width:100%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Birth in Conway’s life. Any position surrounded by precisely\n",
    "three live cells will give birth to a new cell at the next turn.</i>\n",
    "\n",
    "And that’s it. Those are the simple ‘physical laws’ for Conway’s game.\n",
    "\n",
    "The game leads to patterns emerging, some of these patterns are static,\n",
    "but some oscillate in place, with varying periods. Others oscillate, but\n",
    "when they complete their cycle they’ve translated to a new location, in\n",
    "other words they move. In Life the former are known as\n",
    "[oscillators](https://conwaylife.com/wiki/Oscillator) and the latter as\n",
    "[spaceships](https://conwaylife.com/wiki/Spaceship)."
   ],
   "id": "dbdd1438-c847-4b3f-9355-094b3834acfe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loafers and Gliders\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-glider-loafer-conway.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-glider-loafer-conway.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "John Horton Conway, as the creator of the game of life, could be seen\n",
    "somehow as the god of this small universe. He created the rules. The\n",
    "rules are so simple that in many senses he, and we, are all-knowing in\n",
    "this space. But despite our knowledge, this world can still ‘surprise’\n",
    "us. From the simple rules, emergent patterns of behaviour arise. These\n",
    "include static patterns that don’t change from one turn to the next.\n",
    "They also include, oscillators, that pulse between different forms\n",
    "across different periods of time. A particular form of oscillator is\n",
    "known as a ‘spaceship’, this is one that moves across the board as the\n",
    "game evolves. One of the simplest and earliest spaceships to be\n",
    "discovered is known as the glider.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "<center>\n",
    "\n",
    "*Glider (1969)*\n",
    "\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/Glider.gif\" style=\"width:80%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>*Left* A Glider pattern discovered 1969 by Richard K. Guy.\n",
    "*Right*. John Horton Conway, creator of *Life* (1937-2020). The glider\n",
    "is an oscillator that moves diagonally after creation. From the simple\n",
    "rules of Life it’s not obvious that such an object does exist, until you\n",
    "do the necessary computation.</i>\n",
    "\n",
    "The glider was ‘discovered’ in 1969 by Richard K. Guy. What do we mean\n",
    "by discovered in this context? Well, as soon as the game of life is\n",
    "defined, objects such as the glider do somehow exist, but the many\n",
    "configurations of the game mean that it takes some time for us to see\n",
    "one and know it exists. This means, that despite being the creator,\n",
    "Conway, and despite the rules of the game being simple, and despite the\n",
    "rules being deterministic, we are not ‘omniscient’ in any simplistic\n",
    "sense. It requires computation to ‘discover’ what can exist in this\n",
    "universe once it’s been defined.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/Gosperglidergun.gif\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The Gosper glider gun is a configuration that creates\n",
    "gliders. A new glider is released after every 30 turns.</i>\n",
    "\n",
    "These patterns had to be discovered, in the same way that a scientist\n",
    "might discover a disease, or an explorer a new land. For example, the\n",
    "Gosper glider gun was [discovered by Bill Gosper in\n",
    "1970](https://conwaylife.com/wiki/Bill_Gosper). It is a pattern that\n",
    "creates a new glider every 30 turns of the game.\n",
    "\n",
    "Despite widespread interest in Life, some of its patterns were only very\n",
    "recently discovered like the Loafer, discovered in 2013 by Josh Ball.\n",
    "So, despite the game having existed for over forty years, and the rules\n",
    "of the game being simple, there are emergent behaviors that are unknown.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "<center>\n",
    "\n",
    "*Loafer (2013)*\n",
    "\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/Loafer.gif\" style=\"width:80%\">\n",
    "\n",
    "</center>\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>*Left* A Loafer pattern discovered by Josh Ball in 2013.\n",
    "*Right*. John Horton Conway, creator of *Life* (1937-2020).</i>\n",
    "\n",
    "Once these patterns are discovered, they are combined (or engineered) to\n",
    "create new Life patterns that do some remarkable things. For example,\n",
    "there’s a life pattern that runs a Turing machine, or more remarkably\n",
    "there’s a Life pattern that runs Life itself.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//simulation/life-in-life.gif\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The Game of Life running in Life. The video is drawing out\n",
    "recursively showing pixels that are being formed by filling cells with\n",
    "moving spaceships. Each individual pixel in this game of life is made up\n",
    "of $2048 \\times 2048$ pixels called an [OTCA\n",
    "metapixel](https://www.conwaylife.com/wiki/OTCA_metapixel).</i>\n",
    "\n",
    "To find out more about the Game of Life you can watch this video by Alan\n",
    "Zucconi or read his [associated blog\n",
    "post](https://www.alanzucconi.com/2020/10/13/conways-game-of-life/)."
   ],
   "id": "d4212d41-623e-48fb-b933-b0f6d14eb04b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('Kk2MH9O4pXY')"
   ],
   "id": "43c05241-9eaa-4106-99dd-40b442c224ab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>An introduction to the Game of Life by Alan Zucconi.</i>\n",
    "\n",
    "Contrast this with our situation where in ‘real life’ we don’t know the\n",
    "simple rules of the game, the state space is larger, and emergent\n",
    "behaviors (hurricanes, earthquakes, volcanos, climate change) have\n",
    "direct consequences for our daily lives, and we understand why the\n",
    "process of ‘understanding’ the physical world is so difficult. We also\n",
    "see immediately how much easier we might expect the physical sciences to\n",
    "be than the social sciences, where the emergent behaviors are contingent\n",
    "on highly complex human interactions."
   ],
   "id": "87f8afb8-3793-4fe5-8b96-2b1af752ea67"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference by Rejection Sampling\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "One view of Bayesian inference is to assume we are given a mechanism for\n",
    "generating samples, where we assume that mechanism is representing an\n",
    "accurate view on the way we believe the world works.\n",
    "\n",
    "This mechanism is known as our *prior* belief.\n",
    "\n",
    "We combine our prior belief with our observations of the real world by\n",
    "discarding all those prior samples that are inconsistent with our\n",
    "observations. The *likelihood* defines mathematically what we mean by\n",
    "inconsistent with the observations. The higher the noise level in the\n",
    "likelihood, the looser the notion of consistent.\n",
    "\n",
    "The samples that remain are samples from the *posterior*.\n",
    "\n",
    "This approach to Bayesian inference is closely related to two sampling\n",
    "techniques known as *rejection sampling* and *importance sampling*. It\n",
    "is realized in practice in an approach known as *approximate Bayesian\n",
    "computation* (ABC) or likelihood-free inference.\n",
    "\n",
    "In practice, the algorithm is often too slow to be practical, because\n",
    "most samples will be inconsistent with the observations and as a result\n",
    "the mechanism must be operated many times to obtain a few posterior\n",
    "samples.\n",
    "\n",
    "However, in the Gaussian process case, when the likelihood also assumes\n",
    "Gaussian noise, we can operate this mechanism mathematically, and obtain\n",
    "the posterior density *analytically*. This is the benefit of Gaussian\n",
    "processes.\n",
    "\n",
    "First, we will load in two python functions for computing the covariance\n",
    "function."
   ],
   "id": "205aa1d7-ad30-4dad-8f47-811ff5f269bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "36f2ed22-0a1a-4a71-bc63-a7214f981bf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.Kernel"
   ],
   "id": "359ab46c-eef5-4839-b741-3c265b28bbc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -n mlai.Kernel\n",
    "class Kernel():\n",
    "    \"\"\"Covariance function\n",
    "    :param function: covariance function\n",
    "    :type function: function\n",
    "    :param name: name of covariance function\n",
    "    :type name: string\n",
    "    :param shortname: abbreviated name of covariance function\n",
    "    :type shortname: string\n",
    "    :param formula: latex formula of covariance function\n",
    "    :type formula: string\n",
    "    :param function: covariance function\n",
    "    :type function: function\n",
    "    :param \\**kwargs:\n",
    "        See below\n",
    "\n",
    "    :Keyword Arguments:\n",
    "        * \"\"\"\n",
    "\n",
    "    def __init__(self, function, name=None, shortname=None, formula=None, **kwargs):        \n",
    "        self.function=function\n",
    "        self.formula = formula\n",
    "        self.name = name\n",
    "        self.shortname = shortname\n",
    "        self.parameters=kwargs\n",
    "        \n",
    "    def K(self, X, X2=None):\n",
    "        \"\"\"Compute the full covariance function given a kernel function for two data points.\"\"\"\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "        K = np.zeros((X.shape[0], X2.shape[0]))\n",
    "        for i in np.arange(X.shape[0]):\n",
    "            for j in np.arange(X2.shape[0]):\n",
    "                K[i, j] = self.function(X[i, :], X2[j, :], **self.parameters)\n",
    "\n",
    "        return K\n",
    "\n",
    "    def diag(self, X):\n",
    "        \"\"\"Compute the diagonal of the covariance function\"\"\"\n",
    "        diagK = np.zeros((X.shape[0], 1))\n",
    "        for i in range(X.shape[0]):            \n",
    "            diagK[i] = self.function(X[i, :], X[i, :], **self.parameters)\n",
    "        return diagK\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        raise NotImplementedError"
   ],
   "id": "bf52f4ab-0169-44b3-9f51-3d332062bbfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "a2ee36a1-408a-432c-a850-a40ce3e29289"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.eq_cov"
   ],
   "id": "cde7404b-90b0-426a-898d-18696501fde1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -n mlai.eq_cov\n",
    "def eq_cov(x, x_prime, variance=1., lengthscale=1.):\n",
    "    \"\"\"Exponentiated quadratic covariance function.\"\"\"\n",
    "    diffx = x - x_prime\n",
    "    return variance*np.exp(-0.5*np.dot(diffx, diffx)/lengthscale**2)"
   ],
   "id": "8f9be015-1618-4146-9334-e4786cea7f42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     lengthscale=0.25)"
   ],
   "id": "f3641c48-5aa6-4c5d-b67f-03a2b5e7ba86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sample from a multivariate normal density (a multivariate\n",
    "Gaussian), using the covariance function as the covariance matrix."
   ],
   "id": "0fadc18f-45a5-41aa-9471-cb243825b6d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import mlai.plot as plot"
   ],
   "id": "2c0300d4-4fc7-4f4f-b1fc-c5ab78c6eaff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.rejection_samples(kernel=kernel, \n",
    "    diagrams='./gp')"
   ],
   "id": "e72a199f-5ae5-4f18-ad27-d0f3017e4ccc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "228e2cdb-8b21-469d-84e9-48bb03987c6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('gp_rejection_sample{sample:0>3}.png', \n",
    "                 directory='./gp', \n",
    "                 sample=IntSlider(1,1,5,1))"
   ],
   "id": "5a95fb28-12e9-439a-8ece-73a2ba6b36d1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample003.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample004.png\" style=\"width:100%\">\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample005.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>One view of Bayesian inference is we have a machine for\n",
    "generating samples (the *prior*), and we discard all samples\n",
    "inconsistent with our data, leaving the samples of interest (the\n",
    "*posterior*). This is a rejection sampling view of Bayesian inference.\n",
    "The Gaussian process allows us to do this analytically by multiplying\n",
    "the *prior* by the *likelihood*.</i>\n",
    "\n",
    "Time scales, how when you expand or contract time signal becomes noise\n",
    "and noise becomes signal illustrate with Dirac delta and and stochastic\n",
    "processes in Fourier space, ito calculus. Latent force models.\n",
    "\n",
    "Practical examples of what happens understochasticity:\n",
    "\n",
    "1.  Derive U = W + TS?? Go from microscopic to macroscopic.\n",
    "\n",
    "2.  Kappenball — world in between where interesting things happen,\n",
    "\n",
    "3.  Queue efficiency (M/M/1 1/(1-))\n",
    "\n",
    "4.  Input to the system being in the form of bias and variance (or\n",
    "    perhaps Brownian motion, wiener process)\n",
    "\n",
    "(Latent force models being driven by this???? Latent force as high\n",
    "frequency information processing? Environment as slow?\n",
    "\n",
    "<!-- lecture 3 -->\n",
    "\n",
    "Connect supply chain as a “challenge” tot he abstraction of\n",
    "Schroedinger’s bridge. Link to Optimal Transport (matching without the\n",
    "“physics”). Maxwell’s demon.\n",
    "\n",
    "Control ability paper with Mauricio and Simo??)\n",
    "\n",
    "<!-- Interfaces AI for Science -->\n",
    "<!--include{_ai/includes/interfaces-ai-for-science.md}-->"
   ],
   "id": "c258e8b7-b684-4f70-887b-8f42d79819cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "What is machine learning? At its most basic level machine learning is a\n",
    "combination of\n",
    "\n",
    "$$\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}$$\n",
    "\n",
    "where *data* is our observations. They can be actively or passively\n",
    "acquired (meta-data). The *model* contains our assumptions, based on\n",
    "previous experience. That experience can be other data, it can come from\n",
    "transfer learning, or it can merely be our beliefs about the\n",
    "regularities of the universe. In humans our models include our inductive\n",
    "biases. The *prediction* is an action to be taken or a categorization or\n",
    "a quality score. The reason that machine learning has become a mainstay\n",
    "of artificial intelligence is the importance of predictions in\n",
    "artificial intelligence. The data and the model are combined through\n",
    "computation.\n",
    "\n",
    "In practice we normally perform machine learning using two functions. To\n",
    "combine data with a model we typically make use of:\n",
    "\n",
    "**a prediction function** it is used to make the predictions. It\n",
    "includes our beliefs about the regularities of the universe, our\n",
    "assumptions about how the world works, e.g., smoothness, spatial\n",
    "similarities, temporal similarities.\n",
    "\n",
    "**an objective function** it defines the ‘cost’ of misprediction.\n",
    "Typically, it includes knowledge about the world’s generating processes\n",
    "(probabilistic objectives) or the costs we pay for mispredictions\n",
    "(empirical risk minimization).\n",
    "\n",
    "The combination of data and model through the prediction function and\n",
    "the objective function leads to a *learning algorithm*. The class of\n",
    "prediction functions and objective functions we can make use of is\n",
    "restricted by the algorithms they lead to. If the prediction function or\n",
    "the objective function are too complex, then it can be difficult to find\n",
    "an appropriate learning algorithm. Much of the academic field of machine\n",
    "learning is the quest for new learning algorithms that allow us to bring\n",
    "different types of models and data together.\n",
    "\n",
    "A useful reference for state of the art in machine learning is the UK\n",
    "Royal Society Report, [Machine Learning: Power and Promise of Computers\n",
    "that Learn by\n",
    "Example](https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf).\n",
    "\n",
    "You can also check my post blog post on [What is Machine\n",
    "Learning?](http://inverseprobability.com/2017/07/17/what-is-machine-learning).\n",
    "\n",
    "In practice, we normally also have uncertainty associated with these\n",
    "functions. Uncertainty in the prediction function arises from\n",
    "\n",
    "1.  scarcity of training data and\n",
    "2.  mismatch between the set of prediction functions we choose and all\n",
    "    possible prediction functions.\n",
    "\n",
    "There are also challenges around specification of the objective\n",
    "function, but for we will save those for another day. For the moment,\n",
    "let us focus on the prediction function."
   ],
   "id": "69c84155-3f2c-48bc-9fab-ea1d959f0366"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and Prediction Functions\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/neural-networks.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/neural-networks.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Neural networks are adaptive non-linear function models. Originally,\n",
    "they were studied (by McCulloch and Pitts (McCulloch and Pitts, 1943))\n",
    "as simple models for neurons, but over the last decade they have become\n",
    "popular because they are a flexible approach to modelling complex data.\n",
    "A particular characteristic of neural network models is that they can be\n",
    "composed to form highly complex functions which encode many of our\n",
    "expectations of the real world. They allow us to encode our assumptions\n",
    "about how the world works.\n",
    "\n",
    "We will return to composition later, but for the moment, let’s focus on\n",
    "a one hidden layer neural network. We are interested in the prediction\n",
    "function, so we’ll ignore the objective function (which is often called\n",
    "an error function) for the moment, and just describe the mathematical\n",
    "object of interest\n",
    "\n",
    "$$\n",
    "f(\\mathbf{ x}) = \\mathbf{W}^\\top \\boldsymbol{ \\phi}(\\mathbf{V}, \\mathbf{ x})\n",
    "$$\n",
    "\n",
    "Where in this case $f(\\cdot)$ is a scalar function with vector inputs,\n",
    "and $\\boldsymbol{ \\phi}(\\cdot)$ is a vector function with vector inputs.\n",
    "The dimensionality of the vector function is known as the number of\n",
    "hidden units, or the number of neurons. The elements of this vector\n",
    "function are known as the *activation* function of the neural network\n",
    "and $\\mathbf{V}$ are the parameters of the activation functions."
   ],
   "id": "cbe0c733-cfc3-4fb2-bcf8-74d845637b33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations with Classical Statistics\n",
    "\n",
    "In statistics activation functions are traditionally known as *basis\n",
    "functions*. And we would think of this as a *linear model*. It’s doesn’t\n",
    "make linear predictions, but it’s linear because in statistics\n",
    "estimation focuses on the parameters, $\\mathbf{W}$, not the parameters,\n",
    "$\\mathbf{V}$. The linear model terminology refers to the fact that the\n",
    "model is *linear in the parameters*, but it is *not* linear in the data\n",
    "unless the activation functions are chosen to be linear."
   ],
   "id": "c70eaa48-0c4d-4fdf-b7e6-a8c3087cd803"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Basis Functions\n",
    "\n",
    "The first difference in the (early) neural network literature to the\n",
    "classical statistical literature is the decision to optimize these\n",
    "parameters, $\\mathbf{V}$, as well as the parameters, $\\mathbf{W}$ (which\n",
    "would normally be denoted in statistics by $\\boldsymbol{\\beta}$)[1].\n",
    "\n",
    "[1] In classical statistics we often interpret these parameters,\n",
    "$\\beta$, whereas in machine learning we are normally more interested in\n",
    "the result of the prediction, and less in the prediction. Although this\n",
    "is changing with more need for accountability. In honour of this I\n",
    "normally use $\\boldsymbol{\\beta}$ when I care about the value of these\n",
    "parameters, and $\\mathbf{ w}$ when I care more about the quality of the\n",
    "prediction."
   ],
   "id": "f8aa2d64-2db9-4e9f-abdc-77463d268db8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Basis Functions\n",
    "\n",
    "We’re going to go revisit that decision, and follow the path of Radford\n",
    "Neal (Neal, 1994) who, inspired by work of David MacKay (MacKay, 1992)\n",
    "and others did his PhD thesis on Bayesian Neural Networks. If we take a\n",
    "Bayesian approach to parameter inference (note I am using inference here\n",
    "in the classical sense, not in the sense of prediction of test data,\n",
    "which seems to be a newer usage), then we don’t wish to fit parameters\n",
    "at all, rather we wish to integrate them away and understand the family\n",
    "of functions that the model describes."
   ],
   "id": "232fc55d-d478-4015-ad20-340ca1f91a67"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Modelling\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/probabilistic-modelling.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/probabilistic-modelling.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "This Bayesian approach is designed to deal with uncertainty arising from\n",
    "fitting our prediction function to the data we have, a reduced data set.\n",
    "\n",
    "The Bayesian approach can be derived from a broader understanding of\n",
    "what our objective is. If we accept that we can jointly represent all\n",
    "things that happen in the world with a probability distribution, then we\n",
    "can interogate that probability to make predictions. So, if we are\n",
    "interested in predictions, $y_*$ at future points input locations of\n",
    "interest, $\\mathbf{ x}_*$ given previously training data, $\\mathbf{ y}$\n",
    "and corresponding inputs, $\\mathbf{X}$, then we are really interogating\n",
    "the following probability density, $$\n",
    "p(y_*|\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}_*),\n",
    "$$ there is nothing controversial here, as long as you accept that you\n",
    "have a good joint model of the world around you that relates test data\n",
    "to training data, $p(y_*, \\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}_*)$ then\n",
    "this conditional distribution can be recovered through standard rules of\n",
    "probability\n",
    "($\\text{data} + \\text{model} \\rightarrow \\text{prediction}$).\n",
    "\n",
    "We can construct this joint density through the use of the following\n",
    "decomposition: $$\n",
    "p(y_*|\\mathbf{ y}, \\mathbf{X}, \\mathbf{ x}_*) = \\int p(y_*|\\mathbf{ x}_*, \\mathbf{W}) p(\\mathbf{W}| \\mathbf{ y}, \\mathbf{X}) \\text{d} \\mathbf{W}\n",
    "$$\n",
    "\n",
    "where, for convenience, we are assuming *all* the parameters of the\n",
    "model are now represented by $\\boldsymbol{ \\theta}$ (which contains\n",
    "$\\mathbf{W}$ and $\\mathbf{V}$) and\n",
    "$p(\\boldsymbol{ \\theta}| \\mathbf{ y}, \\mathbf{X})$ is recognised as the\n",
    "posterior density of the parameters given data and\n",
    "$p(y_*|\\mathbf{ x}_*, \\boldsymbol{ \\theta})$ is the *likelihood* of an\n",
    "individual test data point given the parameters.\n",
    "\n",
    "The likelihood of the data is normally assumed to be independent across\n",
    "the parameters, $$\n",
    "p(\\mathbf{ y}|\\mathbf{X}, \\mathbf{W}) = \\prod_{i=1}^np(y_i|\\mathbf{ x}_i, \\mathbf{W}),$$\n",
    "\n",
    "and if that is so, it is easy to extend our predictions across all\n",
    "future, potential, locations, $$\n",
    "p(\\mathbf{ y}_*|\\mathbf{ y}, \\mathbf{X}, \\mathbf{X}_*) = \\int p(\\mathbf{ y}_*|\\mathbf{X}_*, \\boldsymbol{ \\theta}) p(\\boldsymbol{ \\theta}| \\mathbf{ y}, \\mathbf{X}) \\text{d} \\boldsymbol{ \\theta}.\n",
    "$$\n",
    "\n",
    "The likelihood is also where the *prediction function* is incorporated.\n",
    "For example in the regression case, we consider an objective based\n",
    "around the Gaussian density, $$\n",
    "p(y_i | f(\\mathbf{ x}_i)) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{\\left(y_i - f(\\mathbf{ x}_i)\\right)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "In short, that is the classical approach to probabilistic inference, and\n",
    "all approaches to Bayesian neural networks fall within this path. For a\n",
    "deep probabilistic model, we can simply take this one stage further and\n",
    "place a probability distribution over the input locations, $$\n",
    "p(\\mathbf{ y}_*|\\mathbf{ y}) = \\int p(\\mathbf{ y}_*|\\mathbf{X}_*, \\boldsymbol{ \\theta}) p(\\boldsymbol{ \\theta}| \\mathbf{ y}, \\mathbf{X}) p(\\mathbf{X}) p(\\mathbf{X}_*) \\text{d} \\boldsymbol{ \\theta}\\text{d} \\mathbf{X}\\text{d}\\mathbf{X}_*\n",
    "$$ and we have *unsupervised learning* (from where we can get deep\n",
    "generative models)."
   ],
   "id": "e74276c4-bbc6-43ec-bb1a-5c22a8a16855"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Models\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/graphical-models.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/graphical-models.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "One way of representing a joint distribution is to consider conditional\n",
    "dependencies between data. Conditional dependencies allow us to\n",
    "factorize the distribution. For example, a Markov chain is a\n",
    "factorization of a distribution into components that represent the\n",
    "conditional relationships between points that are neighboring, often in\n",
    "time or space. It can be decomposed in the following form.\n",
    "$$p(\\mathbf{ y}) = p(y_n| y_{n-1}) p(y_{n-1}|y_{n-2}) \\dots p(y_{2} | y_{1})$$"
   ],
   "id": "b3847b9f-a422-49d2-90aa-07ee02b5590d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica']}, size=30)\n",
    "rc(\"text\", usetex=True)"
   ],
   "id": "75298bf1-a8fa-4a79-b904-1517eb7f7400"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = daft.PGM(shape=[3, 1],\n",
    "               origin=[0, 0], \n",
    "               grid_unit=5, \n",
    "               node_unit=1.9, \n",
    "               observed_style='shaded',\n",
    "              line_width=3)\n",
    "\n",
    "\n",
    "pgm.add_node(daft.Node(\"y_1\", r\"$y_1$\", 0.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_2\", r\"$y_2$\", 1.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_3\", r\"$y_3$\", 2.5, 0.5, fixed=False))\n",
    "pgm.add_edge(\"y_1\", \"y_2\")\n",
    "pgm.add_edge(\"y_2\", \"y_3\")\n",
    "\n",
    "pgm.render().figure.savefig(\"./ml/markov.svg\", transparent=True)"
   ],
   "id": "395ff990-dc1b-4007-a584-19aaae3a30f9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//ml/markov.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Markov chain is a simple form of probabilistic graphical\n",
    "model providing a particular decomposition of the joint density.</i>\n",
    "\n",
    "By specifying conditional independencies we can reduce the\n",
    "parameterization required for our data, instead of directly specifying\n",
    "the parameters of the joint distribution, we can specify each set of\n",
    "parameters of the conditonal independently. This can also give an\n",
    "advantage in terms of interpretability. Understanding a conditional\n",
    "independence structure gives a structured understanding of data. If\n",
    "developed correctly, according to causal methodology, it can even inform\n",
    "how we should intervene in the system to drive a desired result (Pearl,\n",
    "1995).\n",
    "\n",
    "However, a challenge arises when the data becomes more complex. Consider\n",
    "the graphical model shown below, used to predict the perioperative risk\n",
    "of *C Difficile* infection following colon surgery (Steele et al.,\n",
    "2012).\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/../slides/diagrams//bayes-net-diagnosis.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>A probabilistic directed graph used to predict the\n",
    "perioperative risk of *C Difficile* infection following colon surgery.\n",
    "When these models have good predictive performance they are often\n",
    "difficult to interpret. This may be due to the limited representation\n",
    "capability of the conditional densities in the model.</i>\n",
    "\n",
    "To capture the complexity in the interelationship between the data, the\n",
    "graph itself becomes more complex, and less interpretable."
   ],
   "id": "7a093d80-0328-4b99-9c02-d1605f21c728"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "As far as combining our data and our model to form our prediction, the\n",
    "devil is in the detail. While everything is easy to write in terms of\n",
    "probability densities, as we move from $\\text{data}$ and $\\text{model}$\n",
    "to $\\text{prediction}$ there is that simple\n",
    "$\\stackrel{\\text{compute}}{\\rightarrow}$ sign, which is now burying a\n",
    "wealth of difficulties. Each integral sign above is a high dimensional\n",
    "integral which will typically need approximation. Approximations also\n",
    "come with computational demands. As we consider more complex classes of\n",
    "functions, the challenges around the integrals become harder and\n",
    "prediction of future test data given our model and the data becomes so\n",
    "involved as to be impractical or impossible.\n",
    "\n",
    "Statisticians realized these challenges early on, indeed, so early that\n",
    "they were actually physicists, both Laplace and Gauss worked on models\n",
    "such as this, in Gauss’s case he made his career on prediction of the\n",
    "location of the lost planet (later reclassified as a asteroid, then\n",
    "dwarf planet), Ceres. Gauss and Laplace made use of maximum a posteriori\n",
    "estimates for simplifying their computations and Laplace developed\n",
    "Laplace’s method (and invented the Gaussian density) to expand around\n",
    "that mode. But classical statistics needs better guarantees around model\n",
    "performance and interpretation, and as a result has focussed more on the\n",
    "*linear* model implied by $$\n",
    "  f(\\mathbf{ x}) = \\left.\\mathbf{ w}^{(2)}\\right.^\\top \\boldsymbol{ \\phi}(\\mathbf{W}_1, \\mathbf{ x})\n",
    "  $$\n",
    "\n",
    "$$\n",
    "  \\mathbf{ w}^{(2)} \\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{C}\\right).\n",
    "  $$\n",
    "\n",
    "The Gaussian likelihood given above implies that the data observation is\n",
    "related to the function by noise corruption so we have, $$\n",
    "  y_i = f(\\mathbf{ x}_i) + \\epsilon_i,\n",
    "  $$ where $$\n",
    "  \\epsilon_i \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\n",
    "  $$\n",
    "\n",
    "and while normally integrating over high dimensional parameter vectors\n",
    "is highly complex, here it is *trivial*. That is because of a property\n",
    "of the multivariate Gaussian.\n",
    "\n",
    "Gaussian processes are initially of interest because\n",
    "\n",
    "1.  linear Gaussian models are easier to deal with\n",
    "2.  Even the parameters *within* the process can be handled, by\n",
    "    considering a particular limit.\n",
    "\n",
    "Let’s first of all review the properties of the multivariate Gaussian\n",
    "distribution that make linear Gaussian models easier to deal with. We’ll\n",
    "return to the, perhaps surprising, result on the parameters within the\n",
    "nonlinearity, $\\boldsymbol{ \\theta}$, shortly.\n",
    "\n",
    "To work with linear Gaussian models, to find the marginal likelihood all\n",
    "you need to know is the following rules. If $$\n",
    "\\mathbf{ y}= \\mathbf{W}\\mathbf{ x}+ \\boldsymbol{ \\epsilon},\n",
    "$$ where $\\mathbf{ y}$, $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$ are\n",
    "vectors and we assume that $\\mathbf{ x}$ and $\\boldsymbol{ \\epsilon}$\n",
    "are drawn from multivariate Gaussians, $$\n",
    "\\begin{align}\n",
    "\\mathbf{ x}& \\sim \\mathcal{N}\\left(\\boldsymbol{ \\mu},\\mathbf{C}\\right)\\\\\n",
    "\\boldsymbol{ \\epsilon}& \\sim \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{ \\Sigma}\\right)\n",
    "\\end{align}\n",
    "$$ then we know that $\\mathbf{ y}$ is also drawn from a multivariate\n",
    "Gaussian with, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{W}\\boldsymbol{ \\mu},\\mathbf{W}\\mathbf{C}\\mathbf{W}^\\top + \\boldsymbol{ \\Sigma}\\right).\n",
    "$$\n",
    "\n",
    "With appropriately defined covariance, $\\boldsymbol{ \\Sigma}$, this is\n",
    "actually the marginal likelihood for Factor Analysis, or Probabilistic\n",
    "Principal Component Analysis (Tipping and Bishop, 1999), because we\n",
    "integrated out the inputs (or *latent* variables they would be called in\n",
    "that case)."
   ],
   "id": "5e00d38e-99b1-49cf-b51e-691694504265"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model Overview\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-model-overview.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-model-overview.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "However, we are focussing on what happens in models which are non-linear\n",
    "in the inputs, whereas the above would be *linear* in the inputs. To\n",
    "consider these, we introduce a matrix, called the design matrix. We set\n",
    "each activation function computed at each data point to be $$\n",
    "\\phi_{i,j} = \\phi(\\mathbf{ w}^{(1)}_{j}, \\mathbf{ x}_{i})\n",
    "$$ and define the matrix of activations (known as the *design matrix* in\n",
    "statistics) to be, $$\n",
    "\\boldsymbol{ \\Phi}= \n",
    "\\begin{bmatrix}\n",
    "\\phi_{1, 1} & \\phi_{1, 2} & \\dots & \\phi_{1, h} \\\\\n",
    "\\phi_{1, 2} & \\phi_{1, 2} & \\dots & \\phi_{1, n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_{n, 1} & \\phi_{n, 2} & \\dots & \\phi_{n, h}\n",
    "\\end{bmatrix}.\n",
    "$$ By convention this matrix always has $n$ rows and $h$ columns, now if\n",
    "we define the vector of all noise corruptions,\n",
    "$\\boldsymbol{ \\epsilon}= \\left[\\epsilon_1, \\dots \\epsilon_n\\right]^\\top$.\n",
    "\n",
    "If we define the prior distribution over the vector $\\mathbf{ w}$ to be\n",
    "Gaussian, $$\n",
    "\\mathbf{ w}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\mathbf{I}\\right),\n",
    "$$ then we can use rules of multivariate Gaussians to see that, $$\n",
    "\\mathbf{ y}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\sigma^2 \\mathbf{I}\\right).\n",
    "$$\n",
    "\n",
    "In other words, our training data is distributed as a multivariate\n",
    "Gaussian, with zero mean and a covariance given by $$\n",
    "\\mathbf{K}= \\alpha \\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top + \\sigma^2 \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "This is an $n\\times n$ size matrix. Its elements are in the form of a\n",
    "function. The maths shows that any element, index by $i$ and $j$, is a\n",
    "function *only* of inputs associated with data points $i$ and $j$,\n",
    "$\\mathbf{ y}_i$, $\\mathbf{ y}_j$.\n",
    "$k_{i,j} = k\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right)$\n",
    "\n",
    "If we look at the portion of this function associated only with\n",
    "$f(\\cdot)$, i.e. we remove the noise, then we can write down the\n",
    "covariance associated with our neural network, $$\n",
    "k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) = \\alpha \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_i\\right)^\\top \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_j\\right)\n",
    "$$ so the elements of the covariance or *kernel* matrix are formed by\n",
    "inner products of the rows of the *design matrix*."
   ],
   "id": "24d7c4b0-7def-4898-b8f2-a02109290678"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process\n",
    "\n",
    "This is the essence of a Gaussian process. Instead of making assumptions\n",
    "about our density over each data point, $y_i$ as i.i.d. we make a joint\n",
    "Gaussian assumption over our data. The covariance matrix is now a\n",
    "function of both the parameters of the activation function,\n",
    "$\\mathbf{V}$, and the input variables, $\\mathbf{X}$. This comes about\n",
    "through integrating out the parameters of the model, $\\mathbf{ w}$."
   ],
   "id": "db59daff-74f7-428d-b023-d7dbced67edd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Functions\n",
    "\n",
    "We can basically put anything inside the basis functions, and many\n",
    "people do. These can be deep kernels (Cho and Saul, 2009) or we can\n",
    "learn the parameters of a convolutional neural network inside there.\n",
    "\n",
    "Viewing a neural network in this way is also what allows us to beform\n",
    "sensible *batch* normalizations (Ioffe and Szegedy, 2015)."
   ],
   "id": "8bb31584-435b-492a-a54d-f902139cc7a3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-degenerate Gaussian Processes\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/non-degenerate-gps.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/non-degenerate-gps.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The process described above is degenerate. The covariance function is of\n",
    "rank at most $h$ and since the theoretical amount of data could always\n",
    "increase $n\\rightarrow \\infty$, the covariance function is not full\n",
    "rank. This means as we increase the amount of data to infinity, there\n",
    "will come a point where we can’t normalize the process because the\n",
    "multivariate Gaussian has the form, $$\n",
    "\\mathcal{N}\\left(\\mathbf{ f}|\\mathbf{0},\\mathbf{K}\\right) = \\frac{1}{\\left(2\\pi\\right)^{\\frac{n}{2}}\\det{\\mathbf{K}}^\\frac{1}{2}} \\exp\\left(-\\frac{\\mathbf{ f}^\\top\\mathbf{K}\\mathbf{ f}}{2}\\right)\n",
    "$$ and a non-degenerate kernel matrix leads to $\\det{\\mathbf{K}} = 0$\n",
    "defeating the normalization (it’s equivalent to finding a projection in\n",
    "the high dimensional Gaussian where the variance of the the resulting\n",
    "univariate Gaussian is zero, i.e. there is a null space on the\n",
    "covariance, or alternatively you can imagine there are one or more\n",
    "directions where the Gaussian has become the delta function).\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Radford Neal\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/radford-neal.jpg\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "In the machine learning field, it was Radford Neal (Neal, 1994) that\n",
    "realized the potential of the next step. In his 1994 thesis, he was\n",
    "considering Bayesian neural networks, of the type we described above,\n",
    "and in considered what would happen if you took the number of hidden\n",
    "nodes, or neurons, to infinity, i.e. $h\\rightarrow \\infty$.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//neal-infinite-priors.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Page 37 of [Radford Neal’s 1994\n",
    "thesis](http://www.cs.toronto.edu/~radford/ftp/thesis.pdf)</i>\n",
    "\n",
    "In loose terms, what Radford considers is what happens to the elements\n",
    "of the covariance function, $$\n",
    "  \\begin{align*}\n",
    "  k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) & = \\alpha \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_i\\right)^\\top \\boldsymbol{ \\phi}\\left(\\mathbf{W}_1, \\mathbf{ x}_j\\right)\\\\\n",
    "  & = \\alpha \\sum_k \\phi\\left(\\mathbf{ w}^{(1)}_k, \\mathbf{ x}_i\\right) \\phi\\left(\\mathbf{ w}^{(1)}_k, \\mathbf{ x}_j\\right)\n",
    "  \\end{align*}\n",
    "  $$ if instead of considering a finite number you sample infinitely\n",
    "many of these activation functions, sampling parameters from a prior\n",
    "density, $p(\\mathbf{ v})$, for each one, $$\n",
    "k_f\\left(\\mathbf{ x}_i, \\mathbf{ x}_j\\right) = \\alpha \\int \\phi\\left(\\mathbf{ w}^{(1)}, \\mathbf{ x}_i\\right) \\phi\\left(\\mathbf{ w}^{(1)}, \\mathbf{ x}_j\\right) p(\\mathbf{ w}^{(1)}) \\text{d}\\mathbf{ w}^{(1)}\n",
    "$$ And that’s not *only* for Gaussian $p(\\mathbf{ v})$. In fact this\n",
    "result holds for a range of activations, and a range of prior densities\n",
    "because of the *central limit theorem*.\n",
    "\n",
    "To write it in the form of a probabilistic program, as long as the\n",
    "distribution for $\\phi_i$ implied by this short probabilistic program,\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  \\mathbf{ v}& \\sim p(\\cdot)\\\\\n",
    "  \\phi_i & = \\phi\\left(\\mathbf{ v}, \\mathbf{ x}_i\\right), \n",
    "  \\end{align*}\n",
    "  $$ has finite variance, then the result of taking the number of hidden\n",
    "units to infinity, with appropriate scaling, is also a Gaussian process."
   ],
   "id": "d7aa8140-5f2c-48d7-ace9-c00b030d0a12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "To understand this argument in more detail, I highly recommend reading\n",
    "chapter 2 of Neal’s thesis (Neal, 1994), which remains easy to read and\n",
    "clear today. Indeed, for readers interested in Bayesian neural networks,\n",
    "both Raford Neal’s and David MacKay’s PhD thesis (MacKay, 1992) remain\n",
    "essential reading. Both theses embody a clarity of thought, and an\n",
    "ability to weave together threads from different fields that was the\n",
    "business of machine learning in the 1990s. Radford and David were also\n",
    "pioneers in making their software widely available and publishing\n",
    "material on the web.\n",
    "\n",
    "<!-- ### Two Dimensional Gaussian Distribution -->\n",
    "<!-- include{_ml/includes/two-d-gaussian.md} -->"
   ],
   "id": "3aa870df-056e-4949-8442-dee3cb462b06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4949)"
   ],
   "id": "4e89a4da-1b27-43fd-9f3b-4c41875961ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a Function\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpdistfunc.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpdistfunc.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "We will consider a Gaussian distribution with a particular structure of\n",
    "covariance matrix. We will generate *one* sample from a 25-dimensional\n",
    "Gaussian density. $$\n",
    "\\mathbf{ f}=\\left[f_{1},f_{2}\\dots f_{25}\\right].\n",
    "$$ in the figure below we plot these data on the $y$-axis against their\n",
    "*indices* on the $x$-axis."
   ],
   "id": "d2366e06-b98c-4cdc-82e6-30346a4a9064"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "c07210fe-0cad-4ca9-b699-3f46c3c5622f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.Kernel"
   ],
   "id": "082f2eac-e1f6-4ad3-afd4-471a0d829dc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "e9d538d1-249d-4b28-baea-f0014e8dbe91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.polynomial_cov"
   ],
   "id": "6ccc3cc7-1f56-49d1-b842-0fde4feb6e68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "ab3104e3-ccc1-4961-820b-74f2f513d9ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.exponentiated_quadratic"
   ],
   "id": "7f2ab79f-aee0-4ea3-9a48-6a3681fc85bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "from mlai import Kernel, exponentiated_quadratic"
   ],
   "id": "11358f8c-ea7d-4f01-bd86-5fb3cb63eb27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel=Kernel(function=exponentiated_quadratic, lengthscale=0.5)\n",
    "plot.two_point_sample(kernel.K, diagrams='./gp')"
   ],
   "id": "af51f358-f69d-4dc5-9eb6-c559c55dc782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "7526c8cd-1566-4abd-8c00-e97119ec70f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "60ea707f-6b24-411d-9b16-565746d43c26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('two_point_sample{sample:0>3}.svg', './gp', sample=IntSlider(0, 0, 8, 1))"
   ],
   "id": "0908a6ae-969b-4d82-b81f-3fba6c3784f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample008.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A 25 dimensional correlated random variable (values ploted\n",
    "against index)</i>"
   ],
   "id": "b0c19a45-0f79-4f64-9cbc-1d2cae6c5122"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling a Function from a Gaussian\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-two.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-two.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "786d3289-2285-4af8-994d-e0ca77e5786f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "06e59b67-2e3d-412f-b0aa-d5db4cc0694e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "05e9c11c-1644-413a-bf16-3006b39a3689"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(0, 0, 8, 1))"
   ],
   "id": "cef9e0c8-a961-43da-8950-bcec9a160c60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample001.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_2$ along with the\n",
    "conditional distribution of $f_2$ given $f_1$</i>"
   ],
   "id": "f6af0b45-8ad2-45d1-bc3a-05786c09d77a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Density of $f_1$ and $f_2$"
   ],
   "id": "46ed991e-7722-49f1-bb80-4dec2aa01524"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "55a55932-8625-4332-b848-242fcbc3d94e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "ef8730a6-cc36-4314-838e-d141f17a7e18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(9, 9, 12, 1))"
   ],
   "id": "f2089bb0-6819-4927-9c34-240f27f23e82"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample012.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_2$ along with the\n",
    "conditional distribution of $f_2$ given $f_1$</i>"
   ],
   "id": "70c2bd38-ce81-426e-9530-bf5122880a58"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uluru\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/799px-Uluru_Panorama.jpg\" style=\"width:\">\n",
    "\n",
    "Figure: <i>Uluru, the sacred rock in Australia. If we think of it as a\n",
    "probability density, viewing it from this side gives us one *marginal*\n",
    "from the density. Figuratively speaking, slicing through the rock would\n",
    "give a conditional density.</i>\n",
    "\n",
    "When viewing these contour plots, I sometimes find it helpful to think\n",
    "of Uluru, the prominent rock formation in Australia. The rock rises\n",
    "above the surface of the plane, just like a probability density rising\n",
    "above the zero line. The rock is three dimensional, but when we view\n",
    "Uluru from the classical position, we are looking at one side of it.\n",
    "This is equivalent to viewing the marginal density.\n",
    "\n",
    "The joint density can be viewed from above, using contours. The\n",
    "conditional density is equivalent to *slicing* the rock. Uluru is a holy\n",
    "rock, so this has to be an imaginary slice. Imagine we cut down a\n",
    "vertical plane orthogonal to our view point (e.g. coming across our view\n",
    "point). This would give a profile of the rock, which when renormalized,\n",
    "would give us the conditional distribution, the value of conditioning\n",
    "would be the location of the slice in the direction we are facing."
   ],
   "id": "17967c33-ab80-4dff-b918-c3467149af20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Correlated Gaussians\n",
    "\n",
    "Of course in practice, rather than manipulating mountains physically,\n",
    "the advantage of the Gaussian density is that we can perform these\n",
    "manipulations mathematically.\n",
    "\n",
    "Prediction of $f_2$ given $f_1$ requires the *conditional density*,\n",
    "$p(f_2|f_1)$.Another remarkable property of the Gaussian density is that\n",
    "this conditional distribution is *also* guaranteed to be a Gaussian\n",
    "density. It has the form, $$\n",
    "p(f_2|f_1) = \\mathcal{N}\\left(f_2|\\frac{k_{1, 2}}{k_{1, 1}}f_1, k_{2, 2} - \\frac{k_{1,2}^2}{k_{1,1}}\\right)\n",
    "$$where we have assumed that the covariance of the original joint\n",
    "density was given by $$\n",
    "\\mathbf{K}= \\begin{bmatrix} k_{1, 1} & k_{1, 2}\\\\ k_{2, 1} & k_{2, 2}.\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using these formulae we can determine the conditional density for any of\n",
    "the elements of our vector $\\mathbf{ f}$. For example, the variable\n",
    "$f_8$ is less correlated with $f_1$ than $f_2$. If we consider this\n",
    "variable we see the conditional density is more diffuse."
   ],
   "id": "8c326f61-3b68-45c1-8b18-c0680439bb36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Density of $f_1$ and $f_8$\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-eight.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-eight.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "25313066-ff82-463b-96dd-86d42c22360e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "be9166a9-8645-4ed9-822b-f349e54ff045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "171b3e4f-9562-4392-b745-90ec61d6f1ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            './gp', \n",
    "                            sample=IntSlider(13, 13, 17, 1))"
   ],
   "id": "6cbf14dc-7573-424e-ac9f-913b8dd212e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample013.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sample from the joint Gaussian model, points indexed by 1 and\n",
    "8 highlighted.</i>"
   ],
   "id": "29a93c66-17ec-4891-bdad-9e5d24192d4c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of $f_{8}$ from $f_{1}$\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample017.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The joint Gaussian over $f_1$ and $f_8$ along with the\n",
    "conditional distribution of $f_8$ given $f_1$</i>\n",
    "\n",
    "-   The single contour of the Gaussian density represents the\n",
    "    <font color=\"blue\">joint distribution, $p(f_1, f_8)$</font>\n",
    "\n",
    ". . .\n",
    "\n",
    "-   We observe a value for <font color=\"green\">$f_1=-?$</font>\n",
    "\n",
    ". . .\n",
    "\n",
    "-   Conditional density: <font color=\"red\">$p(f_8|f_1=?)$</font>.\n",
    "\n",
    "-   Prediction of $\\mathbf{ f}_*$ from $\\mathbf{ f}$ requires\n",
    "    multivariate *conditional density*.\n",
    "\n",
    "-   Multivariate conditional density is *also* Gaussian. <large> $$\n",
    "    p(\\mathbf{ f}_*|\\mathbf{ f}) = {\\mathcal{N}\\left(\\mathbf{ f}_*|\\mathbf{K}_{*,\\mathbf{ f}}\\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{ f},\\mathbf{K}_{*,*}-\\mathbf{K}_{*,\\mathbf{ f}} \\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{K}_{\\mathbf{ f},*}\\right)}\n",
    "    $$ </large>\n",
    "\n",
    "-   Here covariance of joint density is given by $$\n",
    "    \\mathbf{K}= \\begin{bmatrix} \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}} & \\mathbf{K}_{*, \\mathbf{ f}}\\\\ \\mathbf{K}_{\\mathbf{ f}, *} & \\mathbf{K}_{*, *}\\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "-   Prediction of $\\mathbf{ f}_*$ from $\\mathbf{ f}$ requires\n",
    "    multivariate *conditional density*.\n",
    "\n",
    "-   Multivariate conditional density is *also* Gaussian. <large> $$\n",
    "    p(\\mathbf{ f}_*|\\mathbf{ f}) = {\\mathcal{N}\\left(\\mathbf{ f}_*|\\boldsymbol{ \\mu},\\boldsymbol{ \\Sigma}\\right)}\n",
    "    $$ $$\n",
    "    \\boldsymbol{ \\mu}= \\mathbf{K}_{*,\\mathbf{ f}}\\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{ f}\n",
    "    $$ $$\n",
    "    \\boldsymbol{ \\Sigma}= \\mathbf{K}_{*,*}-\\mathbf{K}_{*,\\mathbf{ f}} \\mathbf{K}_{\\mathbf{ f},\\mathbf{ f}}^{-1}\\mathbf{K}_{\\mathbf{ f},*}\n",
    "    $$ </large>\n",
    "\n",
    "-   Here covariance of joint density is given by $$\n",
    "    \\mathbf{K}= \\begin{bmatrix} \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}} & \\mathbf{K}_{*, \\mathbf{ f}}\\\\ \\mathbf{K}_{\\mathbf{ f}, *} & \\mathbf{K}_{*, *}\\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "-   Covariance function, $\\mathbf{K}$\n",
    "\n",
    "-   Determines properties of samples.\n",
    "\n",
    "-   Function of $\\mathbf{X}$,\n",
    "    $$k_{i,j} = k(\\mathbf{ x}_i, \\mathbf{ x}_j)$$\n",
    "\n",
    "-   Posterior mean\n",
    "    $$f_D(\\mathbf{ x}_*) = \\mathbf{ k}(\\mathbf{ x}_*, \\mathbf{X}) \\mathbf{K}^{-1}\n",
    "    \\mathbf{ y}$$\n",
    "\n",
    "-   Posterior covariance\n",
    "    $$\\mathbf{C}_* = \\mathbf{K}_{*,*} - \\mathbf{K}_{*,\\mathbf{ f}}\n",
    "    \\mathbf{K}^{-1} \\mathbf{K}_{\\mathbf{ f}, *}$$\n",
    "\n",
    "-   Posterior mean\n",
    "\n",
    "    $$f_D(\\mathbf{ x}_*) = \\mathbf{ k}(\\mathbf{ x}_*, \\mathbf{X}) \\boldsymbol{\\alpha}$$\n",
    "\n",
    "-   Posterior covariance\n",
    "    $$\\mathbf{C}_* = \\mathbf{K}_{*,*} - \\mathbf{K}_{*,\\mathbf{ f}}\n",
    "    \\mathbf{K}^{-1} \\mathbf{K}_{\\mathbf{ f}, *}$$"
   ],
   "id": "4560aa5f-c819-4b7b-9a4b-dac60029e603"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentiated Quadratic Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/eq-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/eq-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "e554eedf-f7fb-449c-b5ba-a8932a7bf340"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "f7f3861b-f4a1-4cff-90fb-fba95581c3d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.Kernel"
   ],
   "id": "78cb06f2-153e-4cf0-b265-605a76bdfa13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "4f89bc00-975a-4605-9447-72b67b5bd15c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.eq_cov"
   ],
   "id": "d55c0d11-f856-4fc4-9219-f7668a293646"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=eq_cov,\n",
    "                     name='Exponentiated Quadratic',\n",
    "                     shortname='eq',                     \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\exp\\left(-\\frac{\\ltwoNorm{\\inputVector-\\inputVector^\\prime}^2}{2\\lengthScale^2}\\right)',\n",
    "                     lengthscale=0.2)"
   ],
   "id": "aac4b8a4-6ace-4b17-b1e7-969a8f77199d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "eff4882b-dd62-4def-91e3-edbc561e7215"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_func(kernel=kernel, diagrams='./kern/')"
   ],
   "id": "c5690c67-1ead-4522-a625-9250b06b73cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponentiated quadratic covariance, also known as the Gaussian\n",
    "covariance or the RBF covariance and the squared exponential. Covariance\n",
    "between two points is related to the negative exponential of the squared\n",
    "distnace between those points. This covariance function can be derived\n",
    "in a few different ways: as the infinite limit of a radial basis\n",
    "function neural network, as diffusion in the heat equation, as a\n",
    "Gaussian filter in *Fourier space* or as the composition as a series of\n",
    "linear filters applied to a base function.\n",
    "\n",
    "The covariance takes the following form, $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)\n",
    "$$ where $\\ell$ is the *length scale* or *time scale* of the process and\n",
    "$\\alpha$ represents the overall process variance.\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}-\\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/eq_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/eq_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function.</i>"
   ],
   "id": "e40c961c-a954-4b1e-8514-9e438c284db9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic Marathon Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "-   Marathons before 1924 didn’t have a standardized distance.\n",
    "-   Present results using pace per km.\n",
    "-   In 1904 Marathon was badly organized leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//Stephen_Kiprotich.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons <http://bit.ly/16kMKHQ></small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
    "winners for the Olympics from 1896 to present. Let’s load in the data\n",
    "and plot."
   ],
   "id": "bdcdb3f3-5d98-483b-bd92-f2fea1242fd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pods"
   ],
   "id": "22df6188-80f0-4fd9-a108-d55d5cbed17e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ],
   "id": "c677ba20-d7ee-4cbc-9c42-9af8c35c79a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())\n",
    "yhat = (y - offset)/scale"
   ],
   "id": "3398273d-bd26-402c-b7e3-b7178a65d99e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "45b533f7-4e5f-4484-b744-c2e321a52467"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon.svg', \n",
    "                  directory='./datasets')"
   ],
   "id": "35eeacc4-029c-410f-a21a-96586330a5f5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//datasets/olympic-marathon.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic marathon pace times since 1896.</i>\n",
    "\n",
    "Things to notice about the data include the outlier in 1904, in that\n",
    "year the Olympics was in St Louis, USA. Organizational problems and\n",
    "challenges with dust kicked up by the cars following the race meant that\n",
    "participants got lost, and only very few participants completed. More\n",
    "recent years see more consistently quick marathons."
   ],
   "id": "76a6bc74-2e13-4cc5-b72e-314e0b63691f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alan Turing\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//turing-times.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//turing-run.jpg\" style=\"width:50%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Alan Turing, in 1946 he was only 11 minutes slower than the\n",
    "winner of the 1948 games. Would he have won a hypothetical games held in\n",
    "1946? Source:\n",
    "<a href=\"http://www.turing.org.uk/scrapbook/run.html\" target=\"_blank\">Alan\n",
    "Turing Internet Scrapbook</a>.</i>\n",
    "\n",
    "If we had to summarise the objectives of machine learning in one word, a\n",
    "very good candidate for that word would be *generalization*. What is\n",
    "generalization? From a human perspective it might be summarised as the\n",
    "ability to take lessons learned in one domain and apply them to another\n",
    "domain. If we accept the definition given in the first session for\n",
    "machine learning, $$\n",
    "\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}\n",
    "$$ then we see that without a model we can’t generalise: we only have\n",
    "data. Data is fine for answering very specific questions, like “Who won\n",
    "the Olympic Marathon in 2012?”, because we have that answer stored,\n",
    "however, we are not given the answer to many other questions. For\n",
    "example, Alan Turing was a formidable marathon runner, in 1946 he ran a\n",
    "time 2 hours 46 minutes (just under four minutes per kilometer, faster\n",
    "than I and most of the other [Endcliffe Park\n",
    "Run](http://www.parkrun.org.uk/sheffieldhallam/) runners can do 5 km).\n",
    "What is the probability he would have won an Olympics if one had been\n",
    "held in 1946?\n",
    "\n",
    "To answer this question we need to generalize, but before we formalize\n",
    "the concept of generalization let’s introduce some formal representation\n",
    "of what it means to generalize in machine learning."
   ],
   "id": "6bceadbb-b6f4-438f-93c1-85498847a883"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Fit\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/olympic-marathon-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/olympic-marathon-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Our first objective will be to perform a Gaussian process fit to the\n",
    "data, we’ll do this using the [GPy\n",
    "software](https://github.com/SheffieldML/GPy)."
   ],
   "id": "8a2ba62b-0b4b-4834-8e64-da986d7abf02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ],
   "id": "3ebec30e-3141-47fb-af25-d9983e24d54f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ],
   "id": "49799dbd-fdaf-4f83-bcd6-04e51a7ca04a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command sets up the model, then `m_full.optimize()` optimizes\n",
    "the parameters of the covariance function and the noise level of the\n",
    "model. Once the fit is complete, we’ll try creating some test points,\n",
    "and computing the output of the GP model in terms of the mean and\n",
    "standard deviation of the posterior functions between 1870 and 2030. We\n",
    "plot the mean function and the standard deviation at 200 locations. We\n",
    "can obtain the predictions using `y_mean, y_var = m_full.predict(xt)`"
   ],
   "id": "4e69d2f5-97d9-4ac3-adfa-3ffdfd5811c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.linspace(1870,2030,200)[:,np.newaxis]\n",
    "yt_mean, yt_var = m_full.predict(xt)\n",
    "yt_sd=np.sqrt(yt_var)"
   ],
   "id": "c1f394fc-238f-4f6b-8a8a-09a55cedc27b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the results using the helper function in `mlai.plot`."
   ],
   "id": "c458ff9b-85f0-401a-a411-3ed1a6ddc090"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "148d0d4e-81ad-4c90-99a6-1ec38cc68e8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel=\"year\", ylabel=\"pace min/km\", fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename=\"olympic-marathon-gp.svg\", \n",
    "                  directory = \"./gp\",\n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "fb4a773a-762c-4fc3-965d-e92542144e1f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/olympic-marathon-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the Olympic Marathon data. The error\n",
    "bars are too large, perhaps due to the outlier from 1904.</i>"
   ],
   "id": "88866489-8d76-4562-b4a1-495689e5c83c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Quality\n",
    "\n",
    "In the fit we see that the error bars (coming mainly from the noise\n",
    "variance) are quite large. This is likely due to the outlier point in\n",
    "1904, ignoring that point we can see that a tighter fit is obtained. To\n",
    "see this make a version of the model, `m_clean`, where that point is\n",
    "removed."
   ],
   "id": "21dc2ba2-8595-46e1-aab3-1c20f5e03de1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean=np.vstack((x[0:2, :], x[3:, :]))\n",
    "y_clean=np.vstack((yhat[0:2, :], yhat[3:, :]))\n",
    "\n",
    "m_clean = GPy.models.GPRegression(x_clean,y_clean)\n",
    "_ = m_clean.optimize()"
   ],
   "id": "6737cbe7-6dad-432a-8eb4-2b18eda9556d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "526b0a2f-8c58-4ce8-8519-7c80d446b9d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_clean, scale=scale, offset=offset, ax=ax, xlabel='year', ylabel='pace min/km', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/olympic-marathon-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "3a450d6b-41b4-4755-aade-7bc78e931aae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateObject(rotationMatrix, handle):\n",
    "for i = 1:prod(size(handle))\n",
    "    type = get(handle(i), 'type');\n",
    "    if strcmp(type, 'text'):\n",
    "        xy = get(handle(i), 'position');\n",
    "        xy(1:2) = rotationMatrix*xy(1:2)';\n",
    "        set(handle(i), 'position', xy);\n",
    "    else:\n",
    "        xd = get(handle(i), 'xdata');\n",
    "        yd = get(handle(i), 'ydata');\n",
    "        new = rotationMatrix*[xd(:)'; yd(:)'];\n",
    "        set(handle(i), 'xdata', new(1, :));\n",
    "        set(handle(i), 'ydata', new(2, :));"
   ],
   "id": "d176c68f-216c-4926-bb85-6b7d4cab5817"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Covariance Parameters\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Can we determine covariance parameters from the data?\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=\\frac{1}{(2\\pi)^\\frac{n}{2}\\color{blue}{\\det{\\mathbf{K}}^{\\frac{1}{2}}}}\\color{red}{\\exp\\left(-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\log \\mathcal{N}\\left(\\mathbf{ y}|\\mathbf{0},\\mathbf{K}\\right)=&\\color{blue}{-\\frac{1}{2}\\log\\det{\\mathbf{K}}}\\color{red}{-\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}} \\\\ &-\\frac{n}{2}\\log2\\pi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(\\boldsymbol{ \\theta}) = \\color{blue}{\\frac{1}{2}\\log\\det{\\mathbf{K}}} + \\color{red}{\\frac{\\mathbf{ y}^{\\top}\\mathbf{K}^{-1}\\mathbf{ y}}{2}}\n",
    "$$"
   ],
   "id": "68e9c0ae-a10d-46be-a5f9-e33da117aa5b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacity Control through the Determinant\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-capacity.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-capacity.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The parameters are *inside* the covariance function (matrix).\n",
    "$$k_{i, j} = k(\\mathbf{ x}_i, \\mathbf{ x}_j; \\boldsymbol{ \\theta})$$\n",
    "\n",
    "$$\\mathbf{K}= \\mathbf{R}\\boldsymbol{ \\Lambda}^2 \\mathbf{R}^\\top$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimize-eigen.png\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "$\\boldsymbol{ \\Lambda}$ represents distance on axes. $\\mathbf{R}$ gives\n",
    "rotation.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "-   $\\boldsymbol{ \\Lambda}$ is *diagonal*,\n",
    "    $\\mathbf{R}^\\top\\mathbf{R}= \\mathbf{I}$.\n",
    "-   Useful representation since\n",
    "    $\\det{\\mathbf{K}} = \\det{\\boldsymbol{ \\Lambda}^2} = \\det{\\boldsymbol{ \\Lambda}}^2$."
   ],
   "id": "b2e0afe2-7b81-4377-a99c-395e21e5ca86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlai\n",
    "import mlai.plot as plot"
   ],
   "id": "c6733937-0a2f-4780-abf3-ab40002f6c27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_capacity(rotate_angle=np.pi/4, lambda1 = 0.5, lambda2 = 0.3, diagrams = './gp/')"
   ],
   "id": "5d41ad47-622d-4102-93f2-50804b8a5d66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "41b2e1f5-635d-4c58-921a-07ea76ddb538"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('gp-optimise-determinant{sample:0>3}.svg', \n",
    "                                          directory='./gp', \n",
    "                              sample=IntSlider(0, 0, 9, 1))"
   ],
   "id": "afc3fe20-bee5-4a98-b2ae-ef2b138d878f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant009.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The determinant of the covariance is dependent only on the\n",
    "eigenvalues. It represents the ‘footprint’ of the Gaussian.</i>"
   ],
   "id": "508513ad-2350-44bb-8364-1060dbfa53b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Data Fit\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "98c4d903-8268-40fa-9aa7-ab29ad5dbcc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf\n",
    "    includeText = [];\n",
    "    counter = 0;\n",
    "    plotWidth = 0.6*textWidth;\n",
    "    lambda1 = 3;\n",
    "    lambda2 = 1;\n",
    "    t = linspace(-pi, pi, 200);\n",
    "    R = [sqrt(2)/2 -sqrt(2)/2; sqrt(2)/2 sqrt(2)/2];\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)];\n",
    "    contourHand = line(xy(1, :), xy(2, :), 'color', blackColor);\n",
    "    xy = [lambda1*sin(t); lambda2*cos(t)]*2;\n",
    "    lim = [-1 1]*max([lambda1 lambda2])*2.2;\n",
    "    set(gca, 'xlim', lim, 'ylim', lim)\n",
    "    axis equal\n",
    "\n",
    "\n",
    "    contourHand = [contourHand line(xy(1, :), xy(2, :), 'color', blackColor)];\n",
    "    set(contourHand, 'linewidth', 2, 'color', redColor)\n",
    "    arrowHand = arrow([0 lambda1], [0 0]);\n",
    "    arrowHand = [arrowHand arrow([0 0], [0 lambda2])];\n",
    "    set(arrowHand, 'linewidth', 3, 'color', blackColor);\n",
    "    xlim = get(gca, 'xlim');\n",
    "    xspan = xlim(2) - xlim(1);\n",
    "    ylim = get(gca, 'ylim');\n",
    "    yspan = ylim(2) - ylim(1);\n",
    "    eigLabel = text(lambda1*0.5, -yspan*0.05, '$\\eigenvalue_1$', 'horizontalalignment', 'center');\n",
    "    eigLabel = [eigLabel text(-0.05*xspan, lambda2*0.5, '$\\eigenvalue_2$', 'horizontalalignment', 'center')];\n",
    "    xlabel('$\\dataScalar_1$')\n",
    "    ylabel('$\\dataScalar_2$')\n",
    "    \n",
    "    box off\n",
    "    xlim = get(gca, 'xlim');\n",
    "    ylim = get(gca, 'ylim');\n",
    "    line([xlim(1) xlim(1)], ylim, 'color', blackColor)\n",
    "    line(xlim, [ylim(1) ylim(1)], 'color', blackColor)\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    y = [1.2 1.4];\n",
    "    dataHand = line(y(1), y(2), 'marker', 'x', 'markersize', markerSize, 'linewidth', markerWidth, 'color', blackColor);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "\n",
    "    \n",
    "    rotateObject(rotationMatrix, arrowHand);\n",
    "    rotateObject(rotationMatrix, contourHand);\n",
    "    rotateObject(rotationMatrix, eigLabel);\n",
    "    \n",
    "    fileName = ['gpOptimiseQuadratic' num2str(counter)];\n",
    "    printLatexPlot(fileName, directory, plotWidth);\n",
    "    includeText = [includeText '\\only<' num2str(counter) '>{\\input{' directory fileName '.svg}}'];\n",
    "    counter = counter + 1;\n",
    "    \n",
    "    printLatexText(includeText, 'gpOptimiseQuadraticIncludeText.tex', directory)"
   ],
   "id": "751ac3d3-071e-4a99-a0fb-be5f0449685a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-quadratic002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The data fit term of the Gaussian process is a quadratic loss\n",
    "centered around zero. This has eliptical contours, the principal axes of\n",
    "which are given by the covariance matrix.</i>"
   ],
   "id": "3f6625f1-5f66-445a-97c7-d2e07a834030"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fit Term\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit-capacity.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit-capacity.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "a1fb9ddb-1ce1-437d-b52f-b9073b485402"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "099ed2eb-c672-455e-a771-f527e8d0b821"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import gp_tutorial"
   ],
   "id": "1153fee8-d73c-4964-95ac-e9e5fcbd6fb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(125)\n",
    "diagrams = './gp'\n",
    "\n",
    "black_color=[0., 0., 0.]\n",
    "red_color=[1., 0., 0.]\n",
    "blue_color=[0., 0., 1.]\n",
    "magenta_color=[1., 0., 1.]\n",
    "fontsize=18"
   ],
   "id": "c70e1e58-76f0-4e74-91f2-f63a7b38ceaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lim = [-2.2, 2.2]\n",
    "y_ticks = [-2, -1, 0, 1, 2]\n",
    "x_lim = [-2, 2]\n",
    "x_ticks = [-2, -1, 0, 1, 2]\n",
    "err_y_lim = [-12, 20]\n",
    "\n",
    "linewidth=3\n",
    "markersize=15\n",
    "markertype='.'"
   ],
   "id": "ae9cbf77-7f73-4863-9d63-d1a43a25662a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 6)[:, np.newaxis]\n",
    "xtest = np.linspace(x_lim[0], x_lim[1], 200)[:, np.newaxis]\n",
    "\n",
    "# True data\n",
    "true_kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "true_kern.rbf.lengthscale = 1.0\n",
    "true_kern.white.variance = 0.01\n",
    "K = true_kern.K(x) \n",
    "y = np.random.multivariate_normal(np.zeros((6,)), K, 1).T"
   ],
   "id": "17ac3cb2-b79e-42b8-b9d6-499eb66676a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fitted model\n",
    "kern = GPy.kern.RBF(1) + GPy.kern.White(1)\n",
    "kern.rbf.lengthscale = 1.0\n",
    "kern.white.variance = 0.01\n",
    "\n",
    "lengthscales = np.asarray([0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 4, 8, 16, 100])\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=plot.one_figsize)    \n",
    "fig2, ax2 = plt.subplots(figsize=plot.one_figsize)    \n",
    "line = ax2.semilogx(np.NaN, np.NaN, 'x-', \n",
    "                    color=black_color)\n",
    "ax.set_ylim(err_y_lim)\n",
    "ax.set_xlim([0.025, 32])\n",
    "ax.grid(True)\n",
    "ax.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "ax.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "\n",
    "err = np.zeros_like(lengthscales)\n",
    "err_log_det = np.zeros_like(lengthscales)\n",
    "err_fit = np.zeros_like(lengthscales)\n",
    "\n",
    "counter = 0\n",
    "for i, ls in enumerate(lengthscales):\n",
    "        kern.rbf.lengthscale=ls\n",
    "        K = kern.K(x) \n",
    "        invK, L, Li, log_det_K = GPy.util.linalg.pdinv(K)\n",
    "        err[i] = 0.5*(log_det_K + np.dot(np.dot(y.T,invK),y))\n",
    "        err_log_det[i] = 0.5*log_det_K\n",
    "        err_fit[i] = 0.5*np.dot(np.dot(y.T,invK), y)\n",
    "        Kx = kern.K(x, xtest)\n",
    "        ypred_mean = np.dot(np.dot(Kx.T, invK), y)\n",
    "        ypred_var = kern.Kdiag(xtest) - np.sum((np.dot(Kx.T,invK))*Kx.T, 1)\n",
    "        ypred_sd = np.sqrt(ypred_var)\n",
    "        ax1.clear()\n",
    "        _ = gp_tutorial.gpplot(xtest.flatten(),\n",
    "                               ypred_mean.flatten(),\n",
    "                               ypred_mean.flatten()-2*ypred_sd.flatten(),\n",
    "                               ypred_mean.flatten()+2*ypred_sd.flatten(), \n",
    "                               ax=ax1)\n",
    "        x_lim = ax1.get_xlim()\n",
    "        ax1.set_ylabel('$f(x)$', fontsize=fontsize)\n",
    "        ax1.set_xlabel('$x$', fontsize=fontsize)\n",
    "\n",
    "        p = ax1.plot(x, y, markertype, color=black_color, markersize=markersize, linewidth=linewidth)\n",
    "        ax1.set_ylim(y_lim)\n",
    "        ax1.set_xlim(x_lim)                                    \n",
    "        ax1.set_xticks(x_ticks)\n",
    "        #ax.set(box=False)\n",
    "           \n",
    "        ax1.plot([x_lim[0], x_lim[0]], y_lim, color=black_color)\n",
    "        ax1.plot(x_lim, [y_lim[0], y_lim[0]], color=black_color)\n",
    "\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig1,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "\n",
    "        ax2.clear()\n",
    "        t = ax2.semilogx(lengthscales[0:i+1], err[0:i+1], 'x-', \n",
    "                        color=magenta_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t2 = ax2.semilogx(lengthscales[0:i+1], err_log_det[0:i+1], 'x-', \n",
    "                         color=blue_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        t3 = ax2.semilogx(lengthscales[0:i+1], err_fit[0:i+1], 'x-', \n",
    "                         color=red_color, \n",
    "                        markersize=markersize,\n",
    "                        linewidth=linewidth)\n",
    "        ax2.set_ylim(err_y_lim)\n",
    "        ax2.set_xlim([0.025, 32])\n",
    "        ax2.set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "        ax2.set_xticklabels(['$10^{-2}$', '$10^{-1}$', '$10^0$', '$10^1$', '$10^2$'])\n",
    "\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax2.set_ylabel('negative log likelihood', fontsize=fontsize)\n",
    "        ax2.set_xlabel('length scale, $\\ell$', fontsize=fontsize)\n",
    "        file_name = 'gp-optimise{counter:0>3}.svg'.format(counter=counter)\n",
    "        mlai.write_figure(os.path.join(diagrams, file_name),\n",
    "                          figure=fig2,\n",
    "                          transparent=True)\n",
    "        counter += 1\n",
    "        #ax.set_box(False)\n",
    "        xlim = ax2.get_xlim()\n",
    "        ax2.plot([xlim[0], xlim[0]], err_y_lim, color=black_color)\n",
    "        ax2.plot(xlim, [err_y_lim[0], err_y_lim[0]], color=black_color)"
   ],
   "id": "64c5beb2-c6c3-4b34-9395-877e9d8866b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise006.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise010.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise016.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise021.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Variation in the data fit term, the capacity term and the\n",
    "negative log likelihood for different lengthscales.</i>"
   ],
   "id": "a59a67d6-1dfc-487e-8bc8-f9b7df8592a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene Expression Example\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/della-gatta-gene-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/della-gatta-gene-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "We now consider an example in gene expression. Gene expression is the\n",
    "measurement of mRNA levels expressed in cells. These mRNA levels show\n",
    "which genes are ‘switched on’ and producing data. In the example we will\n",
    "use a Gaussian process to determine whether a given gene is active, or\n",
    "we are merely observing a noise response."
   ],
   "id": "472d78e6-edb3-4b4c-b99d-99bf8206b30a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Della Gatta Gene Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/della-gatta-gene-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/della-gatta-gene-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "-   Given given expression levels in the form of a time series from\n",
    "    Della Gatta et al. (2008)."
   ],
   "id": "b28936fc-0ffb-42c9-9694-cd6481d4a1b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ],
   "id": "d2e860e6-ca09-4566-b34e-a033ca866268"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.della_gatta_TRP63_gene_expression(data_set='della_gatta',gene_number=937)\n",
    "\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())"
   ],
   "id": "ef5393d1-bdfa-4dbc-9a73-dcc2102f8571"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "25ea213b-a190-4506-a734-e7a2bc2d500e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xlim = (-20,260)\n",
    "ylim = (5, 7.5)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('time/min', fontsize=20)\n",
    "ax.set_ylabel('expression', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(figure=fig, \n",
    "                  filename='./datasets/della-gatta-gene.svg', \n",
    "                  transparent=True, \n",
    "                  frameon=True)"
   ],
   "id": "7de73564-810a-4657-b383-9bee40bfd9b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//datasets/della-gatta-gene.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gene expression levels over time for a gene from data\n",
    "provided by Della Gatta et al. (2008). We would like to understand\n",
    "whether there is signal in the data, or we are only observing noise.</i>\n",
    "\n",
    "-   Want to detect if a gene is expressed or not, fit a GP to each gene\n",
    "    Kalaitzis and Lawrence (2011).\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip1\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Freddie Kalaitzis\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/freddie-kalaitzis.jpg\" clip-path=\"url(#clip1)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//health/1471-2105-12-180_1.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The example is taken from the paper “A Simple Approach to\n",
    "Ranking Differentially Expressed Gene Expression Time Courses through\n",
    "Gaussian Process Regression.” Kalaitzis and Lawrence (2011).</i>\n",
    "\n",
    "<center>\n",
    "\n",
    "<http://www.biomedcentral.com/1471-2105/12/180>\n",
    "\n",
    "</center>\n",
    "\n",
    "Our first objective will be to perform a Gaussian process fit to the\n",
    "data, we’ll do this using the [GPy\n",
    "software](https://github.com/SheffieldML/GPy)."
   ],
   "id": "f004799f-1e54-469f-a7ac-e86aabd7488f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ],
   "id": "da9551f3-c325-402b-b23f-bbfacbed324d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "m_full.kern.lengthscale=50\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ],
   "id": "da60f603-7042-472a-8551-276c98aed335"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the length scale parameter (which here actually represents a\n",
    "*time scale* of the covariance function) to a reasonable value. Default\n",
    "would be 1, but here we set it to 50 minutes, given points are arriving\n",
    "across zero to 250 minutes."
   ],
   "id": "001c5346-9217-42d4-a06b-d9f9fb8a91c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.linspace(-20,260,200)[:,np.newaxis]\n",
    "yt_mean, yt_var = m_full.predict(xt)\n",
    "yt_sd=np.sqrt(yt_var)"
   ],
   "id": "a4fefe65-f461-4699-b2c6-07e0980d9055"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the results using the helper function in `mlai.plot`."
   ],
   "id": "bfb16ef3-8f3f-4b40-870e-ea5768beb0ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "e87e5a38-7309-4179-84ba-31030e6ce8df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel='time/min', ylabel='expression', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('log likelihood: {ll:.3}'.format(ll=m_full.log_likelihood()), fontsize=20)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/della-gatta-gene-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "309f9db0-f07d-4ce6-aae0-be53099630a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Result of the fit of the Gaussian process model with the time\n",
    "scale parameter initialized to 50 minutes.</i>\n",
    "\n",
    "Now we try a model initialized with a longer length scale."
   ],
   "id": "11b17fd0-6fc5-4411-9c25-caa519824df1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full2 = GPy.models.GPRegression(x,yhat)\n",
    "m_full2.kern.lengthscale=2000\n",
    "_ = m_full2.optimize() # Optimize parameters of covariance function"
   ],
   "id": "dff0d9a0-4add-4541-b792-8cfcd949e0b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "85233cfc-d1f9-45ec-b2c9-d96ebe4e166e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full2, scale=scale, offset=offset, ax=ax, xlabel='time/min', ylabel='expression', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('log likelihood: {ll:.3}'.format(ll=m_full2.log_likelihood()), fontsize=20)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/della-gatta-gene-gp2.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "18333280-deb2-48eb-8081-39fa9f42a2aa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp2.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Result of the fit of the Gaussian process model with the time\n",
    "scale parameter initialized to 2000 minutes.</i>\n",
    "\n",
    "Now we try a model initialized with a lower noise."
   ],
   "id": "1c24780b-57db-4d7a-a558-0931a358d590"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full3 = GPy.models.GPRegression(x,yhat)\n",
    "m_full3.kern.lengthscale=20\n",
    "m_full3.likelihood.variance=0.001\n",
    "_ = m_full3.optimize() # Optimize parameters of covariance function"
   ],
   "id": "d14ae9d8-90fa-40a9-b783-d22201faf557"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "18f1aa54-94a2-453a-9c42-58e61aad4067"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full3, scale=scale, offset=offset, ax=ax, xlabel='time/min', ylabel='expression', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('log likelihood: {ll:.3}'.format(ll=m_full3.log_likelihood()), fontsize=20)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='./gp/della-gatta-gene-gp3.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "ba8eb562-d4c6-4f88-be98-519c1f92d934"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp3.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Result of the fit of the Gaussian process model with the\n",
    "noise initialized low (standard deviation 0.1) and the time scale\n",
    "parameter initialized to 20 minutes.</i>"
   ],
   "id": "14cdc2b6-a1cf-47b1-a35c-010c0aaa2dd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "cd13ff36-40a5-4f51-9fcd-99998a455736"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.multiple_optima(diagrams='./gp')"
   ],
   "id": "0ed581ca-2286-4abf-b6af-0686681c4a6f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima000.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i></i>\n",
    "\n",
    "<!--\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima001.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">-->"
   ],
   "id": "fb382761-a501-4104-a6fe-5b3dc0d8f7ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Prediction of Malaria Incidence in Uganda\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip2\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Martin Mubangizi\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/martin-mubangizi.png\" clip-path=\"url(#clip2)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip3\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Ricardo Andrade Pacecho\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/ricardo-andrade-pacheco.png\" clip-path=\"url(#clip3)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip4\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "John Quinn\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/john-quinn.jpg\" clip-path=\"url(#clip4)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "As an example of using Gaussian process models within the full pipeline\n",
    "from data to decsion, we’ll consider the prediction of Malaria incidence\n",
    "in Uganda. For the purposes of this study malaria reports come in two\n",
    "forms, HMIS reports from health centres and Sentinel data, which is\n",
    "curated by the WHO. There are limited sentinel sites and many HMIS\n",
    "sites.\n",
    "\n",
    "The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in\n",
    "collaboration with John Quinn and Martin Mubangizi (Andrade-Pacheco et\n",
    "al., 2014; Mubangizi et al., 2014). John and Martin were initally from\n",
    "the AI-DEV group from the University of Makerere in Kampala and more\n",
    "latterly they were based at UN Global Pulse in Kampala. You can see the\n",
    "work summarized on the UN Global Pulse [disease outbreaks project site\n",
    "here](https://diseaseoutbreaks.unglobalpulse.net/uganda/).\n",
    "\n",
    "-   See [UN Global Pulse Disease Outbreaks\n",
    "    Site](https://diseaseoutbreaks.unglobalpulse.net/uganda/)\n",
    "\n",
    "Malaria data is spatial data. Uganda is split into districts, and health\n",
    "reports can be found for each district. This suggests that models such\n",
    "as conditional random fields could be used for spatial modelling, but\n",
    "there are two complexities with this. First of all, occasionally\n",
    "districts split into two. Secondly, sentinel sites are a specific\n",
    "location within a district, such as Nagongera which is a sentinel site\n",
    "based in the Tororo district.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//health/uganda-districts-2006.png\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>Ugandan districts. Data SRTM/NASA from\n",
    "<https://dds.cr.usgs.gov/srtm/version2_1>.</i>\n",
    "\n",
    "(Andrade-Pacheco et al., 2014; Mubangizi et al., 2014)\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//health/Kapchorwa_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Kapchorwa District, home district of Stephen\n",
    "Kiprotich.</i>\n",
    "\n",
    "Stephen Kiprotich, the 2012 gold medal winner from the London Olympics,\n",
    "comes from Kapchorwa district, in eastern Uganda, near the border with\n",
    "Kenya.\n",
    "\n",
    "The common standard for collecting health data on the African continent\n",
    "is from the Health management information systems (HMIS). However, this\n",
    "data suffers from missing values (Gething et al., 2006) and diagnosis of\n",
    "diseases like typhoid and malaria may be confounded.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//health/Tororo_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Tororo district, where the sentinel site, Nagongera, is\n",
    "located.</i>\n",
    "\n",
    "[World Health Organization Sentinel Surveillance\n",
    "systems](https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/)\n",
    "are set up “when high-quality data are needed about a particular disease\n",
    "that cannot be obtained through a passive system”. Several sentinel\n",
    "sites give accurate assessment of malaria disease levels in Uganda,\n",
    "including a site in Nagongera.\n",
    "\n",
    "<img class=\"negate\" src=\"https://inverseprobability.com/talks/../slides/diagrams//health/sentinel_nagongera.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Sentinel and HMIS data along with rainfall and temperature\n",
    "for the Nagongera sentinel station in the Tororo district.</i>\n",
    "\n",
    "In collaboration with the AI Research Group at Makerere we chose to\n",
    "investigate whether Gaussian process models could be used to assimilate\n",
    "information from these two different sources of disease informaton.\n",
    "Further, we were interested in whether local information on rainfall and\n",
    "temperature could be used to improve malaria estimates.\n",
    "\n",
    "The aim of the project was to use WHO Sentinel sites, alongside rainfall\n",
    "and temperature, to improve predictions from HMIS data of levels of\n",
    "malaria.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//health/Mubende_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Mubende District.</i>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//health/mubende.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Prediction of malaria incidence in Mubende.</i>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>The project arose out of the Gaussian process summer school\n",
    "held at Makerere in Kampala in 2013. The school led, in turn, to the\n",
    "Data Science Africa initiative.</i>"
   ],
   "id": "3cf8d870-271c-4d25-bdf6-9595111db750"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Warning Systems\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//health/Kabarole_District_in_Uganda.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Kabarole district in Uganda.</i>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//health/kabarole.gif\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Estimate of the current disease situation in the Kabarole\n",
    "district over time. Estimate is constructed with a Gaussian process with\n",
    "an additive covariance funciton.</i>\n",
    "\n",
    "Health monitoring system for the Kabarole district. Here we have fitted\n",
    "the reports with a Gaussian process with an additive covariance\n",
    "function. It has two components, one is a long time scale component (in\n",
    "red above) the other is a short time scale component (in blue).\n",
    "\n",
    "Monitoring proceeds by considering two aspects of the curve. Is the blue\n",
    "line (the short term report signal) above the red (which represents the\n",
    "long term trend? If so we have higher than expected reports. If this is\n",
    "the case *and* the gradient is still positive (i.e. reports are going\n",
    "up) we encode this with a *red* color. If it is the case and the\n",
    "gradient of the blue line is negative (i.e. reports are going down) we\n",
    "encode this with an *amber* color. Conversely, if the blue line is below\n",
    "the red *and* decreasing, we color *green*. On the other hand if it is\n",
    "below red but increasing, we color *yellow*.\n",
    "\n",
    "This gives us an early warning system for disease. Red is a bad\n",
    "situation getting worse, amber is bad, but improving. Green is good and\n",
    "getting better and yellow good but degrading.\n",
    "\n",
    "Finally, there is a gray region which represents when the scale of the\n",
    "effect is small.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//health/monitor.gif\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>The map of Ugandan districts with an overview of the Malaria\n",
    "situation in each district.</i>\n",
    "\n",
    "These colors can now be observed directly on a spatial map of the\n",
    "districts to give an immediate impression of the current status of the\n",
    "disease across the country."
   ],
   "id": "f0eb7d3d-8eb8-4045-b927-8d41329d0f12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/add-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/add-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "99f426b8-d6f1-43cd-a064-6f8269b5e3b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "f4c78fa5-8d76-4445-9e26-761b1526db79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.Kernel"
   ],
   "id": "d0ead5f6-025d-4c43-8d38-d7fb2ca6d186"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "ea968422-8725-4b60-a38a-4cf2248ef58d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.linear_cov"
   ],
   "id": "e02170a1-0da3-4fb1-8af3-c55102a72643"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "221ca71f-2744-4c9c-b112-bc8484c5cfd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.eq_cov"
   ],
   "id": "33cf9d3c-c717-41ba-8d9b-6453e5a9e7ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "d4cce7ce-add4-4b8f-83f1-402a99b4a771"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.add_cov"
   ],
   "id": "798f7fbc-1dc0-42c3-a287-916ea130c0f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(function=add_cov,\n",
    "                     name='Additive',\n",
    "                     shortname='add',                     \n",
    "                     formula='\\kernelScalar_f(\\inputVector, \\inputVector^\\prime) = \\kernelScalar_g(\\inputVector, \\inputVector^\\prime) + \\kernelScalar_h(\\inputVector, \\inputVector^\\prime)', \n",
    "                     kerns=[linear_cov, eq_cov], \n",
    "                     kern_args=[{'variance': 25}, {'lengthscale' : 0.2}])"
   ],
   "id": "b6609b6d-d5a8-4e41-8cb4-ce4fdd7c3b51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "d4655af0-c58b-4db8-ab70-0b81ee0b9690"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.covariance_func(kernel=kernel, diagrams='./kern/')"
   ],
   "id": "e135e233-7c8c-45d9-afe1-7b65ebc1d9b9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additive covariance function is derived from considering the result\n",
    "of summing two Gaussian processes together. If the first Gaussian\n",
    "process is $g(\\cdot)$, governed by covariance $k_g(\\cdot, \\cdot)$ and\n",
    "the second process is $h(\\cdot)$, governed by covariance\n",
    "$k_h(\\cdot, \\cdot)$ then the combined process\n",
    "$f(\\cdot) = g(\\cdot) + h(\\cdot)$ is govererned by a covariance function,\n",
    "$$\n",
    "k_f(\\mathbf{ x}, \\mathbf{ x}^\\prime) = k_g(\\mathbf{ x}, \\mathbf{ x}^\\prime) + k_h(\\mathbf{ x}, \\mathbf{ x}^\\prime)\n",
    "$$\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k_f(\\mathbf{ x}, \\mathbf{ x}^\\prime) = k_g(\\mathbf{ x}, \\mathbf{ x}^\\prime) + k_h(\\mathbf{ x}, \\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/add_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/add_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>An additive covariance function formed by combining a linear\n",
    "and an exponentiated quadratic covariance functions.</i>"
   ],
   "id": "a805c41c-6852-4bf8-a609-76cefb4b9838"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of US Birth Rates\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/bda-forecasting.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/bda-forecasting.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip5\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Aki Vehtari\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/aki-vehtari.jpg\" clip-path=\"url(#clip5)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ml/bialik-fridaythe13th-1.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>This is a retrospective analysis of US births by Aki Vehtari.\n",
    "The challenges of forecasting. Even with seasonal and weekly effects\n",
    "removed there are significant effects on holidays, weekends, etc.</i>\n",
    "\n",
    "There’s a nice analysis of US birth rates by Gaussian processes with\n",
    "additive covariances in Gelman et al. (2013). A combination of\n",
    "covariance functions are used to take account of weekly and yearly\n",
    "trends. The analysis is summarized on the cover of the book.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ml/bda_cover_1.png\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//ml/bda_cover.png\" style=\"width:80%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Two different editions of Bayesian Data Analysis (Gelman et\n",
    "al., 2013).</i>"
   ],
   "id": "75dbcfb5-a487-42e5-b77e-c4ae68b1a9c9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Function Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/basis-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/basis-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The fixed basis function covariance just comes from the properties of a\n",
    "multivariate Gaussian, if we decide $$\n",
    "\\mathbf{ f}=\\boldsymbol{ \\Phi}\\mathbf{ w}\n",
    "$$ and then we assume $$\n",
    "\\mathbf{ w}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\mathbf{I}\\right)\n",
    "$$ then it follows from the properties of a multivariate Gaussian that\n",
    "$$\n",
    "\\mathbf{ f}\\sim \\mathcal{N}\\left(\\mathbf{0},\\alpha\\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top\\right)\n",
    "$$ meaning that the vector of observations from the function is jointly\n",
    "distributed as a Gaussian process and the covariance matrix is\n",
    "$\\mathbf{K}= \\alpha\\boldsymbol{ \\Phi}\\boldsymbol{ \\Phi}^\\top$, each\n",
    "element of the covariance matrix can then be found as the inner product\n",
    "between two rows of the basis funciton matrix."
   ],
   "id": "c8e324f2-d5f6-467d-a1af-3ac1dad1cb39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "da6ff9f5-5bbf-4913-aafc-c1991e6130bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.basis_cov"
   ],
   "id": "b0a72889-ad65-4beb-acbc-70782baf9961"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "aa05f43d-3b39-4195-8568-d62130d5db49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.radial"
   ],
   "id": "94b285e6-c689-41fc-a6a4-afec88e89aa0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "8ba89bf1-7393-4cea-b4ed-9d7ddddda700"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basis = mlai.Basis(function=radial, \n",
    "                   number=3,\n",
    "                   data_limits=[-0.5, 0.5], \n",
    "                   width=0.125)\n",
    "kernel = mlai.Kernel(function=basis_cov,\n",
    "                     name='Basis',\n",
    "                     shortname='basis',                  \n",
    "                     formula='\\kernel(\\inputVector, \\inputVector^\\prime) = \\basisVector(\\inputVector)^\\top \\basisVector(\\inputVector^\\prime)',\n",
    "                     basis=basis)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ],
   "id": "19150f18-19bf-49b8-8d7f-ea75cabf588a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\boldsymbol{ \\phi}(\\mathbf{ x})^\\top \\boldsymbol{ \\phi}(\\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/basis_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/basis_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>A covariance function based on a non-linear basis given by\n",
    "$\\boldsymbol{ \\phi}(\\mathbf{ x})$.</i>"
   ],
   "id": "8406db70-f08a-44fa-a446-6dc47900beb9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/brownian-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/brownian-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "4a13411c-b17a-4c74-9a45-e6cf7c23a618"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "d9391ebd-975f-4c73-a016-5d46812cf21c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.brownian_cov"
   ],
   "id": "351722b4-593a-4a46-bc59-4953d4b1e781"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "ceff2f49-be5c-4bc1-badb-6166956ca969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.linspace(0, 2, 200)[:, np.newaxis]\n",
    "kernel = mlai.Kernel(function=brownian_cov,\n",
    "                     name='Brownian',\n",
    "                     formula='\\kernelScalar(t, t^\\prime)=\\alpha \\min(t, t^\\prime)',\n",
    "                     shortname='brownian')\n",
    "plot.covariance_func(kernel, t, diagrams='./kern/')"
   ],
   "id": "3f0f7a82-942a-4565-99e1-ced350946c97"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brownian motion is also a Gaussian process. It follows a Gaussian random\n",
    "walk, with diffusion occuring at each time point driven by a Gaussian\n",
    "input. This implies it is both Markov and Gaussian. The covariance\n",
    "function for Brownian motion has the form $$\n",
    "k(t, t^\\prime)=\\alpha \\min(t, t^\\prime)\n",
    "$$\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(t, t^\\prime)=\\alpha \\min(t, t^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/brownian_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/brownian_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Brownian motion covariance function.</i>"
   ],
   "id": "78175826-85de-40ce-87e4-a000be6ee622"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/mlp-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/mlp-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "7049f500-db41-4116-8ff0-c2e59985afa2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "50e29f4b-35a7-45fc-a6f0-58185882a1d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.mlp_cov"
   ],
   "id": "4d2cb368-fd2a-408b-a204-b9a12a09622f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "74a92768-505c-48b2-88b7-0fed5a1cd866"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=mlp_cov,\n",
    "                     name='Multilayer Perceptron',\n",
    "                     shortname='mlp',                    \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\arcsin\\left(\\frac{w \\inputVector^\\top \\inputVector^\\prime + b}{\\sqrt{\\left(w \\inputVector^\\top \\inputVector + b + 1\\right)\\left(w \\left.\\inputVector^\\prime\\right.^\\top \\inputVector^\\prime + b + 1\\right)}}\\right)',\n",
    "                     w=5, b=0.5)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ],
   "id": "7805cf5b-bcc4-4e64-8daa-d527dc52af1f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-layer perceptron (MLP) covariance, also known as the neural\n",
    "network covariance or the arcsin covariance, is derived by considering\n",
    "the infinite limit of a neural network.\n",
    "\n",
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\arcsin\\left(\\frac{w \\mathbf{ x}^\\top \\mathbf{ x}^\\prime + b}{\\sqrt{\\left(w \\mathbf{ x}^\\top \\mathbf{ x}+ b + 1\\right)\\left(w \\left.\\mathbf{ x}^\\prime\\right.^\\top \\mathbf{ x}^\\prime + b + 1\\right)}}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/mlp_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/mlp_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>The multi-layer perceptron covariance function. This is\n",
    "derived by considering the infinite limit of a neural network with\n",
    "probit activation functions.</i>"
   ],
   "id": "bca175bf-f71a-4f3e-ab7f-b12e8ab5f0a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/relu-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/relu-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "e0d86096-df2d-415e-8a0f-365f243e562e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "4ce809ea-c9f4-41c4-bf96-9c9945a62d35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.relu_cov"
   ],
   "id": "a718abf1-74a7-4fe0-a93e-b3f9cae38b4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "ff46b68d-c6c6-41ab-9f21-8ba94f38a9d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=relu_cov,\n",
    "                     name='RELU',\n",
    "                     shortname='relu',                   \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\arcsin\\left(\\frac{w \\inputVector^\\top \\inputVector^\\prime + b}{\\sqrt{\\left(w \\inputVector^\\top \\inputVector + b + 1\\right)\\left(w \\left.\\inputVector^\\prime\\right.^\\top \\inputVector^\\prime + b + 1\\right)}}\\right)',\n",
    "                     w=5, b=0.5)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ],
   "id": "7dfa46d0-26b2-4dd8-834f-1167639d784e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \n",
    "\\alpha \\arcsin\\left(\\frac{w \\mathbf{ x}^\\top \\mathbf{ x}^\\prime + b}\n",
    "{\\sqrt{\\left(w \\mathbf{ x}^\\top \\mathbf{ x}+ b + 1\\right)\n",
    "\\left(w \\left.\\mathbf{ x}^\\prime\\right.^\\top \\mathbf{ x}^\\prime + b + 1\\right)}}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/relu_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/relu_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Rectified linear unit covariance function.</i>"
   ],
   "id": "81b2e7e0-6fb3-49ee-9d26-2adf5ac76ae0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinc Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/sinc-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/sinc-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Another approach to developing covariance function exploits Bochner’s\n",
    "theorem Bochner (1959). Bochner’s theorem tells us that any positve\n",
    "filter in Fourier space implies has an associated Gaussian process with\n",
    "a stationary covariance function. The covariance function is the\n",
    "*inverse Fourier transform* of the filter applied in Fourier space.\n",
    "\n",
    "For example, in signal processing, *band limitations* are commonly\n",
    "applied as an assumption. For example, we may believe that no frequency\n",
    "above $w=2$ exists in the signal. This is equivalent to a rectangle\n",
    "function being applied as a the filter in Fourier space.\n",
    "\n",
    "The inverse Fourier transform of the rectangle function is the\n",
    "$\\text{sinc}(\\cdot)$ function. So the sinc is a valid covariance\n",
    "function, and it represents *band limited* signals.\n",
    "\n",
    "Note that other covariance functions we’ve introduced can also be\n",
    "interpreted in this way. For example, the exponentiated quadratic\n",
    "covariance function can be Fourier transformed to see what the implied\n",
    "filter in Fourier space is. The Fourier transform of the exponentiated\n",
    "quadratic is an exponentiated quadratic, so the standard EQ-covariance\n",
    "implies a EQ filter in Fourier space."
   ],
   "id": "9f841200-fa78-4db8-9f47-b9bfa23e3e1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "257616fa-c5ea-4fa4-994a-e89b9beda54e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.sinc_cov"
   ],
   "id": "1c178168-67f0-481e-b319-a20c3c7ed96d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "8372aa12-ae7c-4c41-b155-53c0cc2bfc37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=sinc_cov,\n",
    "                     name='Sinc',\n",
    "                     shortname='sinc',                   \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\text{sinc}\\left(\\pi w r\\right)',\n",
    "                     w=2)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ],
   "id": "9c4a7684-8d74-477b-bc4e-37ae38af9764"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\text{sinc}\\left(\\pi w r\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/sinc_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/sinc_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Sinc covariance function.</i>"
   ],
   "id": "574937c8-e426-4b7b-bf38-14ff1f7052e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/poly-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/poly-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "91b33548-7e20-451b-af43-eb3b25a9feab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "b8c9b9b9-dae1-4bf6-8ffa-495d649b2bf8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.polynomial_cov"
   ],
   "id": "ac844710-52af-43df-9e27-a12300a21e13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "c894c8bb-3846-4d12-b15a-d3e0b672628e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=polynomial_cov,\n",
    "                     name='Polynomial',\n",
    "                     shortname='polynomial',                     \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha(w \\inputVector^\\top\\inputVector^\\prime + b)^d',\n",
    "                     degree=5)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ],
   "id": "a5e9d719-7aaa-46ca-9b6b-4a26b9357ebf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha(w \\mathbf{ x}^\\top\\mathbf{ x}^\\prime + b)^d$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/polynomial_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/polynomial_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Polynomial covariance function.</i>"
   ],
   "id": "2da77fbe-9ee5-4b99-befc-3e8df9f04d63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodic Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/periodic-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/periodic-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "2b792503-bc20-42cf-ad22-de2a071aa628"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "e1a0028c-5b26-43e9-90ef-a515d7c9d4f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.periodic_cov"
   ],
   "id": "5c0cea23-1fb7-48ab-af6c-2c9dc6ad09c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "631afe0e-90c8-45c4-b5f0-bc429c1b72e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mlai.Kernel(function=periodic_cov,\n",
    "                     name='Periodic',\n",
    "                     shortname='periodic',                   \n",
    "                     formula='\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha\\exp\\left(\\frac{-2\\sin(\\pi rw)^2}{\\lengthScale^2}\\right)',\n",
    "                     lengthscale=1.0)\n",
    "                     \n",
    "plot.covariance_func(kernel, diagrams='./kern/')"
   ],
   "id": "4b96cb2d-26d3-470e-90b2-c3a51ea5b2c2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha\\exp\\left(\\frac{-2\\sin(\\pi rw)^2}{\\ell^2}\\right)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/periodic_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/periodic_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Periodic covariance function.</i>"
   ],
   "id": "d487ca84-feef-461a-b81e-e9152b960046"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model of Coregionalization Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/lmc-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/lmc-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "8eebbbe5-45f6-474a-b689-bd5dc0773463"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s lmc_cov mlai.py"
   ],
   "id": "830b4a57-08a9-4acf-83ef-d605155898cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "dd3a0844-e3c9-4b35-8ba1-b48722cd5309"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=lmc_cov, subkernel=eq_cov,\n",
    "                                         B = np.asarray([[1, 0.5],[0.5, 1.5]]))"
   ],
   "id": "9d36069c-431d-409e-aff5-a651da65b828"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ],
   "id": "6a9d206a-23d6-4328-b9e2-7164dbf8374d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ],
   "id": "bef723bb-0147-404c-9fdd-0163bbf54177"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='./kern', \n",
    "                    filename='lmc_covariance.html')"
   ],
   "id": "33c270d7-337c-478c-9ce5-4925c853159a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(i, j, \\mathbf{ x}, \\mathbf{ x}^\\prime) = b_{i,j} k(\\mathbf{ x}, \\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/lmc_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/lmc_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Linear model of coregionalization covariance function.</i>"
   ],
   "id": "04823e4f-4a78-4a5a-9cb9-9c69fd3acf28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic Coregionalization Model Covariance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_kern/includes/icm-covariance.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/icm-covariance.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "86cab39e-d807-421e-a278-e939987168bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "a885e1f3-e1a5-4e8b-bf3a-716bd0f1f480"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -n mlai.icm_cov"
   ],
   "id": "dc2a38c7-b974-43b8-931c-e8c8fa774c1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai\n",
    "import numpy as np"
   ],
   "id": "369dd8ce-eaa1-473a-a75d-d4f416172006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=icm_cov, subkernel=eq_cov,\n",
    "                                         B = np.asarray([[1, 0.5],[0.5, 1.5]]))"
   ],
   "id": "46cd8dda-b57d-4ba0-8b19-6f2231ca019f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ],
   "id": "9f501c11-bf40-4112-b978-aaa0f3825eb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ],
   "id": "64400d3b-e258-4eba-a956-f79ca9a8196a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='./kern', \n",
    "                    filename='icm_covariance.html')"
   ],
   "id": "8ae52d43-4cb2-47fa-96fb-5ba8e36c9378"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "$$k(i, j, \\mathbf{ x}, \\mathbf{ x}^\\prime) = b_{i,j} k(\\mathbf{ x}, \\mathbf{ x}^\\prime)$$\n",
    "\n",
    "</center>\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img src=\"../slides/diagrams/kern/icm_covariance.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/icm_covariance.gif\" style=\"width:100%\">\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Figure: <i>Intrinsic coregionalization model covariance function.</i>"
   ],
   "id": "6719e850-d434-456f-a03d-7f9effeb7105"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Gaussian Processes\n",
    "\n",
    "-   *Deep Gaussian Processes and Variational Propagation of Uncertainty*\n",
    "    Damianou (2015)"
   ],
   "id": "f1530f4e-dc23-4dae-b701-7bae49667f2b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Priors\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/mackay-bathwater.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/mackay-bathwater.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Even in the early days of Gaussian processes in machine learning, it was\n",
    "understood that we were throwing something fundamental away. This is\n",
    "perhaps captured best by David MacKay in his 1997 NeurIPS tutorial on\n",
    "Gaussian processes, where he asked “Have we thrown out the baby with the\n",
    "bathwater?”. The quote below is from his summarization paper.\n",
    "\n",
    "> According to the hype of 1987, neural networks were meant to be\n",
    "> intelligent models which discovered features and patterns in data.\n",
    "> Gaussian processes in contrast are simply smoothing devices. How can\n",
    "> Gaussian processes possibly replace neural networks? What is going on?\n",
    ">\n",
    "> MacKay (n.d.)"
   ],
   "id": "2a49e868-e376-469d-8c05-dc3f9c6259fb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "acd115ac-b928-4617-867e-64b9f28820c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install daft"
   ],
   "id": "2ade4b87-8fb8-4945-957d-5b5ff23671dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# Comment for google colab (no latex available)\n",
    "#matplotlib.rc('text', usetex=True)\n",
    "#matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]"
   ],
   "id": "d352a786-ebc3-4945-a485-866b69960198"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "cfd79130-cc83-4025-a7d8-122fed5cf3ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot.deep_nn(diagrams='./deepgp/')"
   ],
   "id": "25ba3005-3caa-4030-985a-6c8471a72404"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn2.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A deep neural network. Input nodes are shown at the bottom.\n",
    "Each hidden layer is the result of applying an affine transformation to\n",
    "the previous layer and placing through an activation function.</i>\n",
    "\n",
    "Mathematically, each layer of a neural network is given through\n",
    "computing the activation function, $\\phi(\\cdot)$, contingent on the\n",
    "previous layer, or the inputs. In this way the activation functions, are\n",
    "composed to generate more complex interactions than would be possible\n",
    "with any single layer. $$\n",
    "\\begin{align*}\n",
    "    \\mathbf{ h}_{1} &= \\phi\\left(\\mathbf{W}_1 \\mathbf{ x}\\right)\\\\\n",
    "    \\mathbf{ h}_{2} &=  \\phi\\left(\\mathbf{W}_2\\mathbf{ h}_{1}\\right)\\\\\n",
    "    \\mathbf{ h}_{3} &= \\phi\\left(\\mathbf{W}_3 \\mathbf{ h}_{2}\\right)\\\\\n",
    "    f&= \\mathbf{ w}_4 ^\\top\\mathbf{ h}_{3}\n",
    "\\end{align*}\n",
    "$$"
   ],
   "id": "107b1b2d-f664-4551-92dc-d57a2fd54998"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/overfitting-low-rank.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/overfitting-low-rank.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "One potential problem is that as the number of nodes in two adjacent\n",
    "layers increases, the number of parameters in the affine transformation\n",
    "between layers, $\\mathbf{W}$, increases. If there are $k_{i-1}$ nodes in\n",
    "one layer, and $k_i$ nodes in the following, then that matrix contains\n",
    "$k_i k_{i-1}$ parameters, when we have layer widths in the 1000s that\n",
    "leads to millions of parameters.\n",
    "\n",
    "One proposed solution is known as *dropout* where only a sub-set of the\n",
    "neural network is trained at each iteration. An alternative solution\n",
    "would be to reparameterize $\\mathbf{W}$ with its *singular value\n",
    "decomposition*. $$\n",
    "  \\mathbf{W}= \\mathbf{U}\\boldsymbol{ \\Lambda}\\mathbf{V}^\\top\n",
    "  $$ or $$\n",
    "  \\mathbf{W}= \\mathbf{U}\\mathbf{V}^\\top\n",
    "  $$ where if $\\mathbf{W}\\in \\Re^{k_1\\times k_2}$ then\n",
    "$\\mathbf{U}\\in \\Re^{k_1\\times q}$ and $\\mathbf{V}\\in \\Re^{k_2\\times q}$,\n",
    "i.e. we have a low rank matrix factorization for the weights."
   ],
   "id": "4b87b08d-0277-4f47-b547-b1c01ec335df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "0867d74c-39a9-49c0-9836-6eb77f02782a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.low_rank_approximation(diagrams='.')"
   ],
   "id": "dd7bb462-6b9f-4335-9ae0-27851ab9d162"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//wisuvt.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Pictorial representation of the low rank form of the matrix\n",
    "$\\mathbf{W}$.</i>\n",
    "\n",
    "In practice there is evidence that deep models seek these low rank\n",
    "solutions where we expect better generalisation. See e.g. Arora et al.\n",
    "(2019);Jacot et al. (2021)."
   ],
   "id": "060a6f26-ba47-450c-9d74-f103f61555ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottleneck Layers in Deep Neural Networks\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "b374c354-07a0-4079-9f94-3091116251ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "4ba87eb3-99ae-4d08-b8fc-436314c46167"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.deep_nn_bottleneck(diagrams='./deepgp')"
   ],
   "id": "55330193-384f-4e76-a20b-196a19d90907"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck2.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Inserting the bottleneck layers introduces a new set of\n",
    "variables.</i>\n",
    "\n",
    "Including the low rank decomposition of $\\mathbf{W}$ in the neural\n",
    "network, we obtain a new mathematical form. Effectively, we are adding\n",
    "additional *latent* layers, $\\mathbf{ z}$, in between each of the\n",
    "existing hidden layers. In a neural network these are sometimes known as\n",
    "*bottleneck* layers. The network can now be written mathematically as $$\n",
    "\\begin{align}\n",
    "  \\mathbf{ z}_{1} &= \\mathbf{V}^\\top_1 \\mathbf{ x}\\\\\n",
    "  \\mathbf{ h}_{1} &= \\phi\\left(\\mathbf{U}_1 \\mathbf{ z}_{1}\\right)\\\\\n",
    "  \\mathbf{ z}_{2} &= \\mathbf{V}^\\top_2 \\mathbf{ h}_{1}\\\\\n",
    "  \\mathbf{ h}_{2} &= \\phi\\left(\\mathbf{U}_2 \\mathbf{ z}_{2}\\right)\\\\\n",
    "  \\mathbf{ z}_{3} &= \\mathbf{V}^\\top_3 \\mathbf{ h}_{2}\\\\\n",
    "  \\mathbf{ h}_{3} &= \\phi\\left(\\mathbf{U}_3 \\mathbf{ z}_{3}\\right)\\\\\n",
    "  \\mathbf{ y}&= \\mathbf{ w}_4^\\top\\mathbf{ h}_{3}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\mathbf{ z}_{1} &= \\mathbf{V}^\\top_1 \\mathbf{ x}\\\\\n",
    "  \\mathbf{ z}_{2} &= \\mathbf{V}^\\top_2 \\phi\\left(\\mathbf{U}_1 \\mathbf{ z}_{1}\\right)\\\\\n",
    "  \\mathbf{ z}_{3} &= \\mathbf{V}^\\top_3 \\phi\\left(\\mathbf{U}_2 \\mathbf{ z}_{2}\\right)\\\\\n",
    "  \\mathbf{ y}&= \\mathbf{ w}_4 ^\\top \\mathbf{ z}_{3}\n",
    "\\end{align}\n",
    "$$"
   ],
   "id": "fae69ad8-9615-4a4b-886b-1092df1b4f7b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade of Gaussian Processes\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/cascade-of-gps.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/cascade-of-gps.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Now if we replace each of these neural networks with a Gaussian process.\n",
    "This is equivalent to taking the limit as the width of each layer goes\n",
    "to infinity, while appropriately scaling down the outputs.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\mathbf{ z}_{1} &= \\mathbf{ f}_1\\left(\\mathbf{ x}\\right)\\\\\n",
    "  \\mathbf{ z}_{2} &= \\mathbf{ f}_2\\left(\\mathbf{ z}_{1}\\right)\\\\\n",
    "  \\mathbf{ z}_{3} &= \\mathbf{ f}_3\\left(\\mathbf{ z}_{2}\\right)\\\\\n",
    "  \\mathbf{ y}&= \\mathbf{ f}_4\\left(\\mathbf{ z}_{3}\\right)\n",
    "\\end{align}\n",
    "$$"
   ],
   "id": "1c6d9b5d-9004-4392-aa92-9dbc6e3a949c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<!-- No slide titles in this context -->"
   ],
   "id": "985116f6-2216-4f8b-b588-902b0cdb2000"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFace\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//deepface_neg.png\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>The DeepFace architecture (Taigman et al., 2014), visualized\n",
    "through colors to represent the functional mappings at each layer. There\n",
    "are 120 million parameters in the model.</i>\n",
    "\n",
    "The DeepFace architecture (Taigman et al., 2014) consists of layers that\n",
    "deal with *translation* invariances, known as convolutional layers.\n",
    "These layers are followed by three locally-connected layers and two\n",
    "fully-connected layers. Color illustrates feature maps produced at each\n",
    "layer. The neural network includes more than 120 million parameters,\n",
    "where more than 95% come from the local and fully connected layers."
   ],
   "id": "76ecc5be-76c1-4e93-bfeb-b491de26c11e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning as Pinball\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//576px-Early_Pinball.jpg\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>Deep learning models are composition of simple functions. We\n",
    "can think of a pinball machine as an analogy. Each layer of pins\n",
    "corresponds to one of the layers of functions in the model. Input data\n",
    "is represented by the location of the ball from left to right when it is\n",
    "dropped in from the top. Output class comes from the position of the\n",
    "ball as it leaves the pins at the bottom.</i>\n",
    "\n",
    "Sometimes deep learning models are described as being like the brain, or\n",
    "too complex to understand, but one analogy I find useful to help the\n",
    "gist of these models is to think of them as being similar to early pin\n",
    "ball machines.\n",
    "\n",
    "In a deep neural network, we input a number (or numbers), whereas in\n",
    "pinball, we input a ball.\n",
    "\n",
    "Think of the location of the ball on the left-right axis as a single\n",
    "number. Our simple pinball machine can only take one number at a time.\n",
    "As the ball falls through the machine, each layer of pins can be thought\n",
    "of as a different layer of ‘neurons’. Each layer acts to move the ball\n",
    "from left to right.\n",
    "\n",
    "In a pinball machine, when the ball gets to the bottom it might fall\n",
    "into a hole defining a score, in a neural network, that is equivalent to\n",
    "the decision: a classification of the input object.\n",
    "\n",
    "An image has more than one number associated with it, so it is like\n",
    "playing pinball in a *hyper-space*.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//pinball001.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>At initialization, the pins, which represent the parameters\n",
    "of the function, aren’t in the right place to bring the balls to the\n",
    "correct decisions.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//pinball002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>After learning the pins are now in the right place to bring\n",
    "the balls to the correct decisions.</i>\n",
    "\n",
    "Learning involves moving all the pins to be in the correct position, so\n",
    "that the ball ends up in the right place when it’s fallen through the\n",
    "machine. But moving all these pins in hyperspace can be difficult.\n",
    "\n",
    "In a hyper-space you have to put a lot of data through the machine for\n",
    "to explore the positions of all the pins. Even when you feed many\n",
    "millions of data points through the machine, there are likely to be\n",
    "regions in the hyper-space where no ball has passed. When future test\n",
    "data passes through the machine in a new route unusual things can\n",
    "happen.\n",
    "\n",
    "*Adversarial examples* exploit this high dimensional space. If you have\n",
    "access to the pinball machine, you can use gradient methods to find a\n",
    "position for the ball in the hyper space where the image looks like one\n",
    "thing, but will be classified as another.\n",
    "\n",
    "Probabilistic methods explore more of the space by considering a range\n",
    "of possible paths for the ball through the machine. This helps to make\n",
    "them more data efficient and gives some robustness to adversarial\n",
    "examples.\n",
    "\n",
    "Mathematically, a deep Gaussian process can be seen as a composite\n",
    "*multivariate* function, $$\n",
    "  \\mathbf{g}(\\mathbf{ x})=\\mathbf{ f}_5(\\mathbf{ f}_4(\\mathbf{ f}_3(\\mathbf{ f}_2(\\mathbf{ f}_1(\\mathbf{ x}))))).\n",
    "  $$ Or if we view it from the probabilistic perspective we can see that\n",
    "a deep Gaussian process is specifying a factorization of the joint\n",
    "density, the standard deep model takes the form of a Markov chain."
   ],
   "id": "f6c02253-69e5-4861-86a8-3e15e93e92bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica'],'size':30})\n",
    "rc(\"text\", usetex=True)"
   ],
   "id": "be9b849c-43f2-4499-9a49-4ce7dec8153f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = plot.horizontal_chain(depth=5)\n",
    "pgm.render().figure.savefig(\"./deepgp/deep-markov.svg\", transparent=True)"
   ],
   "id": "632560ce-0b20-40bf-a1ef-1d587de73fff"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  p(\\mathbf{ y}|\\mathbf{ x})= p(\\mathbf{ y}|\\mathbf{ f}_5)p(\\mathbf{ f}_5|\\mathbf{ f}_4)p(\\mathbf{ f}_4|\\mathbf{ f}_3)p(\\mathbf{ f}_3|\\mathbf{ f}_2)p(\\mathbf{ f}_2|\\mathbf{ f}_1)p(\\mathbf{ f}_1|\\mathbf{ x})\n",
    "  $$\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Probabilistically the deep Gaussian process can be\n",
    "represented as a Markov chain. Indeed they can even be analyzed in this\n",
    "way (Dunlop et al., n.d.).</i>"
   ],
   "id": "a67151e1-93b3-4a84-b483-9aaaba9f395e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica'], 'size':15})\n",
    "rc(\"text\", usetex=True)"
   ],
   "id": "035a8649-4bf7-45c5-805e-385593830cb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = plot.vertical_chain(depth=5)\n",
    "pgm.render().figure.savefig(\"./deepgp/deep-markov-vertical.svg\", transparent=True)"
   ],
   "id": "cb5e2c7d-c040-4d25-837d-10a75dc9bfb0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical.svg\" class=\"\" width=\"7%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>More usually deep probabilistic models are written vertically\n",
    "rather than horizontally as in the Markov chain.</i>"
   ],
   "id": "2c70aff8-5c31-4ac1-83ce-0b36d64cbe5d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Composition?\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/process-composition.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/process-composition.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "If the result of composing many functions together is simply another\n",
    "function, then why do we bother? The key point is that we can change the\n",
    "class of functions we are modeling by composing in this manner. A\n",
    "Gaussian process is specifying a prior over functions, and one with a\n",
    "number of elegant properties. For example, the derivative process (if it\n",
    "exists) of a Gaussian process is also Gaussian distributed. That makes\n",
    "it easy to assimilate, for example, derivative observations. But that\n",
    "also might raise some alarm bells. That implies that the *marginal\n",
    "derivative distribution* is also Gaussian distributed. If that’s the\n",
    "case, then it means that functions which occasionally exhibit very large\n",
    "derivatives are hard to model with a Gaussian process. For example, a\n",
    "function with jumps in.\n",
    "\n",
    "A one off discontinuity is easy to model with a Gaussian process, or\n",
    "even multiple discontinuities. They can be introduced in the mean\n",
    "function, or independence can be forced between two covariance functions\n",
    "that apply in different areas of the input space. But in these cases we\n",
    "will need to specify the number of discontinuities and where they occur.\n",
    "In otherwords we need to *parameterise* the discontinuities. If we do\n",
    "not know the number of discontinuities and don’t wish to specify where\n",
    "they occur, i.e. if we want a non-parametric representation of\n",
    "discontinuities, then the standard Gaussian process doesn’t help."
   ],
   "id": "6e691577-4451-4a39-b54e-80afdc8bd5c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Process Composition\n",
    "\n",
    "The deep Gaussian process leads to *non-Gaussian* models, and\n",
    "non-Gaussian characteristics in the covariance function. In effect, what\n",
    "we are proposing is that we change the properties of the functions we\n",
    "are considering by *composing stochastic processes*. This is an approach\n",
    "to creating new stochastic processes from well known processes."
   ],
   "id": "6517ee70-568e-46ef-a4e6-3aff26712805"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft"
   ],
   "id": "c7a9b1f2-afe9-45d8-98c6-511292a9718a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = plot.vertical_chain(depth=5, shape=[2, 7])\n",
    "pgm.add_node(daft.Node('y_2', r'$\\mathbf{y}_2$', 1.5, 3.5, observed=True))\n",
    "pgm.add_edge('f_2', 'y_2')\n",
    "pgm.render().figure.savefig(\"./deepgp/deep-markov-vertical-side.svg\", transparent=True)"
   ],
   "id": "9d106b93-d080-407a-9cbc-25d3ff52c65c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we are not constrained to the formalism of the chain. For\n",
    "example, we can easily add single nodes emerging from some point in the\n",
    "depth of the chain. This allows us to combine the benefits of the\n",
    "graphical modelling formalism, but with a powerful framework for\n",
    "relating one set of variables to another, that of Gaussian processes\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical-side.svg\" class=\"\" width=\"15%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>More generally we aren’t constrained by the Markov chain. We\n",
    "can design structures that respect our belief about the underlying\n",
    "conditional dependencies. Here we are adding a side note from the\n",
    "chain.</i>"
   ],
   "id": "934b017a-e55c-4985-87f7-2ff401502453"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty for Probabilistic Approaches\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/non-linear-difficulty.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/non-linear-difficulty.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "cab96961-9ba0-46eb-9356-a1f42b25f909"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "09dfa5a4-8ddc-4e12-8177-adaac01a357f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_3(diagrams='./dimred/')"
   ],
   "id": "66271a64-c816-483b-835d-e570aa70c703"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge for composition of probabilistic models is that you need\n",
    "to propagate a probability densities through non linear mappings. This\n",
    "allows you to create broader classes of probability density.\n",
    "Unfortunately it renders the resulting densities *intractable*.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-3d-plot.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A two dimensional grid mapped into three dimensions to form a\n",
    "two dimensional manifold.</i>"
   ],
   "id": "49ff6ee2-b977-4ce0-9c3d-997e7702ddc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_2(diagrams='./dimred/')"
   ],
   "id": "b8c6c714-cc2e-4cc1-9030-33ed71cf0f11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-2d-plot.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A one dimensional line mapped into two dimensions by two\n",
    "separate independent functions. Each point can be mapped exactly through\n",
    "the mappings.</i>"
   ],
   "id": "2b5ec3cb-db2d-4ec9-927c-c167626c19b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_1(diagrams='./dimred')"
   ],
   "id": "0e45de2f-4a10-4b02-8a7c-48b15b2790a7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//dimred/gaussian-through-nonlinear.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian density over the input of a non linear function\n",
    "leads to a very non Gaussian output. Here the output is multimodal.</i>"
   ],
   "id": "9e5809d9-807c-40c3-a47e-6f73fb934754"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Variational Approach Fails\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gplvm/includes/variational-bayes-gplvm-long.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gplvm/includes/variational-bayes-gplvm-long.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "-   Standard variational bound has the form: $$\n",
    "    \\mathcal{L}= \\left\\langle\\log p(\\mathbf{ y}|\\mathbf{Z})\\right\\rangle_{q(\\mathbf{Z})} + \\text{KL}\\left( q(\\mathbf{Z})\\,\\|\\,p(\\mathbf{Z}) \\right)\n",
    "    $$\n",
    "\n",
    "The standard variational approach would require the expectation of\n",
    "$\\log p(\\mathbf{ y}|\\mathbf{Z})$ under $q(\\mathbf{Z})$. $$\n",
    "  \\begin{align}\n",
    "  \\log p(\\mathbf{ y}|\\mathbf{Z}) = & -\\frac{1}{2}\\mathbf{ y}^\\top\\left(\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}+\\sigma^2\\mathbf{I}\\right)^{-1}\\mathbf{ y}\\\\ & -\\frac{1}{2}\\log \\det{\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}+\\sigma^2 \\mathbf{I}} -\\frac{n}{2}\\log 2\\pi\n",
    "  \\end{align}\n",
    "  $$ But this is extremely difficult to compute because\n",
    "$\\mathbf{K}_{\\mathbf{ f}, \\mathbf{ f}}$ is dependent on $\\mathbf{Z}$ and\n",
    "it appears in the inverse."
   ],
   "id": "aec4a021-a001-4f74-9a6e-d88e1be0641b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayesian GP-LVM\n",
    "\n",
    "The alternative approach is to consider the collapsed variational bound\n",
    "(used for low rank (sparse is a misnomer) Gaussian process\n",
    "approximations. $$\n",
    "    p(\\mathbf{ y})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle,\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$ $$\n",
    "    p(\\mathbf{ y}|\\mathbf{Z})\\geq \\prod_{i=1}^nc_i \\int \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right)p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$ $$\n",
    "      \\int p(\\mathbf{ y}|\\mathbf{Z})p(\\mathbf{Z}) \\text{d}\\mathbf{Z}\\geq \\int \\prod_{i=1}^nc_i \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right) p(\\mathbf{Z})\\text{d}\\mathbf{Z}p(\\mathbf{ u}) \\text{d}\\mathbf{ u}\n",
    "  $$\n",
    "\n",
    "To integrate across $\\mathbf{Z}$ we apply the lower bound to the inner\n",
    "integral. $$\n",
    "    \\begin{align}\n",
    "    \\int \\prod_{i=1}^nc_i \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right) p(\\mathbf{Z})\\text{d}\\mathbf{Z}\\geq & \\left\\langle\\sum_{i=1}^n\\log  c_i\\right\\rangle_{q(\\mathbf{Z})}\\\\ & +\\left\\langle\\log\\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u}, \\mathbf{Z})},\\sigma^2\\mathbf{I}\\right)\\right\\rangle_{q(\\mathbf{Z})}\\\\& + \\text{KL}\\left( q(\\mathbf{Z})\\,\\|\\,p(\\mathbf{Z}) \\right)    \n",
    "    \\end{align}\n",
    "  $$ \\* Which is analytically tractable for Gaussian $q(\\mathbf{Z})$ and\n",
    "some covariance functions.\n",
    "\n",
    "-   Need expectations under $q(\\mathbf{Z})$ of: $$\n",
    "    \\log c_i = \\frac{1}{2\\sigma^2} \\left[k_{i, i} - \\mathbf{ k}_{i, \\mathbf{ u}}^\\top \\mathbf{K}_{\\mathbf{ u}, \\mathbf{ u}}^{-1} \\mathbf{ k}_{i, \\mathbf{ u}}\\right]\n",
    "    $$ and $$\n",
    "    \\log \\mathcal{N}\\left(\\mathbf{ y}|\\left\\langle\\mathbf{ f}\\right\\rangle_{p(\\mathbf{ f}|\\mathbf{ u},\\mathbf{Y})},\\sigma^2\\mathbf{I}\\right) = -\\frac{1}{2}\\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^2}\\left(y_i - \\mathbf{K}_{\\mathbf{ f}, \\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}^{-1}\\mathbf{ u}\\right)^2\n",
    "    $$\n",
    "\n",
    "-   This requires the expectations $$\n",
    "    \\left\\langle\\mathbf{K}_{\\mathbf{ f},\\mathbf{ u}}\\right\\rangle_{q(\\mathbf{Z})}\n",
    "    $$ and $$\n",
    "    \\left\\langle\\mathbf{K}_{\\mathbf{ f},\\mathbf{ u}}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ u}}^{-1}\\mathbf{K}_{\\mathbf{ u},\\mathbf{ f}}\\right\\rangle_{q(\\mathbf{Z})}\n",
    "    $$ which can be computed analytically for some covariance functions\n",
    "    (Damianou et al., 2016) or through sampling (Damianou, 2015;\n",
    "    Salimbeni and Deisenroth, 2017).\n",
    "\n",
    "Variational approximations aren’t the only approach to approximate\n",
    "inference. The original work on deep Gaussian processes made use of MAP\n",
    "approximations (Lawrence and Moore, 2007), which couldn’t propagate the\n",
    "uncertainty through the model at the data points but sustain uncertainty\n",
    "elsewhere. Since the variational approximation was proposed researchers\n",
    "have also considered sampling approaches (Havasi et al., 2018) and\n",
    "expectation propagation (Bui et al., 2016).\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/neural-network-uncertainty.png\" style=\"width:90%\">\n",
    "\n",
    "Figure: <i>Even the latest work on Bayesian neural networks has severe\n",
    "problems handling uncertainty. In this example, (Izmailov et al., 2019),\n",
    "methods even fail to interpolate through the data correctly or provide\n",
    "well calibrated error bars in regions where data is observed.</i>\n",
    "\n",
    "The argument in the deep learning revolution is that deep architectures\n",
    "allow us to develop an abstraction of the feature set through model\n",
    "composition. Composing Gaussian processes is analytically intractable.\n",
    "To form deep Gaussian processes we use a variational approach to stack\n",
    "the models."
   ],
   "id": "295fadb8-e0b5-4bff-8488-59637b307783"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked PCA\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-pca.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-pca.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "40710882-d148-4143-a65b-3050d3562bef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "cb0f5466-7841-435c-8288-8334608c6a60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.stack_gp_sample(kernel=GPy.kern.Linear,\n",
    "                     diagrams=\"./deepgp\")"
   ],
   "id": "05706642-4d2b-40b2-972b-78245db37948"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "38bb37f6-0beb-41ec-aec8-5c9ba299bef2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "8ab1ade9-faf1-4182-a1ee-2d004aea3524"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('stack-gp-sample-Linear-{sample:0>1}.svg', \n",
    "                            directory='./deepgp', sample=(0,4))"
   ],
   "id": "fc397d79-2d30-45f6-885e-71b6ca220359"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-4.svg\" class=\"\" width=\"20%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Composition of linear functions just leads to a new linear\n",
    "function. Here you see the result of multiple affine transformations\n",
    "applied to a square in two dimensions.</i>\n",
    "\n",
    "Stacking a series of linear functions simply leads to a new linear\n",
    "function. The use of multiple linear function merely changes the\n",
    "covariance of the resulting Gaussian. If $$\n",
    "\\mathbf{Z}\\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}\\right)\n",
    "$$ and the $i$th hidden layer is a multivariate linear transformation\n",
    "defined by $\\mathbf{W}_i$, $$\n",
    "\\mathbf{Y}= \\mathbf{Z}\\mathbf{W}_1 \\mathbf{W}_2 \\dots \\mathbf{W}_\\ell\n",
    "$$ then the rules of multivariate Gaussians tell us that $$\n",
    "\\mathbf{Y}\\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{W}_\\ell\\dots \\mathbf{W}_1 \\mathbf{W}^\\top_1 \\dots \\mathbf{W}^\\top_\\ell\\right).\n",
    "$$ So the model can be replaced by one where we set\n",
    "$\\mathbf{V}= \\mathbf{W}_\\ell\\dots \\mathbf{W}_2 \\mathbf{W}_1$. So is such\n",
    "a model trivial? The answer is that it depends. There are two cases in\n",
    "which such a model remaisn interesting. Firstly, if we make intermediate\n",
    "observations stemming from the chain. So, for example, if we decide\n",
    "that, $$\n",
    "\\mathbf{Z}_i = \\mathbf{W}_i \\mathbf{Z}_{i-1}\n",
    "$$ and set\n",
    "$\\mathbf{Z}_{0} = \\mathbf{X}\\sim \\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}\\right)$,\n",
    "then the matrices $\\mathbf{W}$ inter-relate a series of jointly Gaussian\n",
    "observations in an interesting way, stacking the full data matrix to\n",
    "give $$\n",
    "\\mathbf{Z}= \\begin{bmatrix}\n",
    "\\mathbf{Z}_0 \\\\\n",
    "\\mathbf{Z}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{Z}_\\ell\n",
    "\\end{bmatrix}\n",
    "$$ we can obtain\n",
    "$$\\mathbf{Z}\\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\n",
    "\\mathbf{I}& \\mathbf{W}^\\top_1 & \\mathbf{W}_1^\\top\\mathbf{W}_2^\\top & \\dots & \\mathbf{V}^\\top \\\\\n",
    "\\mathbf{W}_1 & \\mathbf{W}_1 \\mathbf{W}_1^\\top & \\mathbf{W}_1 \\mathbf{W}_1^\\top \\mathbf{W}_2^\\top & \\dots & \\mathbf{W}_1 \\mathbf{V}^\\top \\\\\n",
    "\\mathbf{W}_2 \\mathbf{W}_1 & \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{W}_1^\\top & \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{W}_1^\\top \\mathbf{W}_2^\\top & \\dots & \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{V}^\\top \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{V}& \\mathbf{V}\\mathbf{W}_1^\\top  & \\mathbf{V}\\mathbf{W}_1^\\top \\mathbf{W}_2^\\top& \\dots & \\mathbf{V}\\mathbf{V}^\\top\n",
    "\\end{bmatrix}\\right)$$ which is a highly structured Gaussian covariance\n",
    "with hierarchical dependencies between the variables $\\mathbf{Z}_i$."
   ],
   "id": "f8a9bac6-0799-4d0b-8e2b-0ab5520a3d80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked GP\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "01c9300a-0f3f-42de-9130-b60d303b5161"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.stack_gp_sample(kernel=GPy.kern.RBF,\n",
    "                     diagrams=\"./deepgp\")"
   ],
   "id": "69a62c28-d5df-4e65-a81c-443644269cd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "62bb3121-f985-42e3-9202-e9d3b6104183"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('stack-gp-sample-RBF-{sample:0>1}.svg', \n",
    "                            directory='./deepgp', sample=(0,4))"
   ],
   "id": "9794903e-0d9e-4764-a5d5-5ad4ab01f127"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-4.svg\" class=\"\" width=\"20%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Stacking Gaussian process models leads to non linear mappings\n",
    "at each stage. Here we are mapping from two dimensions to two dimensions\n",
    "in each layer.</i>\n",
    "\n",
    "Note that once the box has folded over on itself, it cannot be unfolded.\n",
    "So a feature that is generated near the top of the model cannot be\n",
    "removed further down the model.\n",
    "\n",
    "This folding over effect happens in low dimensions. In higher dimensions\n",
    "it is less common.\n",
    "\n",
    "Observation of this effect at a talk in Cambridge was one of the things\n",
    "that caused David Duvenaud (and collaborators) to consider the behavior\n",
    "of deeper Gaussian process models (Duvenaud et al., 2014).\n",
    "\n",
    "Such folding over in the latent spaces necessarily forces the density to\n",
    "be non-Gaussian. Indeed, since folding-over is avoided as we increase\n",
    "the dimensionality of the latent spaces, such processes become more\n",
    "Gaussian. If we take the limit of the latent space dimensionality as it\n",
    "tends to infinity, the entire deep Gaussian process returns to a\n",
    "standard Gaussian process, with a covariance function given as a deep\n",
    "kernel (such as those described by Cho and Saul (2009)).\n",
    "\n",
    "Further analysis of these deep networks has been conducted by Dunlop et\n",
    "al. (n.d.), who use analysis of the deep network’s stationary density\n",
    "(treating it as a Markov chain across layers), to explore the nature of\n",
    "the implied process prior for a deep GP.\n",
    "\n",
    "Both of these works, however, make constraining assumptions on the form\n",
    "of the Gaussian process prior at each layer (e.g. same covariance at\n",
    "each layer). In practice, the form of this covariance can be learnt and\n",
    "the densities described by the deep GP are more general than those\n",
    "mentioned in either of these papers."
   ],
   "id": "9caf3e63-ffe7-4ceb-b4e8-8f704c90139d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked GPs (video by David Duvenaud)\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-pathologies.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-pathologies.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "3ca563b9-d69c-412b-88f9-9619abc6de4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('XhIvygQYFFQ')"
   ],
   "id": "13c16792-7a26-4397-8934-26a15cc68303"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Visualization of mapping of a two dimensional space through a\n",
    "deep Gaussian process.</i>\n",
    "\n",
    "David Duvenaud also created a YouTube video to help visualize what\n",
    "happens as you drop through the layers of a deep GP."
   ],
   "id": "bb1b1066-145e-4e60-b702-ac7c79c818af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ],
   "id": "247ff8c1-de98-48bb-afec-a14df3081f52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPy: A Gaussian Process Framework in Python\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Gaussian processes are a flexible tool for non-parametric analysis with\n",
    "uncertainty. The GPy software was started in Sheffield to provide a easy\n",
    "to use interface to GPs. One which allowed the user to focus on the\n",
    "modelling rather than the mathematics.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/gpy.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>GPy is a BSD licensed software code base for implementing\n",
    "Gaussian process models in Python. It is designed for teaching and\n",
    "modelling. We welcome contributions which can be made through the GitHub\n",
    "repository <https://github.com/SheffieldML/GPy></i>\n",
    "\n",
    "GPy is a BSD licensed software code base for implementing Gaussian\n",
    "process models in python. This allows GPs to be combined with a wide\n",
    "variety of software libraries.\n",
    "\n",
    "The software itself is available on\n",
    "[GitHub](https://github.com/SheffieldML/GPy) and the team welcomes\n",
    "contributions.\n",
    "\n",
    "The aim for GPy is to be a probabilistic-style programming language,\n",
    "i.e., you specify the model rather than the algorithm. As well as a\n",
    "large range of covariance functions the software allows for non-Gaussian\n",
    "likelihoods, multivariate outputs, dimensionality reduction and\n",
    "approximations for larger data sets.\n",
    "\n",
    "The documentation for GPy can be found\n",
    "[here](https://gpy.readthedocs.io/en/latest/).\n",
    "\n",
    "This notebook depends on PyDeepGP. This library can be installed via\n",
    "pip."
   ],
   "id": "5572f237-a4b5-449c-8ce3-cd2d16b28b70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/SheffieldML/PyDeepGP.git"
   ],
   "id": "08c84cd2-745e-46ed-86f4-b7ebceb41806"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlai"
   ],
   "id": "cf430bfc-56a6-40d6-8f73-507a354c1abf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Late bind setup methods to DeepGP object\n",
    "from mlai.deepgp_tutorial import initialize\n",
    "from mlai.deepgp_tutorial import staged_optimize\n",
    "from mlai.deepgp_tutorial import posterior_sample\n",
    "from mlai.deepgp_tutorial import visualize\n",
    "from mlai.deepgp_tutorial import visualize_pinball\n",
    "\n",
    "import deepgp\n",
    "deepgp.DeepGP.initialize=initialize\n",
    "deepgp.DeepGP.staged_optimize=staged_optimize\n",
    "deepgp.DeepGP.posterior_sample=posterior_sample\n",
    "deepgp.DeepGP.visualize=visualize\n",
    "deepgp.DeepGP.visualize_pinball=visualize_pinball"
   ],
   "id": "cc2e17f2-da9c-4496-af1b-31e1fe984801"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep GP Fit\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/olympic-marathon-deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/olympic-marathon-deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Let’s see if a deep Gaussian process can help here. We will construct a\n",
    "deep Gaussian process with one hidden layer (i.e. one Gaussian process\n",
    "feeding into another).\n",
    "\n",
    "Build a Deep GP with an additional hidden layer (one dimensional) to fit\n",
    "the model."
   ],
   "id": "612f222e-5e67-4e33-8384-c81c5136cff7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import deepgp"
   ],
   "id": "52b702d1-bceb-4671-bb10-85f4f3b363df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 1\n",
    "m = deepgp.DeepGP([y.shape[1],hidden,x.shape[1]],Y=yhat, X=x, inits=['PCA','PCA'], \n",
    "                  kernels=[GPy.kern.RBF(hidden,ARD=True),\n",
    "                           GPy.kern.RBF(x.shape[1],ARD=True)], # the kernels for each layer\n",
    "                  num_inducing=50, back_constraint=False)"
   ],
   "id": "eb1756e9-d484-446a-86fa-83c1f211cfaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the initalization\n",
    "m.initialize()"
   ],
   "id": "308e3c16-ae6e-4c51-b60a-d712cb6070da"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now optimize the model."
   ],
   "id": "f277865e-66ec-4475-8712-b7b26fa2ee84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in m.layers:\n",
    "    layer.likelihood.variance.constrain_positive(warning=False)\n",
    "m.optimize(messages=True,max_iters=10000)"
   ],
   "id": "125f282c-04a4-4d13-89c4-5214cbea8c1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.staged_optimize(messages=(True,True,True))"
   ],
   "id": "505ccda7-a49b-4d04-820d-9a55367868c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "099863e6-df04-4ec8-9993-7512561e21a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, scale=scale, offset=offset, ax=ax, xlabel='year', ylabel='pace min/km', \n",
    "          fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/olympic-marathon-deep-gp.svg', \n",
    "                transparent=True, frameon=True)"
   ],
   "id": "5c89d88e-6090-41ce-bd11-ea1dc8e2e07f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic Marathon Data Deep GP\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp.svg\" class=\"\" width=\"100%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Deep GP fit to the Olympic marathon data. Error bars now\n",
    "change as the prediction evolves.</i>"
   ],
   "id": "2efc5dc0-679c-4c66-a6db-80e11ce13836"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_sample(m, scale=scale, offset=offset, samps=10, ax=ax, \n",
    "                  xlabel='year', ylabel='pace min/km', portion = 0.225)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/olympic-marathon-deep-gp-samples.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "6688bd95-cec4-469a-98e6-012e6100f34c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic Marathon Data Deep GP\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-samples.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Point samples run through the deep Gaussian process show the\n",
    "distribution of output locations.</i>"
   ],
   "id": "97c02056-c155-47e9-8e10-1acb9d537c8c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitted GP for each layer\n",
    "\n",
    "Now we explore the GPs the model has used to fit each layer. First of\n",
    "all, we look at the hidden layer."
   ],
   "id": "8907c632-7aa6-4b03-9424-7c1ba4944251"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize(scale=scale, offset=offset, xlabel='year',\n",
    "            ylabel='pace min/km',xlim=xlim, ylim=ylim,\n",
    "            dataset='olympic-marathon',\n",
    "            diagrams='./deepgp')"
   ],
   "id": "52f72288-c21a-4f3b-b06f-5077b64828c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "3c116d57-4e75-4e3d-bc50-6a4af01fa447"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('olympic-marathon-deep-gp-layer-{sample:0>1}.svg', \n",
    "                            './deepgp', sample=(0,1))"
   ],
   "id": "1394d5c3-c860-4da1-b815-22985f51a7eb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-0.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The mapping from input to the latent layer is broadly, with\n",
    "some flattening as time goes on. Variance is high across the input\n",
    "range.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-1.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The mapping from the latent layer to the output layer.</i>"
   ],
   "id": "c909b4d7-1e4f-44e3-be53-9a4730c091f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "m.visualize_pinball(ax=ax, scale=scale, offset=offset, points=30, portion=0.1,\n",
    "                    xlabel='year', ylabel='pace km/min', vertical=True)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/olympic-marathon-deep-gp-pinball.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "80a7bb50-7bd0-4e22-9eae-439916352177"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic Marathon Pinball Plot\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-pinball.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A pinball plot shows the movement of the ‘ball’ as it passes\n",
    "through each layer of the Gaussian processes. Mean directions of\n",
    "movement are shown by lines. Shading gives one standard deviation of\n",
    "movement position. At each layer, the uncertainty is reset. The overal\n",
    "uncertainty is the cumulative uncertainty from all the layers. There is\n",
    "some grouping of later points towards the right in the first layer,\n",
    "which also injects a large amount of uncertainty. Due to flattening of\n",
    "the curve in the second layer towards the right the uncertainty is\n",
    "reduced in final output.</i>\n",
    "\n",
    "The pinball plot shows the flow of any input ball through the deep\n",
    "Gaussian process. In a pinball plot a series of vertical parallel lines\n",
    "would indicate a purely linear function. For the olypmic marathon data\n",
    "we can see the first layer begins to shift from input towards the right.\n",
    "Note it also does so with some uncertainty (indicated by the shaded\n",
    "backgrounds). The second layer has less uncertainty, but bunches the\n",
    "inputs more strongly to the right. This input layer of uncertainty,\n",
    "followed by a layer that pushes inputs to the right is what gives the\n",
    "heteroschedastic noise."
   ],
   "id": "494cbc96-bc66-476f-b824-fb28357e4b91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [y.shape[1], 1,x.shape[1]]\n",
    "inits = ['PCA']*(len(layers)-1)\n",
    "kernels = []\n",
    "for i in layers[1:]:\n",
    "    kernels += [GPy.kern.RBF(i)]\n",
    "m = deepgp.DeepGP(layers,Y=yhat, X=x, \n",
    "                  inits=inits, \n",
    "                  kernels=kernels, # the kernels for each layer\n",
    "                  num_inducing=20, back_constraint=False)"
   ],
   "id": "b4b71278-07f1-4500-81b6-0931c2609057"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.initialize()\n",
    "m.staged_optimize()"
   ],
   "id": "65335ee0-eebc-4a9a-a60a-f4560b75117d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, scale=scale, offset=offset, ax=ax, fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(filename=\"./deepgp/della-gatta-gene-deep-gp.svg\", \n",
    "            transparent=True, frameon=True)"
   ],
   "id": "3518b53a-32b6-45c5-989d-a1e5e951a5eb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Della Gatta Gene Data Deep GP\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/della-gatta-deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/della-gatta-deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Deep Gaussian process fit to the Della Gatta gene expression\n",
    "data.</i>"
   ],
   "id": "fb52cd83-39f3-447f-8ca0-cfc364566f87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_sample(m, scale=scale, offset=offset, samps=10, ax=ax, portion = 0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/della-gatta-gene-deep-gp-samples.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "5b93da60-2a88-4688-821f-ee11896fca72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Della Gatta Gene Data Deep GP\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-samples.svg\" class=\"\" width=\"\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Deep Gaussian process samples fitted to the Della Gatta gene\n",
    "expression data.</i>"
   ],
   "id": "7950c772-f41b-41c7-afbb-b2dc07ec817e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize(offset=offset, scale=scale, xlim=xlim, ylim=ylim,\n",
    "            dataset=\"della-gatta-gene\",\n",
    "            diagrams=\"./deepgp\")"
   ],
   "id": "ff511ca8-0151-43a8-8a0a-104f669226f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Della Gatta Gene Data Latent 1\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-0.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process mapping from input to latent layer for the\n",
    "della Gatta gene expression data.</i>"
   ],
   "id": "8472a1f4-da8d-4f2f-85a4-fd953635ea72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Della Gatta Gene Data Latent 2\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-1.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process mapping from latent to output layer for the\n",
    "della Gatta gene expression data.</i>"
   ],
   "id": "41de03fc-0209-4e13-835e-9d08c74a9f67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "m.visualize_pinball(offset=offset, ax=ax, scale=scale, xlim=xlim, ylim=ylim, portion=0.1, points=50)\n",
    "mlai.write_figure(figure=fig, filename=\"./deepgp/della-gatta-gene-deep-gp-pinball.svg\", \n",
    "                  transparent=True, frameon=True, ax=ax)"
   ],
   "id": "b5188d4c-b352-4a9e-ab73-f5aeb803c7d1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP53 Gene Pinball Plot\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-pinball.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A pinball plot shows the movement of the ‘ball’ as it passes\n",
    "through each layer of the Gaussian processes. Mean directions of\n",
    "movement are shown by lines. Shading gives one standard deviation of\n",
    "movement position. At each layer, the uncertainty is reset. The overal\n",
    "uncertainty is the cumulative uncertainty from all the layers. Pinball\n",
    "plot of the della Gatta gene expression data.</i>"
   ],
   "id": "e97f581a-840d-43c6-aa49-16c2de42a983"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Function\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/step-function-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/step-function-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Next we consider a simple step function data set."
   ],
   "id": "40cc5410-1e08-4a29-b555-09ac97ce33f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_low=25\n",
    "num_high=25\n",
    "gap = -.1\n",
    "noise=0.0001\n",
    "x = np.vstack((np.linspace(-1, -gap/2.0, num_low)[:, np.newaxis],\n",
    "              np.linspace(gap/2.0, 1, num_high)[:, np.newaxis]))\n",
    "y = np.vstack((np.zeros((num_low, 1)), np.ones((num_high,1))))\n",
    "scale = np.sqrt(y.var())\n",
    "offset = y.mean()\n",
    "yhat = (y-offset)/scale"
   ],
   "id": "384947b8-21f8-4caa-86d7-a0c3baf50cb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "_ = ax.set_xlabel('$x$', fontsize=20)\n",
    "_ = ax.set_ylabel('$y$', fontsize=20)\n",
    "xlim = (-2, 2)\n",
    "ylim = (-0.6, 1.6)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='./datasets/step-function.svg', \n",
    "            transparent=True, frameon=True)"
   ],
   "id": "d8b9cc7f-cf14-4162-af72-24ded15ba00c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Function Data\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//datasets/step-function.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Simulation study of step function data artificially\n",
    "generated. Here there is a small overlap between the two lines.</i>"
   ],
   "id": "7cff0dec-589a-4b52-9014-e8e3fc8fb6d9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Function Data GP\n",
    "\n",
    "We can fit a Gaussian process to the step function data using `GPy` as\n",
    "follows."
   ],
   "id": "2a95a760-57b9-4bee-ba4b-c9241c3787fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ],
   "id": "ee47bceb-cbfc-41f5-9a56-86d92d7ebc90"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `GPy.models.GPRegression()` gives us a standard GP regression\n",
    "model with exponentiated quadratic covariance function.\n",
    "\n",
    "The model is optimized using `m_full.optimize()` which calls an L-BGFS\n",
    "gradient based solver in python."
   ],
   "id": "92f07e7b-0cfc-469f-85e6-46803128c574"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "\n",
    "mlai.write_figure(figure=fig,filename='./gp/step-function-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ],
   "id": "735a1421-3175-4a6e-a314-731296572c12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/step-function-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the step function data. Note the\n",
    "large error bars and the over-smoothing of the discontinuity. Error bars\n",
    "are shown at two standard deviations.</i>\n",
    "\n",
    "The resulting fit to the step function data shows some challenges. In\n",
    "particular, the over smoothing at the discontinuity. If we know how many\n",
    "discontinuities there are, we can parameterize them in the step\n",
    "function. But by doing this, we form a semi-parametric model. The\n",
    "parameters indicate how many discontinuities are, and where they are.\n",
    "They can be optimized as part of the model fit. But if new, unforeseen,\n",
    "discontinuities arise when the model is being deployed in practice,\n",
    "these won’t be accounted for in the predictions."
   ],
   "id": "1177b1ef-36ee-4682-9821-c4637ff4676c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Function Data Deep GP\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/step-function-deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/step-function-deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "First we initialize a deep Gaussian process with three latent layers\n",
    "(four layers total). Within each layer we create a GP with an\n",
    "exponentiated quadratic covariance (`GPy.kern.RBF`).\n",
    "\n",
    "At each layer we use 20 inducing points for the variational\n",
    "approximation."
   ],
   "id": "90d3d6dd-f5d2-4fa0-85c2-25bd109dccb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [y.shape[1], 1, 1, 1,x.shape[1]]\n",
    "inits = ['PCA']*(len(layers)-1)\n",
    "kernels = []\n",
    "for i in layers[1:]:\n",
    "    kernels += [GPy.kern.RBF(i)]\n",
    "    \n",
    "m = deepgp.DeepGP(layers,Y=yhat, X=x, \n",
    "                  inits=inits, \n",
    "                  kernels=kernels, # the kernels for each layer\n",
    "                  num_inducing=20, back_constraint=False)"
   ],
   "id": "63174fd9-f67f-456d-bfd5-b804cce22dc7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is constructed we initialize the parameters, and perform\n",
    "the staged optimization which starts by optimizing variational\n",
    "parameters with a low noise and proceeds to optimize the whole model."
   ],
   "id": "70b29aa7-e3eb-416b-828f-7f52fd6d661f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.initialize()\n",
    "m.staged_optimize()"
   ],
   "id": "d25cebaa-e7dc-4f25-94f6-332af0939de6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the output of the deep Gaussian process fitted to the step data\n",
    "as follows."
   ],
   "id": "d567701c-9800-40b2-872c-cf8aee82c300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, scale=scale, offset=offset, ax=ax, fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(filename='./deepgp/step-function-deep-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ],
   "id": "6de5ea6a-7d92-45d2-80b3-72d3f68e8dba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep Gaussian process does a much better job of fitting the data. It\n",
    "handles the discontinuity easily, and error bars drop to smaller values\n",
    "in the regions of data.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Deep Gaussian process fit to the step function data.</i>"
   ],
   "id": "3ca5bfd7-1896-4010-a222-17e095ec3397"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Function Data Deep GP\n",
    "\n",
    "The samples of the model can be plotted with the helper function from\n",
    "`mlai.plot`, `model_sample`"
   ],
   "id": "d5e17156-3a81-421e-84ca-ea12ad38353c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "67a5a7ca-3844-4fae-8895-060c6eb9e491"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "plot.model_sample(m, scale=scale, offset=offset, samps=10, ax=ax, portion = 0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/step-function-deep-gp-samples.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "86c92696-a79c-49b2-ba04-0603ffc3c3e0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples from the model show that the error bars, which are\n",
    "informative for Gaussian outputs, are less informative for this model.\n",
    "They make clear that the data points lie, in output mainly at 0 or 1, or\n",
    "occasionally in between.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-samples.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples from the deep Gaussian process model for the step\n",
    "function fit.</i>\n",
    "\n",
    "The visualize code allows us to inspect the intermediate layers in the\n",
    "deep GP model to understand how it has reconstructed the step function."
   ],
   "id": "a6006e3c-d1f0-44d2-b081-76259e2da766"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize(offset=offset, scale=scale, xlim=xlim, ylim=ylim,\n",
    "            dataset='step-function',\n",
    "            diagrams='./deepgp')"
   ],
   "id": "988f5d65-32c5-4065-9abf-07e3b50aab73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-0.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-1.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-2.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-3.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>From top to bottom, the Gaussian process mapping function\n",
    "that makes up each layer of the resulting deep Gaussian process.</i>\n",
    "\n",
    "A pinball plot can be created for the resulting model to understand how\n",
    "the input is being translated to the output across the different layers."
   ],
   "id": "d95a1436-5bb4-42fc-b4bf-be22524c61a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "6da012ac-eba3-4efc-817f-7801ab3f118b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "m.visualize_pinball(offset=offset, ax=ax, scale=scale, xlim=xlim, ylim=ylim, portion=0.1, points=50)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/step-function-deep-gp-pinball.svg', \n",
    "                  transparent=True, frameon=True, ax=ax)"
   ],
   "id": "aa4bbc78-9beb-41c6-9e9b-038511b3394d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-pinball.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Pinball plot of the deep GP fitted to the step function data.\n",
    "Each layer of the model pushes the ‘ball’ towards the left or right,\n",
    "saturating at 1 and 0. This causes the final density to be be peaked at\n",
    "0 and 1. Transitions occur driven by the uncertainty of the mapping in\n",
    "each layer.</i>"
   ],
   "id": "12321019-0740-4bb6-8b87-5f717010ad76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ],
   "id": "f8f6cff8-4e12-4a1a-b961-fff87e18ba45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.mcycle()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "scale=np.sqrt(y.var())\n",
    "offset=y.mean()\n",
    "yhat = (y - offset)/scale"
   ],
   "id": "757d09fb-ebf0-4236-8b28-ded6524b8caa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import mlai.plot as plot"
   ],
   "id": "5e999cdb-b332-488a-8067-3e86de9bfb13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "_ = ax.set_xlabel('time', fontsize=20)\n",
    "_ = ax.set_ylabel('acceleration', fontsize=20)\n",
    "xlim = (-20, 80)\n",
    "ylim = (-175, 125)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(filename='motorcycle-helmet.svg', directory='./datasets/',\n",
    "            transparent=True, frameon=True)"
   ],
   "id": "cbc33f96-0a30-44e4-8e9f-f177a9a7a949"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Helmet Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/motorcycle-helmet-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/motorcycle-helmet-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//datasets/motorcycle-helmet.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Motorcycle helmet data. The data consists of acceleration\n",
    "readings on a motorcycle helmet undergoing a collision. The data\n",
    "exhibits heteroschedastic (time varying) noise levles and\n",
    "non-stationarity.</i>"
   ],
   "id": "658195a3-134b-41f1-84b9-fa482e0f98a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ],
   "id": "f22fdcc1-1925-48b9-8d64-e6cb166a7af6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "90fe5354-7217-4087-b98a-5c2fdeefc5e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel='time', ylabel='acceleration/$g$', fontsize=20, portion=0.5)\n",
    "xlim=(-20,80)\n",
    "ylim=(-180,120)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig,filename='./gp/motorcycle-helmet-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ],
   "id": "031c272b-70f2-4b02-8746-c24d73600364"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Helmet Data GP\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_gp/includes/motorcycle-helmet-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/motorcycle-helmet-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/motorcycle-helmet-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the motorcycle helmet accelerometer\n",
    "data.</i>"
   ],
   "id": "cdb0bb10-37cd-4da7-8dae-c7919cff7b40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Helmet Data Deep GP\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/motorcycle-helmet-deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/motorcycle-helmet-deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "ab53d700-3796-4aac-b7d6-e7889ba409b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepgp"
   ],
   "id": "bf4984cc-faa5-48e2-902d-0ba57a993df2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [y.shape[1], 1, x.shape[1]]\n",
    "inits = ['PCA']*(len(layers)-1)\n",
    "kernels = []\n",
    "for i in layers[1:]:\n",
    "    kernels += [GPy.kern.RBF(i)]\n",
    "m = deepgp.DeepGP(layers,Y=yhat, X=x, \n",
    "                  inits=inits, \n",
    "                  kernels=kernels, # the kernels for each layer\n",
    "                  num_inducing=20, back_constraint=False)\n",
    "\n",
    "\n",
    "\n",
    "m.initialize()"
   ],
   "id": "b86577f6-3a59-4d79-827d-814794baeed6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.staged_optimize(iters=(1000,1000,10000), messages=(True, True, True))"
   ],
   "id": "39c6a5a3-6886-4bd3-b551-3ad9fee99607"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "d5738498-2709-458d-b80a-dbf27104780e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, scale=scale, offset=offset, ax=ax, xlabel='time', ylabel='acceleration/$g$', fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(filename='./deepgp/motorcycle-helmet-deep-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ],
   "id": "42667e44-bba4-4d64-b656-d8532292c713"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Deep Gaussian process fit to the motorcycle helmet\n",
    "accelerometer data.</i>"
   ],
   "id": "3c3c64f6-3b39-42e0-ab31-12e4d641f057"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "02ef8afa-daaa-44ca-ba9c-c4b63518b49c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_sample(m, scale=scale, offset=offset, samps=10, ax=ax, xlabel='time', ylabel='acceleration/$g$', portion = 0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/motorcycle-helmet-deep-gp-samples.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "6f8f8ace-d8fc-4e38-8d52-a79387c1f31c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Helmet Data Deep GP\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-samples.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples from the deep Gaussian process as fitted to the\n",
    "motorcycle helmet accelerometer data.</i>"
   ],
   "id": "78c8799f-e4c6-440b-8c04-cf6a71ced00a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize(xlim=xlim, ylim=ylim, scale=scale,offset=offset, \n",
    "            xlabel=\"time\", ylabel=\"acceleration/$g$\", portion=0.5,\n",
    "            dataset='motorcycle-helmet',\n",
    "            diagrams='./deepgp')"
   ],
   "id": "f92f6b16-daf2-46ca-843a-672987e8d49e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Helmet Data Latent 1\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-0.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Mappings from the input to the latent layer for the\n",
    "motorcycle helmet accelerometer data.</i>"
   ],
   "id": "b30e7867-a864-4d5a-91d7-4952155e8d0a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Helmet Data Latent 2\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-1.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Mappings from the latent layer to the output layer for the\n",
    "motorcycle helmet accelerometer data.</i>"
   ],
   "id": "f38babdf-61ce-406c-a12f-2e26ff51f65e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "m.visualize_pinball(ax=ax, xlabel='time', ylabel='acceleration/g', \n",
    "                    points=50, scale=scale, offset=offset, portion=0.1)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/motorcycle-helmet-deep-gp-pinball.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "b96c10a8-20b0-4606-bb00-ad53be685269"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Helmet Pinball Plot\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-pinball.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Pinball plot for the mapping from input to output layer for\n",
    "the motorcycle helmet accelerometer data.</i>"
   ],
   "id": "a47c536c-65fe-456f-befb-143734e8e4d3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Wireless Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/robot-wireless-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/robot-wireless-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The robot wireless data is taken from an experiment run by Brian Ferris\n",
    "at University of Washington. It consists of the measurements of WiFi\n",
    "access point signal strengths as Brian walked in a loop. It was\n",
    "published at IJCAI in 2007 (Ferris et al., 2007)."
   ],
   "id": "e3be5496-14d1-40c8-94c4-be0477b1d082"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "import numpy as np"
   ],
   "id": "308906df-ac42-43d6-9342-f84e19f96315"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pods.datasets.robot_wireless()"
   ],
   "id": "3d5c8da7-f872-4e0f-8147-f60c344e7443"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth is recorded in the data, the actual loop is given in\n",
    "the plot below."
   ],
   "id": "64a79ab3-c6a9-489d-b73c-a18b7edf1e82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import mlai.plot as plot"
   ],
   "id": "665ab531-eab7-4072-90e5-7017a9c9bed5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "plt.plot(data['X'][:, 1], data['X'][:, 2], 'r.', markersize=5)\n",
    "ax.set_xlabel('x position', fontsize=20)\n",
    "ax.set_ylabel('y position', fontsize=20)\n",
    "mlai.write_figure(figure=fig, \n",
    "                  filename='robot-wireless-ground-truth.svg',\n",
    "                  directory='./datasets')"
   ],
   "id": "8a87214a-e55b-4ad9-b73c-051243415f50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Wireless Ground Truth\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-ground-truth.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Ground truth movement for the position taken while recording\n",
    "the multivariate time-course of wireless access point signal\n",
    "strengths.</i>\n",
    "\n",
    "We will ignore this ground truth in making our predictions, but see if\n",
    "the model can recover something similar in one of the latent layers."
   ],
   "id": "c24d3d5b-7dad-4a75-8e16-cb02aeef45ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim=1\n",
    "xlim = (-0.3, 1.3)\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(np.linspace(0,1,215),\n",
    "            data['Y'][:, output_dim], \n",
    "            'r.', markersize=5)\n",
    "\n",
    "ax.set_xlabel('time', fontsize=20)\n",
    "ax.set_ylabel('signal strength', fontsize=20)\n",
    "xlim = (-0.2, 1.2)\n",
    "ylim = (-0.6, 2.0)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ma.write_figure(figure=fig, \n",
    "                filename='robot-wireless-dim-' + str(output_dim) + '.svg', \n",
    "                directory='./datasets')"
   ],
   "id": "07947647-c525-4c8a-80bb-78064c266ab1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot WiFi Data\n",
    "\n",
    "One challenge with the data is that the signal strength ‘drops out’.\n",
    "This is because the device only tracks a limited number of wifi access\n",
    "points, when one of the access points falls outside the track, the value\n",
    "disappears (in the plot below it reads -0.5). The data is missing, but\n",
    "it is not missing at random because the implication is that the wireless\n",
    "access point must be weak to have dropped from the list of those that\n",
    "are tracked.\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-dim-1.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Output dimension 1 from the robot wireless data. This plot\n",
    "shows signal strength changing over time.</i>"
   ],
   "id": "92d0e0b4-f02c-4c72-b626-f90cfbf8dd93"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Fit to Robot Wireless Data\n",
    "\n",
    "Perform a Gaussian process fit on the data using GPy."
   ],
   "id": "d46d8505-302a-4ebd-8df2-ff2859dba637"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ],
   "id": "5c462943-3ad5-4f99-ada9-aaa707981045"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot WiFi Data GP"
   ],
   "id": "a868a0f3-80c1-4cdc-bff8-965dfb0a7fdc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mlai\n",
    "import mlai.plot as plot"
   ],
   "id": "1519023b-4605-4080-9e2f-5838d9b94eed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//gp/robot-wireless-gp-dim-1.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Gaussian process fit to the Robot Wireless dimension 1.</i>"
   ],
   "id": "f893a214-8754-4108-894c-5f6c249a67ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [y.shape[1], 10, 5, 2, 2, x.shape[1]]\n",
    "inits = ['PCA']*(len(layers)-1)\n",
    "kernels = []\n",
    "for i in layers[1:]:\n",
    "    kernels += [GPy.kern.RBF(i, ARD=True)]"
   ],
   "id": "3e90624e-325e-4be6-bc53-cb08b4849702"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = deepgp.DeepGP(layers,Y=y, X=x, inits=inits, \n",
    "                  kernels=kernels,\n",
    "                  num_inducing=50, back_constraint=False)\n",
    "m.initialize()"
   ],
   "id": "aaa8d05e-8872-43a2-a5f5-092045e50307"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.staged_optimize(messages=(True,True,True))"
   ],
   "id": "35823b8d-5e4e-48e0-a41b-6ff1dffc7a3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, output_dim=output_dim, scale=scale, offset=offset, ax=ax, \n",
    "                  xlabel='time', ylabel='signal strength', fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/robot-wireless-deep-gp-dim-' + str(output_dim)+ '.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "2e1c5390-db2d-449b-9508-7f90f37f0e73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot WiFi Data Deep GP\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/robot-wireless-deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/robot-wireless-deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-dim-1.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Fit of the deep Gaussian process to dimension 1 of the robot\n",
    "wireless data.</i>"
   ],
   "id": "e72ac656-a82c-4934-a69c-fe9b9469f48b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_sample(m, output_dim=output_dim, scale=scale, offset=offset, samps=10, ax=ax,\n",
    "                  xlabel='time', ylabel='signal strength', fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/robot-wireless-deep-gp-samples-dim-' + str(output_dim)+ '.svg', \n",
    "                  transparent=True, frameon=True)"
   ],
   "id": "2deb0c68-31b9-45ea-86f2-48955778d093"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot WiFi Data Deep GP\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-samples-dim-1.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Samples from the deep Gaussian process fit to dimension 1 of\n",
    "the robot wireless data.</i>"
   ],
   "id": "f6cea717-a635-4c9a-90c0-bf8eec49522f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot WiFi Data Latent Space"
   ],
   "id": "8936a17b-d949-43cf-abd5-44b7693586b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.plot(m.layers[-2].latent_space.mean[:, 0], \n",
    "        m.layers[-2].latent_space.mean[:, 1], \n",
    "        'r.-', markersize=5)\n",
    "\n",
    "ax.set_xlabel('latent dimension 1', fontsize=20)\n",
    "ax.set_ylabel('latent dimension 2', fontsize=20)\n",
    "\n",
    "mlai.write_figure(figure=fig, filename='./deepgp/robot-wireless-latent-space.svg', \n",
    "            transparent=True, frameon=True)"
   ],
   "id": "e7a5682e-97ef-4108-b153-be9c22d67aca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-latent-space.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Inferred two dimensional latent space from the model for the\n",
    "robot wireless data.</i>"
   ],
   "id": "cbd0d837-d6fb-4b3e-9318-4d351dc6b29b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‘High Five’ Motion Capture Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/high-five-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/high-five-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Motion capture data from the CMU motion capture data base (CMU Motion\n",
    "Capture Lab, 2003). It contains two subjects approaching each other and\n",
    "executing a ‘high five’. The subjects are number 10 and 11 and their\n",
    "motion numbers are 21."
   ],
   "id": "9afb47da-3795-4ef7-8f00-418d8e2722b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ],
   "id": "dc2c2650-4c34-417e-bfd1-3c685204de49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.cmu_mocap_high_five()"
   ],
   "id": "d154bd9d-e15d-4501-80b1-11a25dd4f5c2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary contains the keys ‘Y1’ and ‘Y2’, which represent the\n",
    "motions of the two different subjects. Their skeleton files are included\n",
    "in the keys ‘skel1’ and ‘skel2’."
   ],
   "id": "76fd40b4-eecd-48e7-93cc-5e072b0e419f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Y1'].shape\n",
    "data['Y2'].shape"
   ],
   "id": "2f7b36f8-b8cd-4dca-b592-a66010d8dcee"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was used in the hierarchical GP-LVM paper (Lawrence and Moore,\n",
    "2007) in an experiment that was also recreated in the Deep Gaussian\n",
    "process paper (Damianou and Lawrence, 2013)."
   ],
   "id": "bde8bf08-1bfb-4c0e-8ad9-994e38252cab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['citation'])"
   ],
   "id": "25d35c6e-e9ad-4edf-9487-ed1dcd032c0b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extra information about the data is included, as standard, under the\n",
    "keys `info` and `details`."
   ],
   "id": "7678f3de-1431-411b-81ca-9066ba929168"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['info'])\n",
    "print()\n",
    "print(data['details'])"
   ],
   "id": "785475d2-3554-455a-b14b-0d83a096cf93"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared LVM\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/high-five-deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/high-five-deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//shared.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Shared latent variable model structure. Here two related data\n",
    "sets are brought together with a set of latent variables that are\n",
    "partially shared and partially specific to one of the data sets.</i>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//deep-gp-high-five2.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Latent spaces of the ‘high five’ data. The structure of the\n",
    "model is automatically learnt. One of the latent spaces is coordinating\n",
    "how the two figures walk together, the other latent spaces contain\n",
    "latent variables that are specific to each of the figures\n",
    "separately.</i>"
   ],
   "id": "9b33495e-a990-4387-aac8-e3de796af77e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample of the MNIST Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "We will look at a sub-sample of the MNIST digit data set.\n",
    "\n",
    "First load in the MNIST data set from scikit learn. This can take a\n",
    "little while because it’s large to download."
   ],
   "id": "3ca09416-4572-4103-bd3a-f8d2d9cfe63a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ],
   "id": "7459fed6-5aa1-4004-ad56-640c2e79e3d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')"
   ],
   "id": "a657acfe-c982-4c0e-8aad-0e2990088dee"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-sample the dataset to make the training faster."
   ],
   "id": "c31b71e4-7078-4a3c-872d-edb4b1c2be64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "id": "d0495b41-aece-4d47-bf88-5d3f3928e53a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "digits = [0,1,2,3,4]\n",
    "N_per_digit = 100\n",
    "Y = []\n",
    "labels = []\n",
    "for d in digits:\n",
    "    imgs = mnist['data'][mnist['target']==str(d)]\n",
    "    Y.append(imgs.loc[np.random.permutation(imgs.index)[:N_per_digit]])\n",
    "    labels.append(np.ones(N_per_digit)*d)\n",
    "Y = np.vstack(Y).astype(np.float64)\n",
    "labels = np.hstack(labels)\n",
    "Y /= 255"
   ],
   "id": "5f3cd278-8e61-4290-a714-f0fb050bd772"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Deep GP to a the MNIST Digits Subsample\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/mnist-digits-subsample-deep-gp.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/mnist-digits-subsample-deep-gp.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip6\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Zhenwen Dai\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/zhenwen-dai.jpg\" clip-path=\"url(#clip6)\"/>\n",
    "\n",
    "</svg>\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip7\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Andreas Damianou\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"https://inverseprobability.com/talks/../slides/diagrams//people/andreas-damianou.png\" clip-path=\"url(#clip7)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "We now look at the deep Gaussian processes’ capacity to perform\n",
    "unsupervised learning."
   ],
   "id": "f38a4d81-46de-45a5-bf74-4122819ea78e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Deep GP\n",
    "\n",
    "We’re going to fit a Deep Gaussian process model to the MNIST data with\n",
    "two hidden layers. Each of the two Gaussian processes (one from the\n",
    "first hidden layer to the second, one from the second hidden layer to\n",
    "the data) has an exponentiated quadratic covariance."
   ],
   "id": "3b4f1a79-1b39-47ff-889c-0f7b6def3047"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepgp\n",
    "import GPy"
   ],
   "id": "1121f41e-5471-4e65-8499-1e7de0a1b9ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent = 2\n",
    "num_hidden_2 = 5\n",
    "m = deepgp.DeepGP([Y.shape[1],num_hidden_2,num_latent],\n",
    "                  Y,\n",
    "                  kernels=[GPy.kern.RBF(num_hidden_2,ARD=True), \n",
    "                           GPy.kern.RBF(num_latent,ARD=False)], \n",
    "                  num_inducing=50, back_constraint=False, \n",
    "                  encoder_dims=[[200],[200]])"
   ],
   "id": "31994234-65de-4ba4-9db6-df8c045e7228"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Just like deep neural networks, there are some tricks to intitializing\n",
    "these models. The tricks we use here include some early training of the\n",
    "model with model parameters constrained. This gives the variational\n",
    "inducing parameters some scope to tighten the bound for the case where\n",
    "the noise variance is small and the variances of the Gaussian processes\n",
    "are around 1."
   ],
   "id": "d6e36d78-bca0-41f3-baab-841cbf61f065"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.obslayer.likelihood.variance[:] = Y.var()*0.01\n",
    "for layer in m.layers:\n",
    "    layer.kern.variance.fix(warning=False)\n",
    "    layer.likelihood.variance.fix(warning=False)"
   ],
   "id": "ab9c5714-fa32-43f2-b6c9-fba33e15d2f0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now we optimize for a hundred iterations with the constrained model."
   ],
   "id": "e04ba2cf-b5d5-4af1-a3f2-11330289d2a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize(messages=False,max_iters=100)"
   ],
   "id": "ebc8b374-e5ce-463b-9828-6ced326c3d81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove the fixed constraint on the kernel variance parameters,\n",
    "but keep the noise output constrained, and run for a further 100\n",
    "iterations."
   ],
   "id": "f70162ed-f940-40d1-8758-f175e17342a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in m.layers:\n",
    "    layer.kern.variance.constrain_positive(warning=False)\n",
    "m.optimize(messages=False,max_iters=100)"
   ],
   "id": "0e980e5d-21c6-447d-a2d6-72a13d749c3c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we unconstrain the layer likelihoods and allow the full model to\n",
    "be trained for 1000 iterations."
   ],
   "id": "87e2f371-a8cb-4bf7-a0a3-ed06e9416815"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in m.layers:\n",
    "    layer.likelihood.variance.constrain_positive(warning=False)\n",
    "m.optimize(messages=True,max_iters=10000)"
   ],
   "id": "69a7b508-1aab-418f-b943-db2901fb2946"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the latent space of the top layer\n",
    "\n",
    "Now the model is trained, let’s plot the mean of the posterior\n",
    "distributions in the top latent layer of the model."
   ],
   "id": "1dcecf18-a821-4425-9140-252e8bf25793"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "9b2e76d0-5342-41a9-bb66-b05879d4b891"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc"
   ],
   "id": "4a7584ad-f643-40d5-aaf7-58997b984974"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica'],'size':20})\n",
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "for d in digits:\n",
    "    ax.plot(m.layer_1.X.mean[labels==d,0],m.layer_1.X.mean[labels==d,1],'.',label=str(d))\n",
    "_ = plt.legend()\n",
    "mlai.write_figure(figure=fig, filename=\"./deepgp/mnist-digits-subsample-latent.svg\", transparent=True)"
   ],
   "id": "bff10ac6-6cec-4750-ac06-55a745de0484"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-latent.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Latent space for the deep Gaussian process learned through\n",
    "unsupervised learning and fitted to a subset of the MNIST digits\n",
    "subsample.</i>"
   ],
   "id": "6306a5b5-8d40-4673-b5c3-b4b3c4a3f63c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the latent space of the intermediate layer\n",
    "\n",
    "We can also visualize dimensions of the intermediate layer. First the\n",
    "lengthscale of those dimensions is given by"
   ],
   "id": "abdf6234-0076-425a-aadc-d16405f22e5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.obslayer.kern.lengthscale"
   ],
   "id": "a73455f9-f3f5-41c1-9fe6-101603434433"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "fb114f47-14ee-40e7-b288-67beda761734"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "for i in range(5):\n",
    "    for j in range(i):\n",
    "        dims=[i, j]\n",
    "        ax.cla()\n",
    "        for d in digits:\n",
    "            ax.plot(m.obslayer.X.mean[labels==d,dims[0]],\n",
    "                 m.obslayer.X.mean[labels==d,dims[1]],\n",
    "                 '.', label=str(d))\n",
    "        plt.legend()\n",
    "        plt.xlabel('dimension ' + str(dims[0]))\n",
    "        plt.ylabel('dimension ' + str(dims[1]))\n",
    "        mlai.write_figure(figure=fig, filename=\"./deepgp/mnist-digits-subsample-hidden-\" + str(dims[0]) + '-' + str(dims[1]) + '.svg', transparent=True)"
   ],
   "id": "dd0519a0-a336-4c80-adcd-5dc09f72b4d8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-1-0.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Visualisation of the intermediate layer, plot of dimension 1\n",
    "vs dimension 0.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-2-0.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Visualisation of the intermediate layer, plot of dimension 1\n",
    "vs dimension 0.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-3-0.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Visualisation of the intermediate layer, plot of dimension 1\n",
    "vs dimension 0.</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-4-0.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Visualisation of the intermediate layer, plot of dimension 1\n",
    "vs dimension 0.</i>"
   ],
   "id": "28f885c5-1233-41e6-ad52-6ecd9f89bdf1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate From Model\n",
    "\n",
    "Now we can take a look at a sample from the model, by drawing a Gaussian\n",
    "random sample in the latent space and propagating it through the model."
   ],
   "id": "3e938a81-5c23-4f8c-901f-ef15c572bdc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = 10\n",
    "cols = 20\n",
    "t=np.linspace(-1, 1, rows*cols)[:, None]\n",
    "kern = GPy.kern.RBF(1,lengthscale=0.05)\n",
    "cov = kern.K(t, t)\n",
    "x = np.random.multivariate_normal(np.zeros(rows*cols), cov, num_latent).T"
   ],
   "id": "e2b248b5-42ab-4046-a523-f713e1785e57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "84d50269-f8e6-4901-ab3e-111c883a0895"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = m.predict(x)\n",
    "fig, axs = plt.subplots(rows,cols,figsize=(10,6))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        #v = np.random.normal(loc=yt[0][i*cols+j, :], scale=np.sqrt(yt[1][i*cols+j, :]))\n",
    "        v = yt[0][i*cols+j, :]\n",
    "        axs[i,j].imshow(v.reshape(28,28), \n",
    "                        cmap='gray', interpolation='none',\n",
    "                        aspect='equal')\n",
    "        axs[i,j].set_axis_off()\n",
    "mlai.write_figure(figure=fig, filename=\"./deepgp/digit-samples-deep-gp.svg\", transparent=True)"
   ],
   "id": "ee7ca90b-f559-48d6-bb29-1f7ca13711f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deepgp/digit-samples-deep-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>These digits are produced by taking a tour of the two\n",
    "dimensional latent space (as described by a Gaussian process sample) and\n",
    "mapping the tour into the data space. We visualize the mean of the\n",
    "mapping in the images.</i>"
   ],
   "id": "cbc5872a-6d20-48db-a7f1-8a31d604d7ad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Health\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_health/includes/deep-health-model.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/deep-health-model.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//deep-health.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The deep health model uses different layers of abstraction in\n",
    "the deep Gaussian process to represent information about diagnostics and\n",
    "treatment to model interelationships between a patients different data\n",
    "modalities.</i>\n",
    "\n",
    "From a machine learning perspective, we’d like to be able to interrelate\n",
    "all the different modalities that are informative about the state of the\n",
    "disease. For deep health, the notion is that the state of the disease is\n",
    "appearing at the more abstract levels, as we descend the model, we\n",
    "express relationships between the more abstract concept, that sits\n",
    "within the physician’s mind, and the data we can measure."
   ],
   "id": "c4e60aad-789b-44e6-81b9-8b240dee8d53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The probabilistic modelling community has evolved in an era where the\n",
    "assumption was that ambiguous conclusions are best shared with a\n",
    "(trained) professional through probabilities. Recent advances in\n",
    "generative AI offer the possibility of machines that have a better\n",
    "understanding of human subjective ambiguities and therefore machines\n",
    "that can summarise information in a way that can be interogated rather\n",
    "than just through a series of numbers."
   ],
   "id": "ede96356-5db6-44f0-b279-e3382320d309"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   book: [The Atomic\n",
    "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ],
   "id": "cc982c94-c908-4cc2-80e5-cf44fae9d203"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ],
   "id": "167e34ac-112f-4d95-8440-25f0a38b5eef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ananthanarayanan, R., Esser, S.K., Simon, H.D., Modha, D.S., 2009. The\n",
    "cat is out of the bag: Cortical simulations with $10^9$ neurons,\n",
    "$10^{13}$ synapses, in: Proceedings of the Conference on High\n",
    "Performance Computing Networking, Storage and Analysis - SC ’09.\n",
    "<https://doi.org/10.1145/1654059.1654124>\n",
    "\n",
    "Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.\n",
    "Consistent mapping of government malaria records across a changing\n",
    "territory delimitation. Malaria Journal 13.\n",
    "<https://doi.org/10.1186/1475-2875-13-S1-P5>\n",
    "\n",
    "Arora, S., Cohen, N., Golowich, N., Hu, W., 2019. [A convergence\n",
    "analysis of gradient descent for deep linear neural\n",
    "networks](https://openreview.net/forum?id=SkMQg3C5K7), in: International\n",
    "Conference on Learning Representations.\n",
    "\n",
    "Bochner, S., 1959. [Lectures on Fourier\n",
    "integrals](http://books.google.co.uk/books?id=-vU02QewWK8C). Princeton\n",
    "University Press.\n",
    "\n",
    "Boltzmann, L., n.d. Über die Beziehung zwischen dem zweiten Hauptsatze\n",
    "der mechanischen Warmetheorie und der Wahrscheinlichkeitsrechnung,\n",
    "respective den Sätzen über das wärmegleichgewicht. Sitzungberichte der\n",
    "Kaiserlichen Akademie der Wissenschaften. Mathematisch-Naturwissen\n",
    "Classe. Abt. II LXXVI, 373–435.\n",
    "\n",
    "Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., Turner, R.,\n",
    "2016. [Deep Gaussian processes for regression using approximate\n",
    "expectation propagation](http://proceedings.mlr.press/v48/bui16.html),\n",
    "in: Balcan, M.F., Weinberger, K.Q. (Eds.), Proceedings of the 33rd\n",
    "International Conference on Machine Learning, Proceedings of Machine\n",
    "Learning Research. PMLR, New York, New York, USA, pp. 1472–1481.\n",
    "\n",
    "Cabrera, C., Paleyes, A., Thodoroff, P., Lawrence, N.D., 2023.\n",
    "[Real-world machine learning systems: A survey from a data-oriented\n",
    "architecture perspective](https://arxiv.org/abs/2302.04810).\n",
    "\n",
    "Cho, Y., Saul, L.K., 2009. [Kernel methods for deep\n",
    "learning](http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf),\n",
    "in: Bengio, Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I.,\n",
    "Culotta, A. (Eds.), Advances in Neural Information Processing Systems\n",
    "22. Curran Associates, Inc., pp. 342–350.\n",
    "\n",
    "CMU Motion Capture Lab, 2003. The CMU mocap database.\n",
    "\n",
    "Coales, J.F., Kane, S.J., 2014. The “yellow peril” and after. IEEE\n",
    "Control Systems Magazine 34, 65–69.\n",
    "<https://doi.org/10.1109/MCS.2013.2287387>\n",
    "\n",
    "Damianou, A., 2015. Deep Gaussian processes and variational propagation\n",
    "of uncertainty (PhD thesis). University of Sheffield.\n",
    "\n",
    "Damianou, A., Lawrence, N.D., 2013. Deep Gaussian processes. pp.\n",
    "207–215.\n",
    "\n",
    "Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference\n",
    "for latent variables and uncertain inputs in Gaussian processes. Journal\n",
    "of Machine Learning Research 17.\n",
    "\n",
    "Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D.,\n",
    "Missero, C., Bernardo, D. di, 2008. Direct targets of the TRP63\n",
    "transcription factor revealed by a combination of gene expression\n",
    "profiling and reverse engineering. Genome Research 18, 939–948.\n",
    "<https://doi.org/10.1101/gr.073601.107>\n",
    "\n",
    "Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. [How\n",
    "deep are deep Gaussian\n",
    "processes?](http://jmlr.org/papers/v19/18-015.html) Journal of Machine\n",
    "Learning Research 19, 1–46.\n",
    "\n",
    "Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding\n",
    "pathologies in very deep networks.\n",
    "\n",
    "Eddington, A.S., 1929. The nature of the physical world. Dent (London).\n",
    "<https://doi.org/10.2307/2180099>\n",
    "\n",
    "Ferris, B.D., Fox, D., Lawrence, N.D., 2007. WiFi-SLAM using Gaussian\n",
    "process latent variable models, in: Veloso, M.M. (Ed.), Proceedings of\n",
    "the 20th International Joint Conference on Artificial Intelligence\n",
    "(IJCAI 2007). pp. 2480–2485.\n",
    "\n",
    "Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin,\n",
    "D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.\n",
    "\n",
    "Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,\n",
    "Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data\n",
    "from health management information systems in Africa using space–time\n",
    "geostatistics. PLoS Medicine 3.\n",
    "<https://doi.org/10.1371/journal.pmed.0030271>\n",
    "\n",
    "Havasi, M., Hernández-Lobato, J.M., Murillo-Fuentes, J.J., 2018.\n",
    "[Inference in deep Gaussian processes using stochastic gradient\n",
    "Hamiltonian Monte\n",
    "Carlo](http://papers.nips.cc/paper/7979-inference-in-deep-gaussian-processes-using-stochastic-gradient-hamiltonian-monte-carlo.pdf),\n",
    "in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi,\n",
    "N., Garnett, R. (Eds.), Advances in Neural Information Processing\n",
    "Systems 31. Curran Associates, Inc., pp. 7506–7516.\n",
    "\n",
    "Heider, F., 1958. The psychology of interpersonal relations. John Wiley.\n",
    "\n",
    "Heider, F., Simmel, M., 1944. An experimental study of apparent\n",
    "behavior. The American Journal of Psychology 57, 243–259.\n",
    "<https://doi.org/10.2307/1416950>\n",
    "\n",
    "Henrich, J., Muthukrishna, M., 2021. The origins and psychology of human\n",
    "cooperation. Annual Review of Psychology 72, 207–240.\n",
    "<https://doi.org/10.1146/annurev-psych-081920-042106>\n",
    "\n",
    "Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng,\n",
    "A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T.,\n",
    "Brown, N., Luu, L., Levine, S., Hausman, K., ichter, brian, 2023. [Inner\n",
    "monologue: Embodied reasoning through planning with language\n",
    "models](https://proceedings.mlr.press/v205/huang23c.html), in: Liu, K.,\n",
    "Kulic, D., Ichnowski, J. (Eds.), Proceedings of the 6th Conference on\n",
    "Robot Learning, Proceedings of Machine Learning Research. PMLR, pp.\n",
    "1769–1782.\n",
    "\n",
    "Ioffe, S., Szegedy, C., 2015. [Batch normalization: Accelerating deep\n",
    "network training by reducing internal covariate\n",
    "shift](http://proceedings.mlr.press/v37/ioffe15.html), in: Bach, F.,\n",
    "Blei, D. (Eds.), Proceedings of the 32nd International Conference on\n",
    "Machine Learning, Proceedings of Machine Learning Research. PMLR, Lille,\n",
    "France, pp. 448–456.\n",
    "\n",
    "Izmailov, P., Maddox, W.J., Kirichenko, P., Garipov, T., Vetrov, D.P.,\n",
    "Wilson, A.G., 2019. [Subspace inference for bayesian deep\n",
    "learning](http://arxiv.org/abs/1907.07504). CoRR abs/1907.07504.\n",
    "\n",
    "Jacot, A., Ged, F., Gabriel, F., Şimşek, B., Hongler, C., 2021. [Deep\n",
    "linear networks dynamics: Low-rank biases induced by initialization\n",
    "scale and L2 regularization](https://arxiv.org/abs/2106.15933).\n",
    "\n",
    "Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking\n",
    "differentially expressed gene expression time courses through Gaussian\n",
    "process regression. BMC Bioinformatics 12.\n",
    "<https://doi.org/10.1186/1471-2105-12-180>\n",
    "\n",
    "Lawrence, N.D., 2024. The atomic human: Understanding ourselves in the\n",
    "age of AI. Allen Lane.\n",
    "\n",
    "Lawrence, N.D., 2017. [Living together: Mind and machine\n",
    "intelligence](https://arxiv.org/abs/1705.07996). arXiv.\n",
    "\n",
    "Lawrence, N.D., Moore, A.J., 2007. Hierarchical Gaussian process latent\n",
    "variable models. pp. 481–488.\n",
    "\n",
    "MacKay, D.J.C., n.d. Introduction to Gaussian processes. pp. 133–166.\n",
    "\n",
    "MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis).\n",
    "California Institute of Technology.\n",
    "\n",
    "MacKay, D.M., 1991. Behind the eye. Basil Blackwell.\n",
    "\n",
    "McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas\n",
    "immanent in nervous activity. Bulletin of Mathematical Biophysics 5,\n",
    "115–133. <https://doi.org/10.1007/BF02478259>\n",
    "\n",
    "Mikhailov, G.K., n.d. Daniel bernoulli, hydrodynamica (1738).\n",
    "\n",
    "Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,\n",
    "N.D., 2014. Malaria surveillance with multiple data sources using\n",
    "Gaussian process models, in: 1st International Conference on the Use of\n",
    "Mobile ICT in Africa.\n",
    "\n",
    "Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis).\n",
    "Dept. of Computer Science, University of Toronto.\n",
    "\n",
    "Pearl, J., 1995. From Bayesian networks to causal networks, in:\n",
    "Gammerman, A. (Ed.), Probabilistic Reasoning and Bayesian Belief\n",
    "Networks. Alfred Waller, pp. 1–31.\n",
    "\n",
    "Reed, C., Durlach, N.I., 1998. Note on information transfer rates in\n",
    "human communication. Presence Teleoperators & Virtual Environments 7,\n",
    "509–518. <https://doi.org/10.1162/105474698565893>\n",
    "\n",
    "Salimbeni, H., Deisenroth, M., 2017. [Doubly stochastic variational\n",
    "inference for deep Gaussian\n",
    "processes](http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf),\n",
    "in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R.,\n",
    "Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information\n",
    "Processing Systems 30. Curran Associates, Inc., pp. 4591–4602.\n",
    "\n",
    "Sharp, K., Matschinsky, F., 2015. Translation of Ludwig Boltzmann’s\n",
    "paper “on the relationship between the second fundamental theorem of the\n",
    "mechanical theory of heat and probability calculations regarding the\n",
    "conditions for thermal equilibrium.” Entropy 17, 1971–2009.\n",
    "<https://doi.org/10.3390/e17041971>\n",
    "\n",
    "Simons, D.J., Chabris, C.F., 1999. Gorillas in our midst: Sustained\n",
    "inattentional blindness for dynamic events. Perception 28, 1059–1074.\n",
    "<https://doi.org/10.1068/p281059>\n",
    "\n",
    "Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson,\n",
    "E., Avital, I., Stojadinovic, A., 2012. Using machine-learned Bayesian\n",
    "belief networks to predict perioperative risk of clostridium difficile\n",
    "infection following colon surgery. Interact J Med Res 1, e6.\n",
    "<https://doi.org/10.2196/ijmr.2131>\n",
    "\n",
    "Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. DeepFace: Closing\n",
    "the gap to human-level performance in face verification, in: Proceedings\n",
    "of the IEEE Computer Society Conference on Computer Vision and Pattern\n",
    "Recognition. <https://doi.org/10.1109/CVPR.2014.220>\n",
    "\n",
    "The Admiralty, 1945. [The gunnery pocket book, b.r.\n",
    "224/45](https://www.maritime.org/doc/br224/).\n",
    "\n",
    "Thompson, W.C., 1989. [Are juries competent to evaluate statistical\n",
    "evidence?](http://www.jstor.org/stable/1191906) Law and Contemporary\n",
    "Problems 52, 9–41.\n",
    "\n",
    "Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component\n",
    "analysis. Journal of the Royal Statistical Society, B 6, 611–622.\n",
    "<https://doi.org/doi:10.1111/1467-9868.00196>\n",
    "\n",
    "Wiener, N., 1953. Ex-prodigy: My childhood and youth. mitp, Cambridge,\n",
    "MA.\n",
    "\n",
    "Wiener, N., 1949. The extrapolation, interpolation and smoothing of\n",
    "stationary time series with engineering applications. wiley."
   ],
   "id": "60c2b207-8f7c-4109-84bd-5f5ae8bece35"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
