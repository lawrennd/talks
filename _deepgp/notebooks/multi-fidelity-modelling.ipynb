{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-fidelity Modelling\n",
    "\n",
    "Deep neural networks are a powerful approach to dealing with images, speech and language. All interesting domains where we are gathering a large amount of data. \n",
    "\n",
    "Earlier on in my career, I learned an important lesson. I had discovered Gaussian processes, and was interested in approaches for making them practical. Particularly for 'popular' tasks of the day. One of which was classification of handwritten digits: the MNIST data. \n",
    "\n",
    "With Matthias Seeger and Ralf Herbrich I worked on an algorithm for fitting Gaussian process classifier models that was able to scale as well as the dominant methodology of the day, support vector machines, and perform well. The paper was published at NIPS and the work is known as the \"informative vector machine\". It's a moderately successful work, but it did not have the hoped for response of alerting the wider SVM community to the utility and flexibility of Gaussian process models. \n",
    "\n",
    "It's horses for courses, and where as GP Classifiers are interesting models, it seems fairly pointless to direct efforts at making a methodology work well in a domain where we already have good coverage. The real success of Gaussian process models came in domains where other frameworks were not appropriate. Hyper parameter optimization springs to mind [@Snoek:practical12], a paper with over a thousand citations. My own most successful work, GP-LVM exploits characteristics of GPs that are not associated with other models [@Lawrence:pnpca05].\n",
    "\n",
    "With that in mind my own philosophy is to look explicitly for domains where the characteristics of Gaussian process models can be more usefully exploited, that is not to say that these models can't be used in the more traditional domains, but a lot of effort could be placed there to merely making them also-rans, whereas in domains where uncertainty is critical the balance is tilted in favour of GPs.\n",
    "\n",
    "As a broad domain I'm excited by the domains of probabilistic numerics, surrogate modelling, emulation and uncertainty quantification. There are two reasons for this. Firstly, while I'm not a fan of the term artificial intelligence, or the image instigated in the public mind by the term, or the narrative futures it implies. The truth is that we are faced with a need for an increasing amount of algorithmic decision making. Decision making that is based on data. The dominant framework for achieving those ends, at the moment, is machine learning. That is through the combination of data with the model to form the prediction, followed by *decision making*. The term AI definitely includes decision making as part of its requirements. But a critical challenge is decision making in the presence of uncertainty. \n",
    "\n",
    "Data in our phones, and our devices is all well in good, but things get more interesting when considering the interaction between our virtual world and the physical world. Interaction is key, because we can choose to spend more effort on data acquisition, computation or inference. Each of these requires a decision.\n",
    "\n",
    "Wikipedia defines [Uncertainty Quanitification](https://en.wikipedia.org/wiki/Uncertainty_quantification) as:\n",
    "\n",
    ">Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known.\n",
    "\n",
    "And this drives to the heart of what we require for practical decision making systems. Probabilistic numerics associates uncertainty (in the form of probabilities) with our computations. Emulation (or surrogate modelling) is a key component of this approach because an emulator is a model, we can think of it as a machine learning model, of a computational process. For example, we can think of the Gaussian process model in Bayesian optimization as *surrogate* for the process of optimization. A way of assessing our expectations of what that answer will be without running the optimization. Bayesian optimization then involves an *acquisition function* which encodes our decision making framework about what parameters to try next. \n",
    "\n",
    "Bayesian optimization, therefore, presents a microcosm of the world of uncertainty quantification, where we are taking the particular approach of emulating the system of interest, often with a Gaussian process. We know they perform well in these domains. We have\n",
    "\n",
    "The interaction between the physical and virtual worlds was, for me, a major reason for working at Amazon. When I was exploring Amazon, a colleague explained to me that Amazon is a 'bits and atoms' company. It moves stuff and it moves information. For me, an excellent definition of intelligence is the use of *information* to achieve goals with *less resource*. Amazon is therefore an excellent ecosystem for exploring intelligence. However, to do so we need good models of the physical world, and approaches to decision making in the physical world. \n",
    "\n",
    "But let's move aware from personal corporate allegiances and in to a domain that is more neutral territory for most machine learning practitioners. Formula one motor racing. How can we use information and compute to improve motor racing?\n",
    "\n",
    "For our practical example I want to \n",
    "\n",
    "\n",
    "in the public mind  reinterpretation of numerical algorithms as probabilistic. Why? Well in artificial intelligence we are interested in decision making \n",
    "As an example of a domain where deep Gaussian process models are an interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@incollection{Snoek:practical12,\n",
    "title = {Practical Bayesian Optimization of Machine Learning Algorithms},\n",
    "author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},\n",
    "booktitle = {Advances in Neural Information Processing Systems 25},\n",
    "editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},\n",
    "pages = {2951--2959},\n",
    "year = {2012},\n",
    "publisher = {Curran Associates, Inc.},\n",
    "url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf}\n",
    "}\n",
    "@Article{Lawrence:pnpca05,\n",
    "  author =\t {Neil D. Lawrence},\n",
    "  title =\t {Probabilistic Non-linear Principal Component\n",
    "                  Analysis with {G}aussian Process Latent Variable\n",
    "                  Models},\n",
    "  journal =\t jmlr,\n",
    "  year =\t 2005,\n",
    "  volume =\t 6,\n",
    "  pages =\t {1783--1816},\n",
    "  month =\t 11,\n",
    "  pdf = {http://www.jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf},\n",
    "  abstract =\t {Summarising a high dimensional data set with a low\n",
    "                  dimensional embedding is a standard approach for\n",
    "                  exploring its structure. In this paper we provide an\n",
    "                  overview of some existing techniques for discovering\n",
    "                  such embeddings. We then introduce a novel\n",
    "                  probabilistic interpretation of principal component\n",
    "                  analysis (PCA) that we term dual probabilistic PCA\n",
    "                  (DPPCA). The DPPCA model has the additional\n",
    "                  advantage that the linear mappings from the embedded\n",
    "                  space can easily be non-linearised through Gaussian\n",
    "                  processes. We refer to this model as a Gaussian\n",
    "                  process latent variable model (GP-LVM). Through\n",
    "                  analysis of the GP-LVM objective function, we relate\n",
    "                  the model to popular spectral techniques such as\n",
    "                  kernel PCA and multidimensional scaling. We then\n",
    "                  review a practical algorithm for GP-LVMs in the\n",
    "                  context of large data sets and develop it to also\n",
    "                  handle discrete valued data and missing\n",
    "                  attributes. We demonstrate the model on a range of\n",
    "                  real-world and artificially generated data sets.}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
