---
title: "The Atomic Human"
venue: "St Andrews’ Distinguished Lecture Series"
abstract: "<p>A vital perspective is missing from the discussions we’re
having about Artificial Intelligence: what does it mean for our
identity?</p>
<p>Our fascination with AI stems from the perceived uniqueness of human
intelligence. We believe it’s what differentiates us. Fears of AI not
only concern how it invades our digital lives, but also the implied
threat of an intelligence that displaces us from our position at the
centre of the world.</p>
<p>Atomism, proposed by Democritus, suggested it was impossible to
continue dividing matter down into ever smaller components: eventually
we reach a point where a cut cannot be made (the Greek for uncuttable is
‘atom’). In the same way, by slicing away at the facets of human
intelligence that can be replaced by machines, AI uncovers what is left:
an indivisible core that is the essence of humanity.</p>
<p>By contrasting our own (evolved, locked-in, embodied) intelligence
with the capabilities of machine intelligence through history, The
Atomic Human reveals the technical origins, capabilities and limitations
of AI systems, and how they should be wielded. Not just by the experts,
but ordinary people. Either AI is a tool for us, or we become a tool of
AI. Understanding this will enable us to choose the future we want.</p>
<p>This talk is based on Neil’s forthcoming book to be published with
Allen Lane in June 2024. Machine learning solutions, in particular those
based on deep learning methods, form an underpinning of the current
revolution in “artificial intelligence” that has dominated popular press
headlines and is having a significant influence on the wider tech
agenda.</p>
<p>In this talk I will give an overview of where we are now with machine
learning solutions, and what challenges we face both in the near and far
future. These include practical application of existing algorithms in
the face of the need to explain decision making, mechanisms for
improving the quality and availability of data, dealing with large
unstructured datasets.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_ai/the-atomic-human-st-andrews.md
date: 2024-03-12
published: 2024-03-12
reveal: 2024-03-12-the-atomic-human-st-andrews.slides.html
transition: None
ipynb: 2024-03-12-the-atomic-human-st-andrews.ipynb
slidesipynb: 2024-03-12-the-atomic-human-st-andrews.slides.ipynb
layout: talk
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="the-atomic-human">The Atomic Human</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-human.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-human.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-atomic-human-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/the-atomic-human.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-atomic-human-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-atomic-human&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-atomic-human-caption" class="caption-frame">
<p>Figure: <a href="https://www.amazon.co.uk/dp/B0CGZHBSLL">The Atomic
Human</a> <span class="citation"
data-cites="Lawrence-atomic24">(Lawrence, 2024)</span> due for release
in June 2024.</p>
</div>
</div>
<h2 id="henry-fords-faster-horse">Henry Ford’s Faster Horse</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/henry-ford-intro.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/henry-ford-intro.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="ford-model-t-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/1925_Ford_Model_T_touring.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="ford-model-t-magnify" class="magnify"
onclick="magnifyFigure(&#39;ford-model-t&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ford-model-t-caption" class="caption-frame">
<p>Figure: A 1925 Ford Model T built at Henry Ford’s Highland Park Plant
in Dearborn, Michigan. This example now resides in Australia, owned by
the founder of FordModelT.net. From <a
href="https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg"
class="uri">https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg</a></p>
</div>
</div>
<p>It’s said that Henry Ford’s customers wanted a “a faster horse”. If
Henry Ford was selling us artificial intelligence today, what would the
customer call for, “a smarter human”? That’s certainly the picture of
machine intelligence we find in science fiction narratives, but the
reality of what we’ve developed is much more mundane.</p>
<p>Car engines produce prodigious power from petrol. Machine
intelligences deliver decisions derived from data. In both cases the
scale of consumption enables a speed of operation that is far beyond the
capabilities of their natural counterparts. Unfettered energy
consumption has consequences in the form of climate change. Does
unbridled data consumption also have consequences for us?</p>
<p>If we devolve decision making to machines, we depend on those
machines to accommodate our needs. If we don’t understand how those
machines operate, we lose control over our destiny. Our mistake has been
to see machine intelligence as a reflection of our intelligence. We
cannot understand the smarter human without understanding the human. To
understand the machine, we need to better understand ourselves.</p>
<h2 id="embodiment-factors">Embodiment Factors</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
<div id="embodiment-factors-table-magnify" class="magnify"
onclick="magnifyFigure(&#39;embodiment-factors-table&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="embodiment-factors-table-caption" class="caption-frame">
<p>Figure: Embodiment factors are the ratio between our ability to
compute and our ability to communicate. Relative to the machine we are
also locked in. In the table we represent embodiment as the length of
time it would take to communicate one second’s worth of computation. For
computers it is a matter of minutes, but for a human, it is a matter of
thousands of millions of years. See also “Living Together: Mind and
Machine Intelligence” <span class="citation"
data-cites="Lawrence:embodiment17">Lawrence (2017)</span></p>
</div>
</div>
<p>There is a fundamental limit placed on our intelligence based on our
ability to communicate. Claude Shannon founded the field of information
theory. The clever part of this theory is it allows us to separate our
measurement of information from what the information pertains to.<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></p>
<p>Shannon measured information in bits. One bit of information is the
amount of information I pass to you when I give you the result of a coin
toss. Shannon was also interested in the amount of information in the
English language. He estimated that on average a word in the English
language contains 12 bits of information.</p>
<p>Given typical speaking rates, that gives us an estimate of our
ability to communicate of around 100 bits per second <span
class="citation" data-cites="Reed-information98">(Reed and Durlach,
1998)</span>. Computers on the other hand can communicate much more
rapidly. Current wired network speeds are around a billion bits per
second, ten million times faster.</p>
<p>When it comes to compute though, our best estimates indicate our
computers are slower. A typical modern computer can process make around
100 billion floating-point operations per second, each floating-point
operation involves a 64 bit number. So the computer is processing around
6,400 billion bits per second.</p>
<p>It’s difficult to get similar estimates for humans, but by some
estimates the amount of compute we would require to <em>simulate</em> a
human brain is equivalent to that in the UK’s fastest computer <span
class="citation" data-cites="Ananthanarayanan-cat09">(Ananthanarayanan
et al., 2009)</span>, the MET office machine in Exeter, which in 2018
ranked as the 11th fastest computer in the world. That machine simulates
the world’s weather each morning, and then simulates the world’s climate
in the afternoon. It is a 16-petaflop machine, processing around 1,000
<em>trillion</em> bits per second.</p>
<h2 id="revolution">Revolution</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/cuneiform.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/cuneiform.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Arguably the information revolution we are experiencing is
unprecedented in history. But changes in the way we share information
have a long history. Over 5,000 years ago in the city of Uruk, on the
banks of the Euphrates, communities which relied on the water to
irrigate their corps developed an approach to recording transactions in
clay. Eventually the system of recording system became sophisticated
enough that their oral histories could be recorded in the form of the
first epic: Gilgamesh.</p>
<div class="figure">
<div id="chicago-cuneiform-stone-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//cuneiform/chicago-cuneiform-stone.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="chicago-cuneiform-stone-magnify" class="magnify"
onclick="magnifyFigure(&#39;chicago-cuneiform-stone&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="chicago-cuneiform-stone-caption" class="caption-frame">
<p>Figure: Chicago Stone, side 2, recording sale of a number of fields,
probably from Isin, Early Dynastic Period, c. 2600 BC, black basalt</p>
</div>
</div>
<p>It was initially develoepd for people as a recordd of who owed what
to whom, expanding individuals’ capacity to remember. But over a five
hundred year period writing evolved to become a tool for literature as
well. More pithily put, writing was invented by accountants not poets
(see e.g. <a href="https://www.bbc.co.uk/news/business-39870485">this
piece by Tim Harford</a>).</p>
<p>In some respects today’s revolution is different, because it involves
also the creation of stories as well as their curation. But in some
fundamental ways we can see what we have produced as another tool for us
in the information revolution.</p>
<!-- Faster horse -->
<!-- Embodiment Factors -->
<h2 id="information-and-embodiment">Information and Embodiment</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-celsius.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/embodiment-factors-celsius.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="claude-shannon-figure" class="figure-frame">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ClaudeShannon_MFO3807.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Claude Shannon</em>
</center>
</div>
<div id="claude-shannon-magnify" class="magnify"
onclick="magnifyFigure(&#39;claude-shannon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="claude-shannon-caption" class="caption-frame">
<p>Figure: Claude Shannon (1916-2001)</p>
</div>
</div>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
<div id="embodiment-factors-table-magnify" class="magnify"
onclick="magnifyFigure(&#39;embodiment-factors-table&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="embodiment-factors-table-caption" class="caption-frame">
<p>Figure: Embodiment factors are the ratio between our ability to
compute and our ability to communicate. Relative to the machine we are
also locked in. In the table we represent embodiment as the length of
time it would take to communicate one second’s worth of computation. For
computers it is a matter of minutes, but for a human, it is a matter of
thousands of millions of years.</p>
</div>
</div>
<!-- Information Triangle -->
<h2 id="new-flow-of-information">New Flow of Information</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classically the field of statistics focused on mediating the
relationship between the machine and the human. Our limited bandwidth of
communication means we tend to over-interpret the limited information
that we are given, in the extreme we assign motives and desires to
inanimate objects (a process known as anthropomorphizing). Much of
mathematical statistics was developed to help temper this tendency and
understand when we are valid in drawing conclusions from data.</p>
<div class="figure">
<div id="new-flow-of-information-3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information003.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-3-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-3&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-3-caption" class="caption-frame">
<p>Figure: The trinity of human, data, and computer, and highlights the
modern phenomenon. The communication channel between computer and data
now has an extremely high bandwidth. The channel between human and
computer and the channel between data and human is narrow. New direction
of information flow, information is reaching us mediated by the
computer. The focus on classical statistics reflected the importance of
the direct communication between human and data. The modern challenges
of data science emerge when that relationship is being mediated by the
machine.</p>
</div>
</div>
<p>Data science brings new challenges. In particular, there is a very
large bandwidth connection between the machine and data. This means that
our relationship with data is now commonly being mediated by the
machine. Whether this is in the acquisition of new data, which now
happens by happenstance rather than with purpose, or the interpretation
of that data where we are increasingly relying on machines to summarize
what the data contains. This is leading to the emerging field of data
science, which must not only deal with the same challenges that
mathematical statistics faced in tempering our tendency to over
interpret data but must also deal with the possibility that the machine
has either inadvertently or maliciously misrepresented the underlying
data.</p>
<h2 id="societal-effects">Societal Effects</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/societal-effects.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/societal-effects.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We have already seen the effects of this changed dynamic in biology
and computational biology. Improved sensorics have led to the new
domains of transcriptomics, epigenomics, and ‘rich phenomics’ as well as
considerably augmenting our capabilities in genomics.</p>
<p>Biologists have had to become data-savvy, they require a rich
understanding of the available data resources and need to assimilate
existing data sets in their hypothesis generation as well as their
experimental design. Modern biology has become a far more quantitative
science, but the quantitativeness has required new methods developed in
the domains of <em>computational biology</em> and
<em>bioinformatics</em>.</p>
<p>There is also great promise for personalized health, but in health
the wide data-sharing that has underpinned success in the computational
biology community is much harder to carry out.</p>
<p>We can expect to see these phenomena reflected in wider society.
Particularly as we make use of more automated decision making based only
on data. This is leading to a requirement to better understand our own
subjective biases to ensure that the human to computer interface allows
domain experts to assimilate data driven conclusions in a well
calibrated manner. This is particularly important where medical
treatments are being prescribed. It also offers potential for different
kinds of medical intervention. More subtle interventions are possible
when the digital environment is able to respond to users in an bespoke
manner. This has particular implications for treatment of mental health
conditions.</p>
<p>The main phenomenon we see across the board is the shift in dynamic
from the direct pathway between human and data, as traditionally
mediated by classical statistics, to a new flow of information via the
computer. This change of dynamics gives us the modern and emerging
domain of <em>data science</em>, where the interactions between human
and data are mediated by the machine.</p>
<!-- AI Fallacy -->
<h1 id="the-great-ai-fallacy">The Great AI Fallacy</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/the-great-ai-fallacy.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/the-great-ai-fallacy.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>There is a lot of variation in the use of the term artificial
intelligence. I’m sometimes asked to define it, but depending on whether
you’re speaking to a member of the public, a fellow machine learning
researcher, or someone from the business community, the sense of the
term differs.</p>
<p>However, underlying its use I’ve detected one disturbing trend. A
trend I’m beginining to think of as “The Great AI Fallacy”.</p>
<p>The fallacy is associated with an implicit promise that is embedded
in many statements about Artificial Intelligence. Artificial
Intelligence, as it currently exists, is merely a form of automated
decision making. The implicit promise of Artificial Intelligence is that
it will be the first wave of automation where the machine adapts to the
human, rather than the human adapting to the machine.</p>
<p>How else can we explain the suspension of sensible business judgment
that is accompanying the hype surrounding AI?</p>
<p>This fallacy is particularly pernicious because there are serious
benefits to society in deploying this new wave of data-driven automated
decision making. But the AI Fallacy is causing us to suspend our
calibrated skepticism that is needed to deploy these systems safely and
efficiently.</p>
<p>The problem is compounded because many of the techniques that we’re
speaking of were originally developed in academic laboratories in
isolation from real-world deployment.</p>
<div class="figure">
<div id="jeeves-springtime-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jeeves_in_the_Springtime_01.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="jeeves-springtime-magnify" class="magnify"
onclick="magnifyFigure(&#39;jeeves-springtime&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="jeeves-springtime-caption" class="caption-frame">
<p>Figure: We seem to have fallen for a perspective on AI that suggests
it will adapt to our schedule, rather in the manner of a 1930s
manservant.</p>
</div>
</div>
<!-- Mathematical Statistics -->
<h2 id="lies-and-damned-lies">Lies and Damned Lies</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/lies-damned-lies.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/lies-damned-lies.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
<p>Arthur Balfour was quoting the lawyer James Munro<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
when he said that there three types of lies: lies, damned lies and
statistics in 1892. This is 20 years before the first academic
department of applied statistics was founded at UCL. If Balfour were
alive today, it is likely that he’d rephrase his quote:</p>
<blockquote>
<p>There are three types of lies, lies damned lies and <em>big
data</em>.</p>
</blockquote>
<p>Why? Because the challenges of understanding and interpreting big
data today are similar to those that Balfour (who was a Conservative
politician and statesman and would later become Prime Minister) faced in
governing an empire through statistics in the latter part of the 19th
century.</p>
<p>The quote lies, damned lies and statistics was also credited to
Benjamin Disraeli by Mark Twain in Twain’s autobiography.<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> It
characterizes the idea that statistic can be made to prove anything. But
Disraeli died in 1881 and Mark Twain died in 1910. The important
breakthrough in overcoming our tendency to over-interpet data came with
the formalization of the field through the development of
<em>mathematical statistics</em>.</p>
<p>Data has an elusive quality, it promises so much but can deliver
little, it can mislead and misrepresent. To harness it, it must be
tamed. In Balfour and Disraeli’s time during the second half of the 19th
century, numbers and data were being accumulated, the social sciences
were being developed. There was a large-scale collection of data for the
purposes of government.</p>
<p>The modern ‘big data era’ is on the verge of delivering the same
sense of frustration that Balfour experienced, the early promise of big
data as a panacea is evolving to demands for delivery. For me,
personally, peak-hype coincided with an email I received inviting
collaboration on a project to deploy “<em>Big Data</em> and <em>Internet
of Things</em> in an <em>Industry 4.0</em> environment”. Further
questioning revealed that the actual project was optimization of the
efficiency of a manufacturing production line, a far more tangible and
<em>realizable</em> goal.</p>
<p>The antidote to this verbiage is found in increasing awareness. When
dealing with data the first trap to avoid is the games of buzzword bingo
that we are wont to play. The first goal is to quantify what challenges
can be addressed and what techniques are required. Behind the hype
fundamentals are changing. The phenomenon is about the increasing access
we have to data. The way customers’ information is recorded and
processes are codified and digitized with little overhead. Internet of
things is about the increasing number of cheap sensors that can be
easily interconnected through our modern network structures. But
businesses are about making money, and these phenomena need to be recast
in those terms before their value can be realized.</p>
<p>For more thoughts on the challenges that statistics brings see
Chapter 8 of <span class="citation"
data-cites="Lawrence-atomic24">Lawrence (2024)</span>.</p>
<h2 id="mathematical-statistics"><em>Mathematical</em> Statistics</h2>
<p><a href="https://en.wikipedia.org/wiki/Karl_Pearson">Karl Pearson</a>
(1857-1936), <a
href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a>
(1890-1962) and others considered the question of what conclusions can
truly be drawn from data. Their mathematical studies act as a restraint
on our tendency to over-interpret and see patterns where there are none.
They introduced concepts such as randomized control trials that form a
mainstay of our decision making today, from government, to clinicians to
large scale A/B testing that determines the nature of the web interfaces
we interact with on social media and shopping.</p>
<div class="figure">
<div id="portrait-of-karl-pearson-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Portrait_of_Karl_Pearson.jpg" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="portrait-of-karl-pearson-magnify" class="magnify"
onclick="magnifyFigure(&#39;portrait-of-karl-pearson&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="portrait-of-karl-pearson-caption" class="caption-frame">
<p>Figure: Karl Pearson (1857-1936), one of the founders of Mathematical
Statistics.</p>
</div>
</div>
<p>Their movement did the most to put statistics to rights, to eradicate
the ‘damned lies’. It was known as <a
href="https://en.wikipedia.org/wiki/Mathematical_statistics">‘mathematical
statistics’</a>. Today I believe we should look to the emerging field of
<em>data science</em> to provide the same role. Data science is an
amalgam of statistics, data mining, computer systems, databases,
computation, machine learning and artificial intelligence. Spread across
these fields are the tools we need to realize data’s potential. For many
businesses this might be thought of as the challenge of ‘converting bits
into atoms’. Bits: the data stored on computer, atoms: the physical
manifestation of what we do; the transfer of goods, the delivery of
service. From fungible to tangible. When solving a challenge through
data there are a series of obstacles that need to be addressed.</p>
<p>Firstly, data awareness: what data you have and where its stored.
Sometimes this includes changing your conception of what data is and how
it can be obtained. From automated production lines to apps on employee
smart phones. Often data is locked away: manual logbooks, confidential
data, personal data. For increasing awareness an internal audit can
help. The website <a href="https://data.gov.uk/">data.gov.uk</a> hosts
data made available by the UK government. To create this website the
government’s departments went through an audit of what data they each
hold and what data they could make available. Similarly, within private
businesses this type of audit could be useful for understanding their
internal digital landscape: after all the key to any successful campaign
is a good map.</p>
<p>Secondly, availability. How well are the data sources interconnected?
How well curated are they? The curse of Disraeli was associated with
unreliable data and <em>unreliable statistics</em>. The
misrepresentations this leads to are worse than the absence of data as
they give a false sense of confidence to decision making. Understanding
how to avoid these pitfalls involves an improved sense of data and its
value, one that needs to permeate the organization.</p>
<p>The final challenge is analysis, the accumulation of the necessary
expertise to digest what the data tells us. Data requires
interpretation, and interpretation requires experience. Analysis is
providing a bottleneck due to a skill shortage, a skill shortage made
more acute by the fact that, ideally, analysis should be carried out by
individuals not only skilled in data science but also equipped with the
domain knowledge to understand the implications in a given application,
and to see opportunities for improvements in efficiency.</p>
<h2 id="mathematical-data-science">‘Mathematical Data Science’</h2>
<p>As a term ‘big data’ promises much and delivers little, to get true
value from data, it needs to be curated and evaluated. The three stages
of awareness, availability and analysis provide a broad framework
through which organizations should be assessing the potential in the
data they hold. Hand waving about big data solutions will not do, it
will only lead to self-deception. The castles we build on our data
landscapes must be based on firm foundations, process and scientific
analysis. If we do things right, those are the foundations that will be
provided by the new field of data science.</p>
<p>Today the statement “There are three types of lies: lies, damned lies
and ‘big data’” may be more apt. We are revisiting many of the mistakes
made in interpreting data from the 19th century. Big data is laid down
by happenstance, rather than actively collected with a particular
question in mind. That means it needs to be treated with care when
conclusions are being drawn. For data science to succeed it needs the
same form of rigor that Pearson and Fisher brought to statistics, a
“mathematical data science” is needed.</p>
<p>You can also check my blog post on <a
href="http://inverseprobability.com/2016/11/19/lies-damned-lies-big-data">Lies,
Damned Lies and Big Data</a>.</p>
<!-- Conversation -->
<h2 id="human-communication">Human Communication</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>For human conversation to work, we require an internal model of who
we are speaking to. We model each other, and combine our sense of who
they are, who they think we are, and what has been said. This is our
approach to dealing with the limited bandwidth connection we have.
Empathy and understanding of intent. Mental dispositional concepts are
used to augment our limited communication bandwidth.</p>
<p>Fritz Heider referred to the important point of a conversation as
being that they are happenings that are “<em>psychologically
represented</em> in each of the participants” (his emphasis) <span
class="citation" data-cites="Heider:interpersonal58">(Heider,
1958)</span>.</p>
<h3 id="bandwidth-constrained-conversations">Bandwidth Constrained
Conversations</h3>
<div class="figure">
<div id="anne-bob-conversation-civil-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation006.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-bob-conversation-civil-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-bob-conversation-civil&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-bob-conversation-civil-caption" class="caption-frame">
<p>Figure: Conversation relies on internal models of other
individuals.</p>
</div>
</div>
<div class="figure">
<div id="anne-bob-conversation-argument-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation007.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-bob-conversation-argument-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-bob-conversation-argument&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-bob-conversation-argument-caption" class="caption-frame">
<p>Figure: Misunderstanding of context and who we are talking to leads
to arguments.</p>
</div>
</div>
<p>Embodiment factors imply that, in our communication between humans,
what is <em>not</em> said is, perhaps, more important than what is said.
To communicate with each other we need to have a model of who each of us
are.</p>
<p>To aid this, in society, we are required to perform roles. Whether as
a parent, a teacher, an employee or a boss. Each of these roles requires
that we conform to certain standards of behaviour to facilitate
communication between ourselves.</p>
<p>Control of self is vitally important to these communications.</p>
<p>The high availability of data available to humans undermines
human-to-human communication channels by providing new routes to
undermining our control of self.</p>
<!-- Fritz Heider -->
<h3 id="heider-and-simmel-1944">Heider and Simmel (1944)</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/heider-simmel.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/heider-simmel.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="heider-simmel-shapes-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/8FIEZXMUM2I?start=7" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="heider-simmel-shapes-magnify" class="magnify"
onclick="magnifyFigure(&#39;heider-simmel-shapes&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="heider-simmel-shapes-caption" class="caption-frame">
<p>Figure: Fritz Heider and Marianne Simmel’s video of shapes from <span
class="citation" data-cites="Heider-experimental44">Heider and Simmel
(1944)</span>.</p>
</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Fritz_Heider">Fritz Heider</a>
and <a href="https://en.wikipedia.org/wiki/Marianne_Simmel">Marianne
Simmel</a>’s experiments with animated shapes from 1944 <span
class="citation" data-cites="Heider-experimental44">(Heider and Simmel,
1944)</span>. Our interpretation of these objects as showing motives and
even emotion is a combination of our desire for narrative, a need for
understanding of each other, and our ability to empathize. At one level,
these are crudely drawn objects, but in another way, the animator has
communicated a story through simple facets such as their relative
motions, their sizes and their actions. We apply our psychological
representations to these faceless shapes to interpret their actions.</p>
<p>See also a recent review paper on Human Cooperation by <span
class="citation" data-cites="Joseph-origins21">Henrich and Muthukrishna
(2021)</span>.</p>
<h3 id="a-six-word-novel">A Six Word Novel</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/baby-shoes.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/baby-shoes.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="classic-baby-shoes-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Classic_baby_shoes.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="classic-baby-shoes-magnify" class="magnify"
onclick="magnifyFigure(&#39;classic-baby-shoes&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="classic-baby-shoes-caption" class="caption-frame">
<p>Figure: Consider the six-word novel, apocryphally credited to Ernest
Hemingway, “For sale: baby shoes, never worn”. To understand what that
means to a human, you need a great deal of additional context. Context
that is not directly accessible to a machine that has not got both the
evolved and contextual understanding of our own condition to realize
both the implication of the advert and what that implication means
emotionally to the previous owner.</p>
</div>
</div>
<p>But this is a very different kind of intelligence than ours. A
computer cannot understand the depth of the Ernest Hemingway’s
apocryphal six-word novel: “For Sale, Baby Shoes, Never worn”, because
it isn’t equipped with that ability to model the complexity of humanity
that underlies that statement.</p>
<!-- Conversation LLM -->
<h2 id="computer-conversations">Computer Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-computer.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-computer.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-computer-conversation-6-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation006.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-computer-conversation-6-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-computer-conversation-6&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-computer-conversation-6-caption" class="caption-frame">
<p>Figure: Conversation relies on internal models of other
individuals.</p>
</div>
</div>
<div class="figure">
<div id="anne-computer-conversation-8-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation007.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-computer-conversation-8-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-computer-conversation-8&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-computer-conversation-8-caption" class="caption-frame">
<p>Figure: Misunderstanding of context and who we are talking to leads
to arguments.</p>
</div>
</div>
<p>Similarly, we find it difficult to comprehend how computers are
making decisions. Because they do so with more data than we can possibly
imagine.</p>
<p>In many respects, this is not a problem, it’s a good thing. Computers
and us are good at different things. But when we interact with a
computer, when it acts in a different way to us, we need to remember
why.</p>
<p>Just as the first step to getting along with other humans is
understanding other humans, so it needs to be with getting along with
our computers.</p>
<p>Embodiment factors explain why, at the same time, computers are so
impressive in simulating our weather, but so poor at predicting our
moods. Our complexity is greater than that of our weather, and each of
us is tuned to read and respond to one another.</p>
<p>Their intelligence is different. It is based on very large quantities
of data that we cannot absorb. Our computers don’t have a complex
internal model of who we are. They don’t understand the human condition.
They are not tuned to respond to us as we are to each other.</p>
<p>Embodiment factors encapsulate a profound thing about the nature of
humans. Our locked in intelligence means that we are striving to
communicate, so we put a lot of thought into what we’re communicating
with. And if we’re communicating with something complex, we naturally
anthropomorphize them.</p>
<p>We give our dogs, our cats, and our cars human motivations. We do the
same with our computers. We anthropomorphize them. We assume that they
have the same objectives as us and the same constraints. They don’t.</p>
<p>This means, that when we worry about artificial intelligence, we
worry about the wrong things. We fear computers that behave like more
powerful versions of ourselves that will struggle to outcompete us.</p>
<p>In reality, the challenge is that our computers cannot be human
enough. They cannot understand us with the depth we understand one
another. They drop below our cognitive radar and operate outside our
mental models.</p>
<p>The real danger is that computers don’t anthropomorphize. They’ll
make decisions in isolation from us without our supervision because they
can’t communicate truly and deeply with us.</p>
<h2 id="probability-conversations">Probability Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-probability-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-probability-conversation.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-probability-conversation-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-probability-conversation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-probability-conversation-caption" class="caption-frame">
<p>Figure: The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.</p>
</div>
</div>
<p>In practice we know that probabilities can be very unintuitive, for
example in court there is a fallacy known as the “prosecutor’s fallacy”
that confuses conditional probabilities. This can cause problems in jury
trials <span class="citation" data-cites="Thompson-juries89">(Thompson,
1989)</span>.</p>
<h2 id="networked-interactions">Networked Interactions</h2>
<p>Our modern society intertwines the machine with human interactions.
The key question is who has control over these interfaces between humans
and machines.</p>
<div class="figure">
<div id="human-computers-interacting-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-computers-interacting.svg" width="80%" style=" ">
</object>
</div>
<div id="human-computers-interacting-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-computers-interacting&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-computers-interacting-caption" class="caption-frame">
<p>Figure: Humans and computers interacting should be a major focus of
our research and engineering efforts.</p>
</div>
</div>
<p>So the real challenge that we face for society is understanding which
systemic interventions will encourage the right interactions between the
humans and the machine at all of these interfaces.</p>
<div class="figure">
<div id="human-culture-interacting-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-culture-interacting.svg" width="80%" style=" ">
</object>
</div>
<div id="human-culture-interacting-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-culture-interacting&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-culture-interacting-caption" class="caption-frame">
<p>Figure: Humans use culture, facts and ‘artefacts’ to communicate.</p>
</div>
</div>
<h2 id="number-theatre">Number Theatre</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/number-data-theatre.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/number-data-theatre.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Unfortunately, we don’t always have time to wait for this process to
converge to an answer we can all rely on before a decision is
required.</p>
<p>Not only can we be misled by data before a decision is made, but
sometimes we can be misled by data to justify the making of a decision.
David Spiegelhalter refers to the phenomenon of “Number Theatre” in a
conversation with Andrew Marr from May 2020 on the presentation of
data.</p>
<div class="figure">
<div id="david-andrew-marr-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/9388XmWIHXg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="david-andrew-marr-magnify" class="magnify"
onclick="magnifyFigure(&#39;david-andrew-marr&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="david-andrew-marr-caption" class="caption-frame">
<p>Figure: Professor Sir David Spiegelhalter on Andrew Marr on 10th May
2020 speaking about some of the challengers around data, data
presentation, and decision making in a pandemic. David mentions number
theatre at 9 minutes 10 seconds.</p>
</div>
</div>
<!--includebbcvideo{p08csg28}-->
<h2 id="data-theatre">Data Theatre</h2>
<p>Data Theatre exploits data inattention bias to present a particular
view on events that may misrepresents through selective presentation.
Statisticians are one of the few groups that are trained with a
sufficient degree of data skepticism. But it can also be combatted
through ensuring there are domain experts present, and that they can
speak freely.</p>
<div class="figure">
<div id="data-theatre-001-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//business/data-theatre001.svg" width="60%" style=" ">
</object>
</div>
<div id="data-theatre-001-magnify" class="magnify"
onclick="magnifyFigure(&#39;data-theatre-001&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="data-theatre-001-caption" class="caption-frame">
<p>Figure: The phenomenon of number theatre or <em>data theatre</em> was
described by David Spiegelhalter and is nicely summarized by Martin
Robbins in this sub-stack article <a
href="https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards"
class="uri">https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards</a>.</p>
</div>
</div>
<!--




## Complexity in Action

<div style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></div>



As an exercise in understanding complexity, watch the following video. You will see the basketball being bounced around, and the players moving. Your job is to count the passes of those dressed in white and ignore those of the individuals dressed in black.

<div class="figure">
<div class="figure-frame" id="monkey-business-figure">
<iframe width="600" height="450" src="https://www.youtube.com/embed/vJG698U2Mvo?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<div class="magnify" id="monkey-business-magnify" onclick="magnifyFigure('monkey-business')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="monkey-business-caption">
Figure: Daniel Simon's famous illusion "monkey business". Focus on the movement of the ball distracts the viewer from seeing other aspects of the image.
</div>
</div>

In a classic study @Simons-gorillas99 ask subjects to count the number of passes of the basketball between players on the team wearing white shirts. Fifty percent of the time, these subjects don't notice the gorilla moving across the scene.

The phenomenon of inattentional blindness is well known, e.g in their paper Simons and Charbris quote the Hungarian neurologist, Rezsö Bálint,

> It is a well-known phenomenon that we do not notice anything happening in our surroundings while being absorbed in the inspection of something; focusing our attention on a certain object may happen to such an extent that we cannot perceive other objects placed in the peripheral parts of our visual field, although the light rays they emit arrive completely at the visual sphere of the cerebral cortex.
>
> Rezsö Bálint 1907 (translated in Husain and Stein 1988, page 91)


When we combine the complexity of the world with our relatively low bandwidth for information, problems can arise. Our focus on what we perceive to be the most important problem can cause us to miss other (potentially vital) contextual information.

This phenomenon is known as selective attention or 'inattentional blindness'.

<div class="figure">
<div class="figure-frame" id="daniel-simons-monkey-business-figure">
<iframe width="600" height="450" src="https://www.youtube.com/embed/_oGAzq5wM_Q?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<div class="magnify" id="daniel-simons-monkey-business-magnify" onclick="magnifyFigure('daniel-simons-monkey-business')"><img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex"></div>
<div class="caption-frame" id="daniel-simons-monkey-business-caption">
Figure: For a longer talk on inattentional bias from Daniel Simons see this video.
</div>
</div>


-->
<!--include{_data-science/includes/data-selection-attention-bias.md}-->
<h2 id="llm-conversations">LLM Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-llm.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-llm.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-llm-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-llm-conversation.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-llm-conversation-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-llm-conversation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-llm-conversation-caption" class="caption-frame">
<p>Figure: The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.</p>
</div>
</div>
<div class="figure">
<div id="ai-for-data-analytics-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/0sJjdxn5kcI?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="ai-for-data-analytics-magnify" class="magnify"
onclick="magnifyFigure(&#39;ai-for-data-analytics&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ai-for-data-analytics-caption" class="caption-frame">
<p>Figure: The Inner Monologue paper suggests using LLMs for robotic
planning <span class="citation" data-cites="Huang-inner22">(Huang et
al., 2023)</span>.</p>
</div>
</div>
<p>By interacting directly with machines that have an understanding of
human cultural context, it should be possible to share the nature of
uncertainty in the same way humans do. See for example the paper <a
href="https://innermonologue.github.io/">Inner Monologue: Embodied
Reasoning through Planning</a> <span class="citation"
data-cites="Huang-inner22">Huang et al. (2023)</span>.</p>
<h2 id="the-moniac">The MONIAC</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><a href="https://en.wikipedia.org/wiki/MONIAC">The MONIAC</a> was an
analogue computer designed to simulate the UK economy. Analogue
comptuers work through analogy, the analogy in the MONIAC is that both
money and water flow. The MONIAC exploits this through a system of
tanks, pipes, valves and floats that represent the flow of money through
the UK economy. Water flowed from the treasury tank at the top of the
model to other tanks representing government spending, such as health
and education. The machine was initially designed for teaching support
but was also found to be a useful economic simulator. Several were built
and today you can see the original at Leeds Business School, there is
also one in the London Science Museum and one <a
href="https://www.econ.cam.ac.uk/economics-alumni/drip-down-economics-phillips-machine">in
the Unisversity of Cambridge’s economics faculty</a>.</p>
<div class="figure">
<div id="the-moniac-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-moniac-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-moniac&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-moniac-caption" class="caption-frame">
<p>Figure: Bill Phillips and his MONIAC (completed in 1949). The machine
is an analogue computer designed to simulate the workings of the UK
economy.</p>
</div>
</div>
<h2 id="donald-mackay">Donald MacKay</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/donald-mackay-brain.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/donald-mackay-brain.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="donald-maccrimmon-mackay-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//people/DonaldMacKay1952.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="donald-maccrimmon-mackay-magnify" class="magnify"
onclick="magnifyFigure(&#39;donald-maccrimmon-mackay&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="donald-maccrimmon-mackay-caption" class="caption-frame">
<p>Figure: Donald M. MacKay (1922-1987), a physicist who was an early
member of the cybernetics community and member of the Ratio Club.</p>
</div>
</div>
<p>Donald MacKay was a physicist who worked on naval gun targetting
during the second world war. The challenge with gun targetting for ships
is that both the target and the gun platform are moving. The challenge
was tackled using analogue computers, for example in the US the <a
href="https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer">Mark I
fire control computer</a> which was a mechanical computer. MacKay worked
on radar systems for gun laying, here the velocity and distance of the
target could be assessed through radar and an mechanical electrical
analogue computer.</p>
<h2 id="fire-control-systems">Fire Control Systems</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/fire-control-systems.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/fire-control-systems.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Naval gunnery systems deal with targeting guns while taking into
account movement of ships. The Royal Navy’s Gunnery Pocket Book <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span> gives details of one system for gun laying.</p>
<p>Like many challenges we face today, in the second world war, fire
control was handled by a hybrid system of humans and computers. This
means deploying human beings for the tasks that they can manage, and
machines for the tasks that are better performed by a machine. This
leads to a division of labour between the machine and the human that can
still be found in our modern digital ecosystems.</p>
<div class="figure">
<div id="low-angle-fire-control-team-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/low-angle-fire-control-team.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="low-angle-fire-control-team-magnify" class="magnify"
onclick="magnifyFigure(&#39;low-angle-fire-control-team&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="low-angle-fire-control-team-caption" class="caption-frame">
<p>Figure: The fire control computer set at the centre of a system of
observation and tracking <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.</p>
</div>
</div>
<p>As analogue computers, fire control computers from the second world
war would contain components that directly represented the different
variables that were important in the problem to be solved, such as the
inclination between two ships.</p>
<div class="figure">
<div id="the-measurement-of-inclination-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-measurement-of-inclination.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-measurement-of-inclination-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-measurement-of-inclination&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-measurement-of-inclination-caption" class="caption-frame">
<p>Figure: Measuring inclination between two ships <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span>. Sophisticated fire control computers allowed the ship to
continue to fire while under maneuvers.</p>
</div>
</div>
<p>The fire control systems were electro-mechanical analogue computers
that represented the “state variables” of interest, such as inclination
and ship speed with gears and cams within the machine.</p>
<div class="figure">
<div id="typical-modern-fire-control-table-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/typical-modern-fire-control-table.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="typical-modern-fire-control-table-magnify" class="magnify"
onclick="magnifyFigure(&#39;typical-modern-fire-control-table&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="typical-modern-fire-control-table-caption"
class="caption-frame">
<p>Figure: A second world war gun computer’s control table <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span>.</p>
</div>
</div>
<p>For more details on fire control computers, you can watch a 1953 film
on the the US the <a
href="https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer">Mark
IA fire control computer</a> from Periscope Film.</p>
<div class="figure">
<div id="us-navy-training-film-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/gwf5mAlI7Ug?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="us-navy-training-film-magnify" class="magnify"
onclick="magnifyFigure(&#39;us-navy-training-film&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="us-navy-training-film-caption" class="caption-frame">
<p>Figure: U.S. Navy training film MN-6783a. Basic Mechanisms of Fire
Control Computers. Mechanical Computer Instructional Film 27794 (1953)
for the Mk 1A Fire Control Computer.</p>
</div>
</div>
<h2 id="behind-the-eye">Behind the Eye</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/behind-the-eye.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/behind-the-eye.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="behind-the-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/behind-the-eye.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="behind-the-eye-magnify" class="magnify"
onclick="magnifyFigure(&#39;behind-the-eye&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="behind-the-eye-caption" class="caption-frame">
<p>Figure: <a
href="https://www.amazon.co.uk/Behind-Eye-Gifford-Lectures-MACKAY/dp/0631173323">Behind
the Eye</a> <span class="citation" data-cites="Mackay-behind91">(MacKay,
1991)</span> summarises MacKay’s Gifford Lectures, where MacKay uses the
operation of the eye as a window on the operation of the brain.</p>
</div>
</div>
<p>Donald MacKay was at King’s College for his PhD. He was just down the
road from Bill Phillips at LSE who was building the MONIAC. He was part
of the Ratio Club. A group of early career scientists who were
interested in communication and control in animals and humans, or more
specifically they were interested in computers and brains. The were part
of an international movement known as cybernetics.</p>
<p>Donald MacKay wrote of the influence that his own work on radar had
on his interest in the brain.</p>
<blockquote>
<p>… during the war I had worked on the theory of automated and
electronic computing and on the theory of information, all of which are
highly relevant to such things as automatic pilots and automatic gun
direction. I found myself grappling with problems in the design of
artificial sense organs for naval gun-directors and with the principles
on which electronic circuits could be used to simulate situations in the
external world so as to provide goal-directed guidance for ships,
aircraft, missiles and the like.</p>
</blockquote>
<blockquote>
<p>Later in the 1940’s, when I was doing my Ph.D. work, there was much
talk of the brain as a computer and of the early digital computers that
were just making the headlines as “electronic brains.” As an analogue
computer man I felt strongly convinced that the brain, whatever it was,
was not a digital computer. I didn’t think it was an analogue computer
either in the conventional sense.</p>
</blockquote>
<blockquote>
<p>But this naturally rubbed under my skin the question: well, if it is
not either of these, what kind of system is it? Is there any way of
following through the kind of analysis that is appropriate to their
artificial automata so as to understand better the kind of system the
human brain is? That was the beginning of my slippery slope into brain
research.</p>
<p><em>Behind the Eye</em> pg 40. Edited version of the 1986 Gifford
Lectures given by Donald M. MacKay and edited by Valerie MacKay</p>
</blockquote>
<p>Importantly, MacKay distinguishes between the <em>analogue</em>
computer and the <em>digital</em> computer. As he mentions, his
experience was with analogue machines. An analogue machine is
<em>literally</em> an analogue. The radar systems that Wiener and MacKay
both worked on were made up of electronic components such as resistors,
capacitors, inductors and/or mechanical components such as cams and
gears. Together these components could represent a physical system, such
as an anti-aircraft gun and a plane. The design of the analogue computer
required the engineer to simulate the real world in analogue
electronics, using dualities that exist between e.g. mechanical circuits
(mass, spring, damper) and electronic circuits (inductor, resistor,
capacitor). The analogy between mass and a damper, between spring and a
resistor and between capacitor and a damper works because the underlying
mathematics is approximated with the same linear system: a second order
differential equation. This mathematical analogy allowed the designer to
map from the real world, through mathematics, to a virtual world where
the components reflected the real world through analogy.</p>
<h2 id="human-analogue-machine">Human Analogue Machine</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/human-analogue-machines-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/human-analogue-machines-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The machine learning systems we have built today that can reconstruct
human text, or human classification of images, necessarily must have
some aspects to them that are analagous to our understanding. As MacKay
suggests the brain is neither a digital or an analogue computer, and the
same can be said of the modern neural network systems that are being
tagged as “artificial intelligence”.</p>
<p>I believe a better term for them is “human-analogue machines”,
because what we have built is not a system that can make intelligent
decisions from first principles (a rational approach) but one that
observes how humans have made decisions through our data and
reconstructs that process. Machine learning is more empiricist than
rational, but now we n empirical approach that distils our evolved
intelligence.</p>
<div class="figure">
<div id="human-analogue-machine-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/human-analogue-machine.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="human-analogue-machine-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-analogue-machine&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-analogue-machine-caption" class="caption-frame">
<p>Figure: The human analogue machine creates a feature space which is
analagous to that we use to reason, one way of doing this is to have a
machine attempt to compress all human generated text in an
auto-regressive manner.</p>
</div>
</div>
<p>The perils of developing this capability include counterfeit people,
a notion that the philosopher <a
href="https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/">Daniel
Dennett has described in <em>The Atlantic</em></a>. This is where
computers can represent themselves as human and fool people into doing
things on that basis.</p>
<h2 id="intellectual-debt">Intellectual Debt</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/intellectual-debt-blog-post.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/intellectual-debt-blog-post.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="intellectual-debt-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/2020-02-12-intellectual-debt.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="intellectual-debt-magnify" class="magnify"
onclick="magnifyFigure(&#39;intellectual-debt&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intellectual-debt-caption" class="caption-frame">
<p>Figure: Jonathan Zittrain’s term to describe the challenges of
explanation that come with AI is Intellectual Debt.</p>
</div>
</div>
<p>In the context of machine learning and complex systems, Jonathan
Zittrain has coined the term <a
href="https://medium.com/berkman-klein-center/from-technical-debt-to-intellectual-debt-in-ai-e05ac56a502c">“Intellectual
Debt”</a> to describe the challenge of understanding what you’ve
created. In <a
href="https://mlatcl.github.io/projects/data-oriented-architectures-for-ai-based-systems.html">the
ML@CL group we’ve been foucssing on developing the notion of a
<em>data-oriented architecture</em></a> to deal with intellectual debt
<span class="citation" data-cites="Cabrera-realworld23">(Cabrera et al.,
2023)</span>.</p>
<p>Zittrain points out the challenge around the lack of interpretability
of individual ML models as the origin of intellectual debt. In machine
learning I refer to work in this area as fairness, interpretability and
transparency or FIT models. To an extent I agree with Zittrain, but if
we understand the context and purpose of the decision making, I believe
this is readily put right by the correct monitoring and retraining
regime around the model. A concept I refer to as “progression testing”.
Indeed, the best teams do this at the moment, and their failure to do it
feels more of a matter of technical debt rather than intellectual,
because arguably it is a maintenance task rather than an explanation
task. After all, we have good statistical tools for interpreting
individual models and decisions when we have the context. We can
linearise around the operating point, we can perform counterfactual
tests on the model. We can build empirical validation sets that explore
fairness or accuracy of the model.</p>
<p>But if we can avoid the pitfalls of counterfeit people, this also
offers us an opportunity to <em>psychologically represent</em> <span
class="citation" data-cites="Heider:interpersonal58">(Heider,
1958)</span> the machine in a manner where humans can communicate
without special training. This in turn offers the opportunity to
overcome the challenge of <em>intellectual debt</em>.</p>
<p>Despite the lack of interpretability of machine learning models, they
allow us access to what the machine is doing in a way that bypasses many
of the traditional techniques developed in statistics. But understanding
this new route for access is a major new challenge.</p>
<h2 id="ham">HAM</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="new-flow-of-information-4-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information004.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-4-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-4-caption" class="caption-frame">
<p>Figure: The trinity of human, data, and computer, and highlights the
modern phenomenon. The communication channel between computer and data
now has an extremely high bandwidth. The channel between human and
computer and the channel between data and human is narrow. New direction
of information flow, information is reaching us mediated by the
computer. The focus on classical statistics reflected the importance of
the direct communication between human and data. The modern challenges
of data science emerge when that relationship is being mediated by the
machine.</p>
</div>
</div>
<div class="figure">
<div id="new-flow-of-information-ham-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information-ham.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-ham-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-ham&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-ham-caption" class="caption-frame">
<p>Figure: The HAM now sits between us and the traditional digital
computer.</p>
</div>
</div>
<h2 id="richard-feynmann-on-doubt">Richard Feynmann on Doubt</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/richard-feynmann-doubt.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/richard-feynmann-doubt.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<blockquote>
<p>One thing is I can live with is doubt, and uncertainty and not
knowing. I think it’s much more interesting to live with not knowing
than to have an answer that might be wrong.</p>
<p>Richard P. Feynmann in the <em>The Pleasure of Finding Things
Out</em> 1981.</p>
</blockquote>
<h2 id="hydrodynamica">Hydrodynamica</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/daniel-bernoulli-hydrodynamica.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/daniel-bernoulli-hydrodynamica.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>When Laplace spoke of the curve of a simple molecule of air, he may
well have been thinking of Daniel Bernoulli (1700-1782). Daniel
Bernoulli was one name in a prodigious family. His father and brother
were both mathematicians. Daniel’s main work was known as
<em>Hydrodynamica</em>.</p>
<div class="figure">
<div id="hydrodynamica-danielis-bernoulli-figure" class="figure-frame">
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=3yRVAAAAcAAJ&amp;pg=PP7&amp;output=embed" width="700" height="500">
</iframe>
</div>
<div id="hydrodynamica-danielis-bernoulli-magnify" class="magnify"
onclick="magnifyFigure(&#39;hydrodynamica-danielis-bernoulli&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="hydrodynamica-danielis-bernoulli-caption"
class="caption-frame">
<p>Figure: Daniel Bernoulli’s <em>Hydrodynamica</em> published in 1738.
It was one of the first works to use the idea of conservation of energy.
It used Newton’s laws to predict the behaviour of gases.</p>
</div>
</div>
<p>Daniel Bernoulli described a kinetic theory of gases, but it wasn’t
until 170 years later when these ideas were verified after Einstein had
proposed a model of Brownian motion which was experimentally verified by
Jean Baptiste Perrin.</p>
<div class="figure">
<div id="-figure" class="figure-frame">
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=3yRVAAAAcAAJ&amp;pg=PA200&amp;output=embed" width="700" height="500">
</iframe>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: Daniel Bernoulli’s chapter on the kinetic theory of gases,
for a review on the context of this chapter see <span class="citation"
data-cites="Mikhailov:hydrodynamica05">Mikhailov (n.d.)</span>. For 1738
this is extraordinary thinking. The notion of kinetic theory of gases
wouldn’t become fully accepted in Physics until 1908 when a model of
Einstein’s was verified by Jean Baptiste Perrin.</p>
</div>
</div>
<h2 id="entropy-billiards">Entropy Billiards</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/entropy-billiards.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/entropy-billiards.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="entropy-billiards-js-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="multiball-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left ">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="multiball-entropy">
</output>
</div>
<div id="multiball-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="multiball-newball" style="text-align:right">
New Ball
</button>
<button id="multiball-pause" style="text-align:right">
Pause
</button>
<button id="multiball-skip" style="text-align:right">
Skip 1000s
</button>
<button id="multiball-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/ballworld.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/multiball.js"></script>
</div>
<div id="entropy-billiards-js-magnify" class="magnify"
onclick="magnifyFigure(&#39;entropy-billiards-js&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="entropy-billiards-js-caption" class="caption-frame">
<p>Figure: Bernoulli’s simple kinetic models of gases assume that the
molecules of air operate like billiard balls.</p>
</div>
</div>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.random.randn(<span class="dv">10000</span>, <span class="dv">1</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> [<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">200</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)<span class="op">*</span>np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">*</span>x)</span></code></pre></div>
<p>Another important figure for Cambridge was the first to derive the
probability distribution that results from small balls banging together
in this manner. In doing so, James Clerk Maxwell founded the field of
statistical physics.</p>
<div class="figure">
<div id="gaussian-histogram-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ml/gaussian-histogram.svg" width="80%" style=" ">
</object>
</div>
<div id="gaussian-histogram-magnify" class="magnify"
onclick="magnifyFigure(&#39;gaussian-histogram&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-histogram-caption" class="caption-frame">
<p>Figure: James Clerk Maxwell 1831-1879 Derived distribution of
velocities of particles in an ideal gas (elastic fluid).</p>
</div>
</div>
<div class="figure">
<div id="maxwell-boltzmann-gibbs-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="maxwell-boltzmann-gibbs-magnify" class="magnify"
onclick="magnifyFigure(&#39;maxwell-boltzmann-gibbs&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="maxwell-boltzmann-gibbs-caption" class="caption-frame">
<p>Figure: James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906)
Josiah Willard Gibbs (1839-1903)</p>
</div>
</div>
<p>Many of the ideas of early statistical physicists were rejected by a
cadre of physicists who didn’t believe in the notion of a molecule. The
stress of trying to have his ideas established caused Boltzmann to
commit suicide in 1906, only two years before the same ideas became
widely accepted.</p>
<div class="figure">
<div id="boltzmann-warmetheorie-figure" class="figure-frame">
<iframe frameborder="0" scrolling="no" style="border:0px" src="https://books.google.co.uk/books?id=Vuk5AQAAMAAJ&amp;pg=PA373&amp;output=embed" width="700" height="500">
</iframe>
</div>
<div id="boltzmann-warmetheorie-magnify" class="magnify"
onclick="magnifyFigure(&#39;boltzmann-warmetheorie&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="boltzmann-warmetheorie-caption" class="caption-frame">
<p>Figure: Boltzmann’s paper <span class="citation"
data-cites="Boltzmann-warmetheorie77">Boltzmann (n.d.)</span> which
introduced the relationship between entropy and probability. A
translation with notes is available in <span class="citation"
data-cites="Kim-translation15">Sharp and Matschinsky (2015)</span>.</p>
</div>
</div>
<p>The important point about the uncertainty being represented here is
that it is not genuine stochasticity, it is a lack of knowledge about
the system. The techniques proposed by Maxwell, Boltzmann and Gibbs
allow us to exactly represent the state of the system through a set of
parameters that represent the sufficient statistics of the physical
system. We know these values as the volume, temperature, and pressure.
The challenge for us, when approximating the physical world with the
techniques we will use is that we will have to sit somewhere between the
deterministic and purely stochastic worlds that these different
scientists described.</p>
<p>One ongoing characteristic of people who study probability and
uncertainty is the confidence with which they hold opinions about it.
Another leader of the Cavendish laboratory expressed his support of the
second law of thermodynamics (which can be proven through the work of
Gibbs/Boltzmann) with an emphatic statement at the beginning of his
book.</p>
<div class="figure">
<div id="eddington-book-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/arthur-stanley-eddington.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_7.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eddington-book-magnify" class="magnify"
onclick="magnifyFigure(&#39;eddington-book&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eddington-book-caption" class="caption-frame">
<p>Figure: Eddington’s book on the Nature of the Physical World <span
class="citation" data-cites="Eddington:nature29">(Eddington,
1929)</span></p>
</div>
</div>
<p>The same Eddington is also famous for dismissing the ideas of a young
Chandrasekhar who had come to Cambridge to study in the Cavendish lab.
Chandrasekhar demonstrated the limit at which a star would collapse
under its own weight to a singularity, but when he presented the work to
Eddington, he was dismissive suggesting that there “must be some natural
law that prevents this abomination from happening”.</p>
<div class="figure">
<div id="physical-world-chandra-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/ChandraNobel.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="physical-world-chandra-magnify" class="magnify"
onclick="magnifyFigure(&#39;physical-world-chandra&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="physical-world-chandra-caption" class="caption-frame">
<p>Figure: Chandrasekhar (1910-1995) derived the limit at which a star
collapses in on itself. Eddington’s confidence in the 2nd law may have
been what drove him to dismiss Chandrasekhar’s ideas, humiliating a
young scientist who would later receive a Nobel prize for the work.</p>
</div>
</div>
<div class="figure">
<div id="deepest-humiliation-eddington-cropped-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deepest-humiliation-eddington-cropped-magnify" class="magnify"
onclick="magnifyFigure(&#39;deepest-humiliation-eddington-cropped&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deepest-humiliation-eddington-cropped-caption"
class="caption-frame">
<p>Figure: Eddington makes his feelings about the primacy of the second
law clear. This primacy is perhaps because the second law can be
demonstrated mathematically, building on the work of Maxwell, Gibbs and
Boltzmann. <span class="citation"
data-cites="Eddington:nature29">Eddington (1929)</span></p>
</div>
</div>
<p>Presumably he meant that the creation of a black hole seemed to
transgress the second law of thermodynamics, although later Hawking was
able to show that blackholes do evaporate, but the time scales at which
this evaporation occurs is many orders of magnitude slower than other
processes in the universe.</p>
<h2 id="brownian-motion-and-wiener">Brownian Motion and Wiener</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/brownian-wiener.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/brownian-wiener.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Robert Brown was a botanist who was studying plant pollen in 1827
when he noticed a trembling motion of very small particles contained
within cavities within the pollen. He worked hard to eliminate the
potential source of the movement by exploring other materials where he
found it to be continuously present. Thus, the movement was not
associated, as he originally thought, with life.</p>
<p>In 1905 Albert Einstein produced the first mathematical explanation
of the phenomenon. This can be seen as our first model of a ‘curve of a
simple molecule of air’. To model the phenomenon Einstein introduced
stochasticity to a differential equation. The particles were being
peppered with high-speed water molecules, that was triggering the
motion. Einstein modelled this as a stochastic process.</p>
<div class="figure">
<div id="albert-einstein-photo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="albert-einstein-photo-magnify" class="magnify"
onclick="magnifyFigure(&#39;albert-einstein-photo&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="albert-einstein-photo-caption" class="caption-frame">
<p>Figure: Albert Einstein’s 1905 paper on Brownian motion introduced
stochastic differential equations which can be used to model the ‘curve
of a simple molecule of air’.</p>
</div>
</div>
<p>Norbert Wiener was a child prodigy, whose father had schooled him in
philosophy. He was keen to have his son work with the leading
philosophers of the age, so at the age of 18 Wiener arrived in Cambridge
(already with a PhD). He was despatched to study with Bertrand Russell
but Wiener and Russell didn’t get along. Wiener wasn’t persuaded by
Russell’s ideas for theories of knowledge through logic. He was more
aligned with Laplace and his desire for a theory of ignorance. In is
autobiography he relates it as the first thing he could see his father
was proud of (at around the age of 10 or 11) <span class="citation"
data-cites="Wiener-exprodigy53">(Wiener, 1953)</span>.</p>
<div class="figure">
<div id="russell-wiener-russell-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="85%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="russell-wiener-russell-magnify" class="magnify"
onclick="magnifyFigure(&#39;russell-wiener-russell&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="russell-wiener-russell-caption" class="caption-frame">
<p>Figure: Bertrand Russell (1872-1970), Albert Einstein (1879-1955),
Norbert Wiener, (1894-1964)</p>
</div>
</div>
<p>But Russell (despite also not getting along well with Wiener)
introduced Wiener to Einstein’s works, and Wiener also met G. H. Hardy.
He left Cambridge for Göttingen where he studied with Hilbert. He
developed the underlying mathematics for proving the existence of the
solutions to Einstein’s equation, which are now known as Wiener
processes.</p>
<div class="figure">
<div id="brownian-motion-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="brownian-motion-magnify" class="magnify"
onclick="magnifyFigure(&#39;brownian-motion&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="brownian-motion-caption" class="caption-frame">
<p>Figure: Brownian motion of a large particle in a group of smaller
particles. The movement is known as a <em>Wiener process</em> after
Norbert Wiener.</p>
</div>
</div>
<div class="figure">
<div id="norbert-wiener-yellow-peril-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/wiener-yellow-peril.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="norbert-wiener-yellow-peril-magnify" class="magnify"
onclick="magnifyFigure(&#39;norbert-wiener-yellow-peril&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="norbert-wiener-yellow-peril-caption" class="caption-frame">
<p>Figure: Norbert Wiener (1894 - 1964). Founder of cybernetics and the
information era. He used Gibbs’s ideas to develop a “theory of
ignorance” that he deployed in early communication. On the right is
Wiener’s wartime report that used stochastic processes in forecasting
with applications in radar control (image from <span class="citation"
data-cites="Coales-yellow14">Coales and Kane (2014)</span>).</p>
</div>
</div>
<p>Wiener himself used the processes in his work. He was focused on
mathematical theories of communication. Between the world wars he was
based at Massachusetts Institute of Technology where the burgeoning
theory of electrical engineering was emerging, with a particular focus
on communication lines. Winer developed theories of communication that
used Gibbs’s entropy to encode information. He also used the ideas
behind the Wiener process for developing tracking methods for radar
systems in the second world war. These processes are what we know of now
as Gaussian processes (<span class="citation"
data-cites="Wiener:yellow49">Wiener (1949)</span>).</p>
<!-- Lecture 2 -->
<h2 id="kappenball">Kappenball</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_physics/includes/kappenball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_physics/includes/kappenball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="kappen-ball-figure" class="figure-frame">
<div>
<div style="width:900px;text-align:center;display:inline">
<span style="float:left;">Score:
<output id="kappenball-score">
</output>
</span> <span style="float:right;">Energy:
<output id="kappenball-energy">
</output>
</span>
<div style="clear: both;">

</div>
</div>
<canvas id="kappenball-canvas" width="900" height="500" style="border:1px solid black;display:inline;text-align:center ">
</canvas>
<div>
<input type="range" min="0" max="100" value="0" class="slider" id="kappenball-stochasticity" style="width:900px;"/>
</div>
<div>
<button id="kappenball-newball" style="text-align:right">
New Ball
</button>
<button id="kappenball-pause" style="text-align:right">
Pause
</button>
</div>
<output id="kappenball-count">
</output>
<script src="https://inverseprobability.com/talks/scripts//ballworld/kappenball.js"></script>
</div>
</div>
<div id="kappen-ball-magnify" class="magnify"
onclick="magnifyFigure(&#39;kappen-ball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kappen-ball-caption" class="caption-frame">
<p>Figure: Kappen Ball</p>
</div>
</div>
<p>If you want to complete a task, should you do it now or should you
put it off until tomorrow? Despite being told to not delay tasks, many
of us are deadline driven. Why is this?</p>
<p>Kappenball is a simple game that illustrates that this behaviour can
be optimal. It is inspired by an example in stochastic optimal control
by <a href="https://www.snn.ru.nl/~bertk/">Bert Kappen</a>. The game is
as follows: you need to place a falling balloon into one of two holes,
but if the balloon misses the holes it will pop on pins placed in the
ground. In ‘deterministic mode’, the balloon falls straight towards the
ground and the game is easy. You simply choose which hole to place the
ball in, and you can start to place it there as soon as the ball appears
at the top of the screen. The game becomes more interesting as you
increase the uncertainty. In Kappenball, the uncertainty takes the form
of the balloon being blown left and right as it falls. This movement
means that it is not sensible to decide early on which hole to place the
balloon in. A better strategy is to wait and see which hole the ball
falls towards. You can then place it in that hole using less energy than
in deterministic mode. Sometimes, the ball even falls into the hole on
its own, and you don’t have to expend any energy, but it requires some
skill to judge when you need to intervene. For this system Bert Kappen
has shown mathematically that the best solution is to wait until the
ball is close to the hole before you push it in. In other words, you
should be deadline driven.</p>
<p>In fact, it seems here uncertainty is a good thing, because on
average you’ll get the ball into the hole with less energy (by playing
intelligently, and being deadline driven!) than you do with
`deterministic mode’. It requires some skill to do this, more than the
deterministic system, but by using your resources intelligently you can
get more out of the system. However, if the uncertainty increases too
much then regardless of your skill, you can’t control the ball at
all.</p>
<p>This simple game explains many of the behaviours we exhibit in real
life. If a system is completely deterministic, then we can make a
decision early on and be sure that the ball will ‘drop in the hole’.
However, if there is uncertainty in a system, it can make sense to delay
our decision making until we’ve seen how events ‘pan out’. Be careful
though, as we also see that when the uncertainty is large, if you don’t
have the resources or the skill to be deadline-driven the uncertainty
can overwhelm you and events can quickly move beyond our control.</p>
<h2 id="game-of-life">Game of Life</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/game-of-life.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/game-of-life.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><a href="https://en.wikipedia.org/wiki/John_Horton_Conway">John
Horton Conway</a> was a mathematician who developed a game known as the
Game of Life. He died in April 2020, but since he invented the game, he
was in effect ‘god’ for this game. But as we will see, just inventing
the rules doesn’t give you omniscience in the game.</p>
<p>The Game of Life is played on a grid of squares, or pixels. Each
pixel is either on or off. The game has no players, but a set of simple
rules that are followed at each turn the rules are.</p>
<h2 id="life-rules">Life Rules</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-rules.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-rules.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>John Conway’s game of life is a cellular automaton where the cells
obey three very simple rules. The cells live on a rectangular grid, so
that each cell has 8 possible neighbors.</p>
<div class="figure">
<div id="life-rules-loneliness-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>loneliness</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="life-rules-loneliness-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-rules-loneliness&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-rules-loneliness-caption" class="caption-frame">
<p>Figure: ‘Death’ through loneliness in Conway’s game of life. If a
cell is surrounded by less than three cells, it ‘dies’ through
loneliness.</p>
</div>
</div>
<p>The game proceeds in turns, and at each location in the grid is
either alive or dead. Each turn, a cell counts its neighbors. If there
are two or fewer neighbors, the cell ‘dies’ of ‘loneliness’.</p>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>overcrowding</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="life-rules-crowding-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-rules-crowding&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-rules-crowding-caption" class="caption-frame">
<p>Figure: ‘Death’ through overpopulation in Conway’s game of life. If a
cell is surrounded by more than three cells, it ‘dies’ through
loneliness.</p>
</div>
</div>
<p>If there are four or more neighbors, the cell ‘dies’ from
‘overcrowding’. If there are three neighbors, the cell persists, or if
it is currently dead, a new cell is born.</p>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>birth</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
<div id="life-rules-crowding-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-rules-crowding&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-rules-crowding-caption" class="caption-frame">
<p>Figure: Birth in Conway’s life. Any position surrounded by precisely
three live cells will give birth to a new cell at the next turn.</p>
</div>
</div>
<p>And that’s it. Those are the simple ‘physical laws’ for Conway’s
game.</p>
<p>The game leads to patterns emerging, some of these patterns are
static, but some oscillate in place, with varying periods. Others
oscillate, but when they complete their cycle they’ve translated to a
new location, in other words they move. In Life the former are known as
<a href="https://conwaylife.com/wiki/Oscillator">oscillators</a> and the
latter as <a
href="https://conwaylife.com/wiki/Spaceship">spaceships</a>.</p>
<h2 id="loafers-and-gliders">Loafers and Gliders</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-glider-loafer-conway.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/life-glider-loafer-conway.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>John Horton Conway, as the creator of the game of life, could be seen
somehow as the god of this small universe. He created the rules. The
rules are so simple that in many senses he, and we, are all-knowing in
this space. But despite our knowledge, this world can still ‘surprise’
us. From the simple rules, emergent patterns of behaviour arise. These
include static patterns that don’t change from one turn to the next.
They also include, oscillators, that pulse between different forms
across different periods of time. A particular form of oscillator is
known as a ‘spaceship’, this is one that moves across the board as the
game evolves. One of the simplest and earliest spaceships to be
discovered is known as the glider.</p>
<div class="figure">
<div id="glider-loafer-conway-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Glider (1969)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Glider.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="glider-loafer-conway-magnify" class="magnify"
onclick="magnifyFigure(&#39;glider-loafer-conway&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="glider-loafer-conway-caption" class="caption-frame">
<p>Figure: <em>Left</em> A Glider pattern discovered 1969 by Richard K.
Guy. <em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020). The glider is an oscillator that moves diagonally after
creation. From the simple rules of Life it’s not obvious that such an
object does exist, until you do the necessary computation.</p>
</div>
</div>
<p>The glider was ‘discovered’ in 1969 by Richard K. Guy. What do we
mean by discovered in this context? Well, as soon as the game of life is
defined, objects such as the glider do somehow exist, but the many
configurations of the game mean that it takes some time for us to see
one and know it exists. This means, that despite being the creator,
Conway, and despite the rules of the game being simple, and despite the
rules being deterministic, we are not ‘omniscient’ in any simplistic
sense. It requires computation to ‘discover’ what can exist in this
universe once it’s been defined.</p>
<div class="figure">
<div id="gosper-glider-gun-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Gosperglidergun.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gosper-glider-gun-magnify" class="magnify"
onclick="magnifyFigure(&#39;gosper-glider-gun&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gosper-glider-gun-caption" class="caption-frame">
<p>Figure: The Gosper glider gun is a configuration that creates
gliders. A new glider is released after every 30 turns.</p>
</div>
</div>
<p>These patterns had to be discovered, in the same way that a scientist
might discover a disease, or an explorer a new land. For example, the
Gosper glider gun was <a
href="https://conwaylife.com/wiki/Bill_Gosper">discovered by Bill Gosper
in 1970</a>. It is a pattern that creates a new glider every 30 turns of
the game.</p>
<p>Despite widespread interest in Life, some of its patterns were only
very recently discovered like the Loafer, discovered in 2013 by Josh
Ball. So, despite the game having existed for over forty years, and the
rules of the game being simple, there are emergent behaviors that are
unknown.</p>
<div class="figure">
<div id="the-loafer-spaceship-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Loafer (2013)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Loafer.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="the-loafer-spaceship-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-loafer-spaceship&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-loafer-spaceship-caption" class="caption-frame">
<p>Figure: <em>Left</em> A Loafer pattern discovered by Josh Ball in
2013. <em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020).</p>
</div>
</div>
<p>Once these patterns are discovered, they are combined (or engineered)
to create new Life patterns that do some remarkable things. For example,
there’s a life pattern that runs a Turing machine, or more remarkably
there’s a Life pattern that runs Life itself.</p>
<div class="figure">
<div id="life-in-life-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-in-life.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="life-in-life-magnify" class="magnify"
onclick="magnifyFigure(&#39;life-in-life&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-in-life-caption" class="caption-frame">
<p>Figure: The Game of Life running in Life. The video is drawing out
recursively showing pixels that are being formed by filling cells with
moving spaceships. Each individual pixel in this game of life is made up
of <span class="math inline">\(2048 \times 2048\)</span> pixels called
an <a href="https://www.conwaylife.com/wiki/OTCA_metapixel">OTCA
metapixel</a>.</p>
</div>
</div>
<p>To find out more about the Game of Life you can watch this video by
Alan Zucconi or read his <a
href="https://www.alanzucconi.com/2020/10/13/conways-game-of-life/">associated
blog post</a>.</p>
<div class="figure">
<div id="intro-to-life-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/Kk2MH9O4pXY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="intro-to-life-magnify" class="magnify"
onclick="magnifyFigure(&#39;intro-to-life&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intro-to-life-caption" class="caption-frame">
<p>Figure: An introduction to the Game of Life by Alan Zucconi.</p>
</div>
</div>
<p>Contrast this with our situation where in ‘real life’ we don’t know
the simple rules of the game, the state space is larger, and emergent
behaviors (hurricanes, earthquakes, volcanos, climate change) have
direct consequences for our daily lives, and we understand why the
process of ‘understanding’ the physical world is so difficult. We also
see immediately how much easier we might expect the physical sciences to
be than the social sciences, where the emergent behaviors are contingent
on highly complex human interactions.</p>
<h2 id="bayesian-inference-by-rejection-sampling">Bayesian Inference by
Rejection Sampling</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-intro-very-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One view of Bayesian inference is to assume we are given a mechanism
for generating samples, where we assume that mechanism is representing
an accurate view on the way we believe the world works.</p>
<p>This mechanism is known as our <em>prior</em> belief.</p>
<p>We combine our prior belief with our observations of the real world
by discarding all those prior samples that are inconsistent with our
observations. The <em>likelihood</em> defines mathematically what we
mean by inconsistent with the observations. The higher the noise level
in the likelihood, the looser the notion of consistent.</p>
<p>The samples that remain are samples from the <em>posterior</em>.</p>
<p>This approach to Bayesian inference is closely related to two
sampling techniques known as <em>rejection sampling</em> and
<em>importance sampling</em>. It is realized in practice in an approach
known as <em>approximate Bayesian computation</em> (ABC) or
likelihood-free inference.</p>
<p>In practice, the algorithm is often too slow to be practical, because
most samples will be inconsistent with the observations and as a result
the mechanism must be operated many times to obtain a few posterior
samples.</p>
<p>However, in the Gaussian process case, when the likelihood also
assumes Gaussian noise, we can operate this mechanism mathematically,
and obtain the posterior density <em>analytically</em>. This is the
benefit of Gaussian processes.</p>
<p>First, we will load in two python functions for computing the
covariance function.</p>
<p>Next, we sample from a multivariate normal density (a multivariate
Gaussian), using the covariance function as the covariance matrix.</p>
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gp-rejection-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-rejection-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-rejection-samples-caption" class="caption-frame">
<p>Figure: One view of Bayesian inference is we have a machine for
generating samples (the <em>prior</em>), and we discard all samples
inconsistent with our data, leaving the samples of interest (the
<em>posterior</em>). This is a rejection sampling view of Bayesian
inference. The Gaussian process allows us to do this analytically by
multiplying the <em>prior</em> by the <em>likelihood</em>.</p>
</div>
</div>
<p>Time scales, how when you expand or contract time signal becomes
noise and noise becomes signal illustrate with Dirac delta and and
stochastic processes in Fourier space, ito calculus. Latent force
models.</p>
<p>Practical examples of what happens understochasticity:</p>
<ol start="0" type="1">
<li><p>Derive U = W + TS?? Go from microscopic to macroscopic.</p></li>
<li><p>Kappenball — world in between where interesting things
happen,</p></li>
<li><p>Queue efficiency (M/M/1 1/(1-))</p></li>
<li><p>Input to the system being in the form of bias and variance (or
perhaps Brownian motion, wiener process)</p></li>
</ol>
<p>(Latent force models being driven by this???? Latent force as high
frequency information processing? Environment as slow?</p>
<!-- lecture 3 -->
<p>Connect supply chain as a “challenge” tot he abstraction of
Schroedinger’s bridge. Link to Optimal Transport (matching without the
“physics”). Maxwell’s demon.</p>
<p>Control ability paper with Mauricio and Simo??)</p>
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
<h1 id="what-is-machine-learning">What is Machine Learning?</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>What is machine learning? At its most basic level machine learning is
a combination of</p>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>where <em>data</em> is our observations. They can be actively or
passively acquired (meta-data). The <em>model</em> contains our
assumptions, based on previous experience. That experience can be other
data, it can come from transfer learning, or it can merely be our
beliefs about the regularities of the universe. In humans our models
include our inductive biases. The <em>prediction</em> is an action to be
taken or a categorization or a quality score. The reason that machine
learning has become a mainstay of artificial intelligence is the
importance of predictions in artificial intelligence. The data and the
model are combined through computation.</p>
<p>In practice we normally perform machine learning using two functions.
To combine data with a model we typically make use of:</p>
<p><strong>a prediction function</strong> it is used to make the
predictions. It includes our beliefs about the regularities of the
universe, our assumptions about how the world works, e.g., smoothness,
spatial similarities, temporal similarities.</p>
<p><strong>an objective function</strong> it defines the ‘cost’ of
misprediction. Typically, it includes knowledge about the world’s
generating processes (probabilistic objectives) or the costs we pay for
mispredictions (empirical risk minimization).</p>
<p>The combination of data and model through the prediction function and
the objective function leads to a <em>learning algorithm</em>. The class
of prediction functions and objective functions we can make use of is
restricted by the algorithms they lead to. If the prediction function or
the objective function are too complex, then it can be difficult to find
an appropriate learning algorithm. Much of the academic field of machine
learning is the quest for new learning algorithms that allow us to bring
different types of models and data together.</p>
<p>A useful reference for state of the art in machine learning is the UK
Royal Society Report, <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a>.</p>
<p>You can also check my post blog post on <a
href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What
is Machine Learning?</a>.</p>
<p>In practice, we normally also have uncertainty associated with these
functions. Uncertainty in the prediction function arises from</p>
<ol type="1">
<li>scarcity of training data and</li>
<li>mismatch between the set of prediction functions we choose and all
possible prediction functions.</li>
</ol>
<p>There are also challenges around specification of the objective
function, but for we will save those for another day. For the moment,
let us focus on the prediction function.</p>
<h2 id="neural-networks-and-prediction-functions">Neural Networks and
Prediction Functions</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/neural-networks.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/neural-networks.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Neural networks are adaptive non-linear function models. Originally,
they were studied (by McCulloch and Pitts <span class="citation"
data-cites="McCulloch-neuron43">(McCulloch and Pitts, 1943)</span>) as
simple models for neurons, but over the last decade they have become
popular because they are a flexible approach to modelling complex data.
A particular characteristic of neural network models is that they can be
composed to form highly complex functions which encode many of our
expectations of the real world. They allow us to encode our assumptions
about how the world works.</p>
<p>We will return to composition later, but for the moment, let’s focus
on a one hidden layer neural network. We are interested in the
prediction function, so we’ll ignore the objective function (which is
often called an error function) for the moment, and just describe the
mathematical object of interest</p>
<p><span class="math display">\[
f(\mathbf{ x}) = \mathbf{W}^\top \boldsymbol{ \phi}(\mathbf{V}, \mathbf{
x})
\]</span></p>
<p>Where in this case <span class="math inline">\(f(\cdot)\)</span> is a
scalar function with vector inputs, and <span
class="math inline">\(\boldsymbol{ \phi}(\cdot)\)</span> is a vector
function with vector inputs. The dimensionality of the vector function
is known as the number of hidden units, or the number of neurons. The
elements of this vector function are known as the <em>activation</em>
function of the neural network and <span
class="math inline">\(\mathbf{V}\)</span> are the parameters of the
activation functions.</p>
<h2 id="relations-with-classical-statistics">Relations with Classical
Statistics</h2>
<p>In statistics activation functions are traditionally known as
<em>basis functions</em>. And we would think of this as a <em>linear
model</em>. It’s doesn’t make linear predictions, but it’s linear
because in statistics estimation focuses on the parameters, <span
class="math inline">\(\mathbf{W}\)</span>, not the parameters, <span
class="math inline">\(\mathbf{V}\)</span>. The linear model terminology
refers to the fact that the model is <em>linear in the parameters</em>,
but it is <em>not</em> linear in the data unless the activation
functions are chosen to be linear.</p>
<h2 id="adaptive-basis-functions">Adaptive Basis Functions</h2>
<p>The first difference in the (early) neural network literature to the
classical statistical literature is the decision to optimize these
parameters, <span class="math inline">\(\mathbf{V}\)</span>, as well as
the parameters, <span class="math inline">\(\mathbf{W}\)</span> (which
would normally be denoted in statistics by <span
class="math inline">\(\boldsymbol{\beta}\)</span>)<a href="#fn4"
class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<h2 id="integrated-basis-functions">Integrated Basis Functions</h2>
<p>We’re going to go revisit that decision, and follow the path of
Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal,
1994)</span> who, inspired by work of David MacKay <span
class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span>
and others did his PhD thesis on Bayesian Neural Networks. If we take a
Bayesian approach to parameter inference (note I am using inference here
in the classical sense, not in the sense of prediction of test data,
which seems to be a newer usage), then we don’t wish to fit parameters
at all, rather we wish to integrate them away and understand the family
of functions that the model describes.</p>
<h2 id="probabilistic-modelling">Probabilistic Modelling</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/probabilistic-modelling.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/probabilistic-modelling.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>This Bayesian approach is designed to deal with uncertainty arising
from fitting our prediction function to the data we have, a reduced data
set.</p>
<p>The Bayesian approach can be derived from a broader understanding of
what our objective is. If we accept that we can jointly represent all
things that happen in the world with a probability distribution, then we
can interogate that probability to make predictions. So, if we are
interested in predictions, <span class="math inline">\(y_*\)</span> at
future points input locations of interest, <span
class="math inline">\(\mathbf{ x}_*\)</span> given previously training
data, <span class="math inline">\(\mathbf{ y}\)</span> and corresponding
inputs, <span class="math inline">\(\mathbf{X}\)</span>, then we are
really interogating the following probability density, <span
class="math display">\[
p(y_*|\mathbf{ y}, \mathbf{X}, \mathbf{ x}_*),
\]</span> there is nothing controversial here, as long as you accept
that you have a good joint model of the world around you that relates
test data to training data, <span class="math inline">\(p(y_*, \mathbf{
y}, \mathbf{X}, \mathbf{ x}_*)\)</span> then this conditional
distribution can be recovered through standard rules of probability
(<span class="math inline">\(\text{data} + \text{model} \rightarrow
\text{prediction}\)</span>).</p>
<p>We can construct this joint density through the use of the following
decomposition: <span class="math display">\[
p(y_*|\mathbf{ y}, \mathbf{X}, \mathbf{ x}_*) = \int p(y_*|\mathbf{
x}_*, \mathbf{W}) p(\mathbf{W}| \mathbf{ y}, \mathbf{X}) \text{d}
\mathbf{W}
\]</span></p>
<p>where, for convenience, we are assuming <em>all</em> the parameters
of the model are now represented by <span
class="math inline">\(\boldsymbol{ \theta}\)</span> (which contains
<span class="math inline">\(\mathbf{W}\)</span> and <span
class="math inline">\(\mathbf{V}\)</span>) and <span
class="math inline">\(p(\boldsymbol{ \theta}| \mathbf{ y},
\mathbf{X})\)</span> is recognised as the posterior density of the
parameters given data and <span class="math inline">\(p(y_*|\mathbf{
x}_*, \boldsymbol{ \theta})\)</span> is the <em>likelihood</em> of an
individual test data point given the parameters.</p>
<p>The likelihood of the data is normally assumed to be independent
across the parameters, <span class="math display">\[
p(\mathbf{ y}|\mathbf{X}, \mathbf{W}) = \prod_{i=1}^np(y_i|\mathbf{
x}_i, \mathbf{W}),\]</span></p>
<p>and if that is so, it is easy to extend our predictions across all
future, potential, locations, <span class="math display">\[
p(\mathbf{ y}_*|\mathbf{ y}, \mathbf{X}, \mathbf{X}_*) = \int p(\mathbf{
y}_*|\mathbf{X}_*, \boldsymbol{ \theta}) p(\boldsymbol{ \theta}|
\mathbf{ y}, \mathbf{X}) \text{d} \boldsymbol{ \theta}.
\]</span></p>
<p>The likelihood is also where the <em>prediction function</em> is
incorporated. For example in the regression case, we consider an
objective based around the Gaussian density, <span
class="math display">\[
p(y_i | f(\mathbf{ x}_i)) = \frac{1}{\sqrt{2\pi \sigma^2}}
\exp\left(-\frac{\left(y_i - f(\mathbf{
x}_i)\right)^2}{2\sigma^2}\right)
\]</span></p>
<p>In short, that is the classical approach to probabilistic inference,
and all approaches to Bayesian neural networks fall within this path.
For a deep probabilistic model, we can simply take this one stage
further and place a probability distribution over the input locations,
<span class="math display">\[
p(\mathbf{ y}_*|\mathbf{ y}) = \int p(\mathbf{ y}_*|\mathbf{X}_*,
\boldsymbol{ \theta}) p(\boldsymbol{ \theta}| \mathbf{ y}, \mathbf{X})
p(\mathbf{X}) p(\mathbf{X}_*) \text{d} \boldsymbol{ \theta}\text{d}
\mathbf{X}\text{d}\mathbf{X}_*
\]</span> and we have <em>unsupervised learning</em> (from where we can
get deep generative models).</p>
<h2 id="graphical-models">Graphical Models</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/graphical-models.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/graphical-models.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One way of representing a joint distribution is to consider
conditional dependencies between data. Conditional dependencies allow us
to factorize the distribution. For example, a Markov chain is a
factorization of a distribution into components that represent the
conditional relationships between points that are neighboring, often in
time or space. It can be decomposed in the following form. <span
class="math display">\[p(\mathbf{ y}) = p(y_n| y_{n-1})
p(y_{n-1}|y_{n-2}) \dots p(y_{2} | y_{1})\]</span></p>
<div class="figure">
<div id="markov-chain-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ml/markov.svg" width="50%" style=" ">
</object>
</div>
<div id="markov-chain-magnify" class="magnify"
onclick="magnifyFigure(&#39;markov-chain&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="markov-chain-caption" class="caption-frame">
<p>Figure: A Markov chain is a simple form of probabilistic graphical
model providing a particular decomposition of the joint density.</p>
</div>
</div>
<p>By specifying conditional independencies we can reduce the
parameterization required for our data, instead of directly specifying
the parameters of the joint distribution, we can specify each set of
parameters of the conditonal independently. This can also give an
advantage in terms of interpretability. Understanding a conditional
independence structure gives a structured understanding of data. If
developed correctly, according to causal methodology, it can even inform
how we should intervene in the system to drive a desired result <span
class="citation" data-cites="Pearl:causality95">(Pearl,
1995)</span>.</p>
<p>However, a challenge arises when the data becomes more complex.
Consider the graphical model shown below, used to predict the
perioperative risk of <em>C Difficile</em> infection following colon
surgery <span class="citation" data-cites="Steele:predictive12">(Steele
et al., 2012)</span>.</p>
<div class="figure">
<div id="c-difficile-bayes-net-diagnosis-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//bayes-net-diagnosis.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="c-difficile-bayes-net-diagnosis-magnify" class="magnify"
onclick="magnifyFigure(&#39;c-difficile-bayes-net-diagnosis&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="c-difficile-bayes-net-diagnosis-caption" class="caption-frame">
<p>Figure: A probabilistic directed graph used to predict the
perioperative risk of <em>C Difficile</em> infection following colon
surgery. When these models have good predictive performance they are
often difficult to interpret. This may be due to the limited
representation capability of the conditional densities in the model.</p>
</div>
</div>
<p>To capture the complexity in the interelationship between the data,
the graph itself becomes more complex, and less interpretable.</p>
<h2 id="performing-inference">Performing Inference</h2>
<p>As far as combining our data and our model to form our prediction,
the devil is in the detail. While everything is easy to write in terms
of probability densities, as we move from <span
class="math inline">\(\text{data}\)</span> and <span
class="math inline">\(\text{model}\)</span> to <span
class="math inline">\(\text{prediction}\)</span> there is that simple
<span
class="math inline">\(\stackrel{\text{compute}}{\rightarrow}\)</span>
sign, which is now burying a wealth of difficulties. Each integral sign
above is a high dimensional integral which will typically need
approximation. Approximations also come with computational demands. As
we consider more complex classes of functions, the challenges around the
integrals become harder and prediction of future test data given our
model and the data becomes so involved as to be impractical or
impossible.</p>
<p>Statisticians realized these challenges early on, indeed, so early
that they were actually physicists, both Laplace and Gauss worked on
models such as this, in Gauss’s case he made his career on prediction of
the location of the lost planet (later reclassified as a asteroid, then
dwarf planet), Ceres. Gauss and Laplace made use of maximum a posteriori
estimates for simplifying their computations and Laplace developed
Laplace’s method (and invented the Gaussian density) to expand around
that mode. But classical statistics needs better guarantees around model
performance and interpretation, and as a result has focussed more on the
<em>linear</em> model implied by <span class="math display">\[
  f(\mathbf{ x}) = \left.\mathbf{ w}^{(2)}\right.^\top \boldsymbol{
\phi}(\mathbf{W}_1, \mathbf{ x})
  \]</span></p>
<p><span class="math display">\[
  \mathbf{ w}^{(2)} \sim \mathcal{N}\left(\mathbf{0},\mathbf{C}\right).
  \]</span></p>
<p>The Gaussian likelihood given above implies that the data observation
is related to the function by noise corruption so we have, <span
class="math display">\[
  y_i = f(\mathbf{ x}_i) + \epsilon_i,
  \]</span> where <span class="math display">\[
  \epsilon_i \sim \mathcal{N}\left(0,\sigma^2\right)
  \]</span></p>
<p>and while normally integrating over high dimensional parameter
vectors is highly complex, here it is <em>trivial</em>. That is because
of a property of the multivariate Gaussian.</p>
<p>Gaussian processes are initially of interest because</p>
<ol type="1">
<li>linear Gaussian models are easier to deal with</li>
<li>Even the parameters <em>within</em> the process can be handled, by
considering a particular limit.</li>
</ol>
<p>Let’s first of all review the properties of the multivariate Gaussian
distribution that make linear Gaussian models easier to deal with. We’ll
return to the, perhaps surprising, result on the parameters within the
nonlinearity, <span class="math inline">\(\boldsymbol{ \theta}\)</span>,
shortly.</p>
<p>To work with linear Gaussian models, to find the marginal likelihood
all you need to know is the following rules. If <span
class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span> where <span class="math inline">\(\mathbf{ y}\)</span>, <span
class="math inline">\(\mathbf{ x}\)</span> and <span
class="math inline">\(\boldsymbol{ \epsilon}\)</span> are vectors and we
assume that <span class="math inline">\(\mathbf{ x}\)</span> and <span
class="math inline">\(\boldsymbol{ \epsilon}\)</span> are drawn from
multivariate Gaussians, <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{
\mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim
\mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span> then we know that <span class="math inline">\(\mathbf{
y}\)</span> is also drawn from a multivariate Gaussian with, <span
class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span></p>
<p>With appropriately defined covariance, <span
class="math inline">\(\boldsymbol{ \Sigma}\)</span>, this is actually
the marginal likelihood for Factor Analysis, or Probabilistic Principal
Component Analysis <span class="citation"
data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>,
because we integrated out the inputs (or <em>latent</em> variables they
would be called in that case).</p>
<h2 id="linear-model-overview">Linear Model Overview</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-model-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-model-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>However, we are focussing on what happens in models which are
non-linear in the inputs, whereas the above would be <em>linear</em> in
the inputs. To consider these, we introduce a matrix, called the design
matrix. We set each activation function computed at each data point to
be <span class="math display">\[
\phi_{i,j} = \phi(\mathbf{ w}^{(1)}_{j}, \mathbf{ x}_{i})
\]</span> and define the matrix of activations (known as the <em>design
matrix</em> in statistics) to be, <span class="math display">\[
\boldsymbol{ \Phi}=
\begin{bmatrix}
\phi_{1, 1} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, h} \\
\phi_{1, 2} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi_{n, 1} &amp; \phi_{n, 2} &amp; \dots &amp; \phi_{n, h}
\end{bmatrix}.
\]</span> By convention this matrix always has <span
class="math inline">\(n\)</span> rows and <span
class="math inline">\(h\)</span> columns, now if we define the vector of
all noise corruptions, <span class="math inline">\(\boldsymbol{
\epsilon}= \left[\epsilon_1, \dots \epsilon_n\right]^\top\)</span>.</p>
<p>If we define the prior distribution over the vector <span
class="math inline">\(\mathbf{ w}\)</span> to be Gaussian, <span
class="math display">\[
\mathbf{ w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right),
\]</span> then we can use rules of multivariate Gaussians to see that,
<span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{0},\alpha \boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top + \sigma^2 \mathbf{I}\right).
\]</span></p>
<p>In other words, our training data is distributed as a multivariate
Gaussian, with zero mean and a covariance given by <span
class="math display">\[
\mathbf{K}= \alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top + \sigma^2
\mathbf{I}.
\]</span></p>
<p>This is an <span class="math inline">\(n\times n\)</span> size
matrix. Its elements are in the form of a function. The maths shows that
any element, index by <span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span>, is a function <em>only</em> of inputs
associated with data points <span class="math inline">\(i\)</span> and
<span class="math inline">\(j\)</span>, <span
class="math inline">\(\mathbf{ y}_i\)</span>, <span
class="math inline">\(\mathbf{ y}_j\)</span>. <span
class="math inline">\(k_{i,j} = k\left(\mathbf{ x}_i, \mathbf{
x}_j\right)\)</span></p>
<p>If we look at the portion of this function associated only with <span
class="math inline">\(f(\cdot)\)</span>, i.e. we remove the noise, then
we can write down the covariance associated with our neural network,
<span class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)
\]</span> so the elements of the covariance or <em>kernel</em> matrix
are formed by inner products of the rows of the <em>design
matrix</em>.</p>
<h2 id="gaussian-process">Gaussian Process</h2>
<p>This is the essence of a Gaussian process. Instead of making
assumptions about our density over each data point, <span
class="math inline">\(y_i\)</span> as i.i.d. we make a joint Gaussian
assumption over our data. The covariance matrix is now a function of
both the parameters of the activation function, <span
class="math inline">\(\mathbf{V}\)</span>, and the input variables,
<span class="math inline">\(\mathbf{X}\)</span>. This comes about
through integrating out the parameters of the model, <span
class="math inline">\(\mathbf{ w}\)</span>.</p>
<h2 id="basis-functions">Basis Functions</h2>
<p>We can basically put anything inside the basis functions, and many
people do. These can be deep kernels <span class="citation"
data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or we can learn the
parameters of a convolutional neural network inside there.</p>
<p>Viewing a neural network in this way is also what allows us to beform
sensible <em>batch</em> normalizations <span class="citation"
data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</p>
<h2 id="non-degenerate-gaussian-processes">Non-degenerate Gaussian
Processes</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/non-degenerate-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/non-degenerate-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The process described above is degenerate. The covariance function is
of rank at most <span class="math inline">\(h\)</span> and since the
theoretical amount of data could always increase <span
class="math inline">\(n\rightarrow \infty\)</span>, the covariance
function is not full rank. This means as we increase the amount of data
to infinity, there will come a point where we can’t normalize the
process because the multivariate Gaussian has the form, <span
class="math display">\[
\mathcal{N}\left(\mathbf{ f}|\mathbf{0},\mathbf{K}\right) =
\frac{1}{\left(2\pi\right)^{\frac{n}{2}}\det{\mathbf{K}}^\frac{1}{2}}
\exp\left(-\frac{\mathbf{ f}^\top\mathbf{K}\mathbf{ f}}{2}\right)
\]</span> and a non-degenerate kernel matrix leads to <span
class="math inline">\(\det{\mathbf{K}} = 0\)</span> defeating the
normalization (it’s equivalent to finding a projection in the high
dimensional Gaussian where the variance of the the resulting univariate
Gaussian is zero, i.e. there is a null space on the covariance, or
alternatively you can imagine there are one or more directions where the
Gaussian has become the delta function).</p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Radford Neal
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/radford-neal.jpg" clip-path="url(#clip0)"/>
</svg>
</div>
<p>In the machine learning field, it was Radford Neal <span
class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> that
realized the potential of the next step. In his 1994 thesis, he was
considering Bayesian neural networks, of the type we described above,
and in considered what would happen if you took the number of hidden
nodes, or neurons, to infinity, i.e. <span
class="math inline">\(h\rightarrow \infty\)</span>.</p>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="neal-infinite-priors-magnify" class="magnify"
onclick="magnifyFigure(&#39;neal-infinite-priors&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="neal-infinite-priors-caption" class="caption-frame">
<p>Figure: Page 37 of <a
href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s
1994 thesis</a></p>
</div>
</div>
<p>In loose terms, what Radford considers is what happens to the
elements of the covariance function, <span class="math display">\[
  \begin{align*}
  k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) &amp; = \alpha
\boldsymbol{ \phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top
\boldsymbol{ \phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)\\
  &amp; = \alpha \sum_k \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{
x}_i\right) \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{ x}_j\right)
  \end{align*}
  \]</span> if instead of considering a finite number you sample
infinitely many of these activation functions, sampling parameters from
a prior density, <span class="math inline">\(p(\mathbf{ v})\)</span>,
for each one, <span class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \int
\phi\left(\mathbf{ w}^{(1)}, \mathbf{ x}_i\right) \phi\left(\mathbf{
w}^{(1)}, \mathbf{ x}_j\right) p(\mathbf{ w}^{(1)}) \text{d}\mathbf{
w}^{(1)}
\]</span> And that’s not <em>only</em> for Gaussian <span
class="math inline">\(p(\mathbf{ v})\)</span>. In fact this result holds
for a range of activations, and a range of prior densities because of
the <em>central limit theorem</em>.</p>
<p>To write it in the form of a probabilistic program, as long as the
distribution for <span class="math inline">\(\phi_i\)</span> implied by
this short probabilistic program, <span class="math display">\[
  \begin{align*}
  \mathbf{ v}&amp; \sim p(\cdot)\\
  \phi_i &amp; = \phi\left(\mathbf{ v}, \mathbf{ x}_i\right),
  \end{align*}
  \]</span> has finite variance, then the result of taking the number of
hidden units to infinity, with appropriate scaling, is also a Gaussian
process.</p>
<h2 id="further-reading">Further Reading</h2>
<p>To understand this argument in more detail, I highly recommend
reading chapter 2 of Neal’s thesis <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span>, which remains easy to
read and clear today. Indeed, for readers interested in Bayesian neural
networks, both Raford Neal’s and David MacKay’s PhD thesis <span
class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span>
remain essential reading. Both theses embody a clarity of thought, and
an ability to weave together threads from different fields that was the
business of machine learning in the 1990s. Radford and David were also
pioneers in making their software widely available and publishing
material on the web.</p>
<!-- ### Two Dimensional Gaussian Distribution -->
<!-- include{_ml/includes/two-d-gaussian.md} -->
<h2 id="sampling-a-function">Sampling a Function</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpdistfunc.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gpdistfunc.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We will consider a Gaussian distribution with a particular structure
of covariance matrix. We will generate <em>one</em> sample from a
25-dimensional Gaussian density. <span class="math display">\[
\mathbf{ f}=\left[f_{1},f_{2}\dots f_{25}\right].
\]</span> in the figure below we plot these data on the <span
class="math inline">\(y\)</span>-axis against their <em>indices</em> on
the <span class="math inline">\(x\)</span>-axis.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> Kernel</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> polynomial_cov</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> exponentiated_quadratic</span></code></pre></div>
<div class="figure">
<div id="gp-two-point-sample-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample008.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-two-point-sample-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-two-point-sample-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-two-point-sample-1-caption" class="caption-frame">
<p>Figure: A 25 dimensional correlated random variable (values ploted
against index)</p>
</div>
</div>
<h3 id="sampling-a-function-from-a-gaussian">Sampling a Function from a
Gaussian</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-two.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-two.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample001.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_2\)</span> along with the conditional
distribution of <span class="math inline">\(f_2\)</span> given <span
class="math inline">\(f_1\)</span></p>
</div>
</div>
<h3 id="joint-density-of-f_1-and-f_2">Joint Density of <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_2\)</span></h3>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample012.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_2\)</span> along with the conditional
distribution of <span class="math inline">\(f_2\)</span> given <span
class="math inline">\(f_1\)</span></p>
</div>
</div>
<h2 id="uluru">Uluru</h2>
<div class="figure">
<div id="uluru-as-probability-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/799px-Uluru_Panorama.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uluru-as-probability-magnify" class="magnify"
onclick="magnifyFigure(&#39;uluru-as-probability&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uluru-as-probability-caption" class="caption-frame">
<p>Figure: Uluru, the sacred rock in Australia. If we think of it as a
probability density, viewing it from this side gives us one
<em>marginal</em> from the density. Figuratively speaking, slicing
through the rock would give a conditional density.</p>
</div>
</div>
<p>When viewing these contour plots, I sometimes find it helpful to
think of Uluru, the prominent rock formation in Australia. The rock
rises above the surface of the plane, just like a probability density
rising above the zero line. The rock is three dimensional, but when we
view Uluru from the classical position, we are looking at one side of
it. This is equivalent to viewing the marginal density.</p>
<p>The joint density can be viewed from above, using contours. The
conditional density is equivalent to <em>slicing</em> the rock. Uluru is
a holy rock, so this has to be an imaginary slice. Imagine we cut down a
vertical plane orthogonal to our view point (e.g. coming across our view
point). This would give a profile of the rock, which when renormalized,
would give us the conditional distribution, the value of conditioning
would be the location of the slice in the direction we are facing.</p>
<h2 id="prediction-with-correlated-gaussians">Prediction with Correlated
Gaussians</h2>
<p>Of course in practice, rather than manipulating mountains physically,
the advantage of the Gaussian density is that we can perform these
manipulations mathematically.</p>
<p>Prediction of <span class="math inline">\(f_2\)</span> given <span
class="math inline">\(f_1\)</span> requires the <em>conditional
density</em>, <span class="math inline">\(p(f_2|f_1)\)</span>.Another
remarkable property of the Gaussian density is that this conditional
distribution is <em>also</em> guaranteed to be a Gaussian density. It
has the form, <span class="math display">\[
p(f_2|f_1) = \mathcal{N}\left(f_2|\frac{k_{1, 2}}{k_{1, 1}}f_1, k_{2, 2}
- \frac{k_{1,2}^2}{k_{1,1}}\right)
\]</span>where we have assumed that the covariance of the original joint
density was given by <span class="math display">\[
\mathbf{K}= \begin{bmatrix} k_{1, 1} &amp; k_{1, 2}\\ k_{2, 1} &amp;
k_{2, 2}.\end{bmatrix}
\]</span></p>
<p>Using these formulae we can determine the conditional density for any
of the elements of our vector <span class="math inline">\(\mathbf{
f}\)</span>. For example, the variable <span
class="math inline">\(f_8\)</span> is less correlated with <span
class="math inline">\(f_1\)</span> than <span
class="math inline">\(f_2\)</span>. If we consider this variable we see
the conditional density is more diffuse.</p>
<h3 id="joint-density-of-f_1-and-f_8">Joint Density of <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_8\)</span></h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-eight.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gaussian-predict-index-one-and-eight.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="two-point-sample-13-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample013.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-13-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-13&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-13-caption" class="caption-frame">
<p>Figure: Sample from the joint Gaussian model, points indexed by 1 and
8 highlighted.</p>
</div>
</div>
<h3 id="prediction-of-f_8-from-f_1">Prediction of <span
class="math inline">\(f_{8}\)</span> from <span
class="math inline">\(f_{1}\)</span></h3>
<div class="figure">
<div id="two-point-sample-one-eight-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample017.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-eight-magnify" class="magnify"
onclick="magnifyFigure(&#39;two-point-sample-one-eight&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="two-point-sample-one-eight-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_8\)</span> along with the conditional
distribution of <span class="math inline">\(f_8\)</span> given <span
class="math inline">\(f_1\)</span></p>
</div>
</div>
<ul>
<li>The single contour of the Gaussian density represents the
<font color="blue">joint distribution, <span
class="math inline">\(p(f_1, f_8)\)</span></font></li>
</ul>
<p>. . .</p>
<ul>
<li>We observe a value for <font color="green"><span
class="math inline">\(f_1=-?\)</span></font></li>
</ul>
<p>. . .</p>
<ul>
<li><p>Conditional density: <font color="red"><span
class="math inline">\(p(f_8|f_1=?)\)</span></font>.</p></li>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{ f},\mathbf{
f}}^{-1}\mathbf{ f},\mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{
f},*}\right)}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\boldsymbol{ \mu},\boldsymbol{ \Sigma}\right)}
\]</span> <span class="math display">\[
\boldsymbol{ \mu}= \mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{
f},\mathbf{ f}}^{-1}\mathbf{ f}
\]</span> <span class="math display">\[
\boldsymbol{ \Sigma}= \mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{ f},*}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
<li><p>Covariance function, <span
class="math inline">\(\mathbf{K}\)</span></p></li>
<li><p>Determines properties of samples.</p></li>
<li><p>Function of <span class="math inline">\(\mathbf{X}\)</span>,
<span class="math display">\[k_{i,j} = k(\mathbf{ x}_i, \mathbf{
x}_j)\]</span></p></li>
<li><p>Posterior mean <span class="math display">\[f_D(\mathbf{ x}_*) =
\mathbf{ k}(\mathbf{ x}_*, \mathbf{X}) \mathbf{K}^{-1}
\mathbf{ y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* =
\mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{ f}, *}\]</span></p></li>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D(\mathbf{ x}_*) = \mathbf{
k}(\mathbf{ x}_*, \mathbf{X}) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* =
\mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{ f}, *}\]</span></p></li>
</ul>
<h2 id="exponentiated-quadratic-covariance">Exponentiated Quadratic
Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/eq-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/eq-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The exponentiated quadratic covariance, also known as the Gaussian
covariance or the RBF covariance and the squared exponential. Covariance
between two points is related to the negative exponential of the squared
distnace between those points. This covariance function can be derived
in a few different ways: as the infinite limit of a radial basis
function neural network, as diffusion in the heat equation, as a
Gaussian filter in <em>Fourier space</em> or as the composition as a
series of linear filters applied to a base function.</p>
<p>The covariance takes the following form, <span
class="math display">\[
k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha \exp\left(-\frac{\left\Vert
\mathbf{ x}-\mathbf{ x}^\prime \right\Vert_2^2}{2\ell^2}\right)
\]</span> where <span class="math inline">\(\ell\)</span> is the
<em>length scale</em> or <em>time scale</em> of the process and <span
class="math inline">\(\alpha\)</span> represents the overall process
variance.</p>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\exp\left(-\frac{\left\Vert \mathbf{ x}-\mathbf{ x}^\prime
\right\Vert_2^2}{2\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eq-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;eq-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="eq-covariance-plot-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function.</p>
</div>
</div>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression
modelling. The data consists of the pace of Olympic Gold Medal Marathon
winners for the Olympics from 1896 to present. Let’s load in the data
and plot.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pods</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1896.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in that
year the Olympics was in St Louis, USA. Organizational problems and
challenges with dust kicked up by the cars following the race meant that
participants got lost, and only very few participants completed. More
recent years see more consistently quick marathons.</p>
<h2 id="alan-turing">Alan Turing</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="turing-run-times-magnify" class="magnify"
onclick="magnifyFigure(&#39;turing-run-times&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="turing-run-times-caption" class="caption-frame">
<p>Figure: Alan Turing, in 1946 he was only 11 minutes slower than the
winner of the 1948 games. Would he have won a hypothetical games held in
1946? Source:
<a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank">Alan
Turing Internet Scrapbook</a>.</p>
</div>
</div>
<p>If we had to summarise the objectives of machine learning in one
word, a very good candidate for that word would be
<em>generalization</em>. What is generalization? From a human
perspective it might be summarised as the ability to take lessons
learned in one domain and apply them to another domain. If we accept the
definition given in the first session for machine learning, <span
class="math display">\[
\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow}
\text{prediction}
\]</span> then we see that without a model we can’t generalise: we only
have data. Data is fine for answering very specific questions, like “Who
won the Olympic Marathon in 2012?”, because we have that answer stored,
however, we are not given the answer to many other questions. For
example, Alan Turing was a formidable marathon runner, in 1946 he ran a
time 2 hours 46 minutes (just under four minutes per kilometer, faster
than I and most of the other <a
href="http://www.parkrun.org.uk/sheffieldhallam/">Endcliffe Park Run</a>
runners can do 5 km). What is the probability he would have won an
Olympics if one had been held in 1946?</p>
<p>To answer this question we need to generalize, but before we
formalize the concept of generalization let’s introduce some formal
representation of what it means to generalize in machine learning.</p>
<h2 id="gaussian-process-fit">Gaussian Process Fit</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/olympic-marathon-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/olympic-marathon-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Our first objective will be to perform a Gaussian process fit to the
data, we’ll do this using the <a
href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<p>The first command sets up the model, then
<code>m_full.optimize()</code> optimizes the parameters of the
covariance function and the noise level of the model. Once the fit is
complete, we’ll try creating some test points, and computing the output
of the GP model in terms of the mean and standard deviation of the
posterior functions between 1870 and 2030. We plot the mean function and
the standard deviation at 200 locations. We can obtain the predictions
using <code>y_mean, y_var = m_full.predict(xt)</code></p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.linspace(<span class="dv">1870</span>,<span class="dv">2030</span>,<span class="dv">200</span>)[:,np.newaxis]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>yt_mean, yt_var <span class="op">=</span> m_full.predict(xt)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>yt_sd<span class="op">=</span>np.sqrt(yt_var)</span></code></pre></div>
<p>Now we plot the results using the helper function in
<code>mlai.plot</code>.</p>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/olympic-marathon-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Olympic Marathon data. The error
bars are too large, perhaps due to the outlier from 1904.</p>
</div>
</div>
<h2 id="fit-quality">Fit Quality</h2>
<p>In the fit we see that the error bars (coming mainly from the noise
variance) are quite large. This is likely due to the outlier point in
1904, ignoring that point we can see that a tighter fit is obtained. To
see this make a version of the model, <code>m_clean</code>, where that
point is removed.</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>x_clean<span class="op">=</span>np.vstack((x[<span class="dv">0</span>:<span class="dv">2</span>, :], x[<span class="dv">3</span>:, :]))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>y_clean<span class="op">=</span>np.vstack((yhat[<span class="dv">0</span>:<span class="dv">2</span>, :], yhat[<span class="dv">3</span>:, :]))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>m_clean <span class="op">=</span> GPy.models.GPRegression(x_clean,y_clean)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_clean.optimize()</span></code></pre></div>
<h2 id="learning-covariance-parameters">Learning Covariance
Parameters</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Can we determine covariance parameters from the data?</p>
<p><span class="math display">\[
\mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}{\det{\mathbf{K}}^{\frac{1}{2}}}}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}\color{blue}{\det{\mathbf{K}}^{\frac{1}{2}}}}\color{red}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
    \log \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=&amp;\color{blue}{-\frac{1}{2}\log\det{\mathbf{K}}}\color{red}{-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}} \\ &amp;-\frac{n}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
E(\boldsymbol{ \theta}) = \color{blue}{\frac{1}{2}\log\det{\mathbf{K}}}
+ \color{red}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}
\]</span></p>
<h2 id="capacity-control-through-the-determinant">Capacity Control
through the Determinant</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The parameters are <em>inside</em> the covariance function (matrix).
<span class="math display">\[k_{i, j} = k(\mathbf{ x}_i, \mathbf{ x}_j;
\boldsymbol{ \theta})\]</span></p>
<p><span> <span class="math display">\[\mathbf{K}=
\mathbf{R}\boldsymbol{ \Lambda}^2 \mathbf{R}^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\boldsymbol{ \Lambda}\)</span> represents
distance on axes. <span class="math inline">\(\mathbf{R}\)</span> gives
rotation.
</td>
</tr>
</table>
<ul>
<li><span class="math inline">\(\boldsymbol{ \Lambda}\)</span> is
<em>diagonal</em>, <span
class="math inline">\(\mathbf{R}^\top\mathbf{R}=
\mathbf{I}\)</span>.</li>
<li>Useful representation since <span
class="math inline">\(\det{\mathbf{K}} = \det{\boldsymbol{ \Lambda}^2} =
\det{\boldsymbol{ \Lambda}}^2\)</span>.</li>
</ul>
<div class="figure">
<div id="gp-optimise-determinant-figure-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-determinant-figure-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-optimise-determinant-figure&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-determinant-figure-caption" class="caption-frame">
<p>Figure: The determinant of the covariance is dependent only on the
eigenvalues. It represents the ‘footprint’ of the Gaussian.</p>
</div>
</div>
<h2 id="quadratic-data-fit">Quadratic Data Fit</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div id="gp-optimise-quadratic-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-optimise-quadratic&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-quadratic-caption" class="caption-frame">
<p>Figure: The data fit term of the Gaussian process is a quadratic loss
centered around zero. This has eliptical contours, the principal axes of
which are given by the covariance matrix.</p>
</div>
</div>
<h2 id="data-fit-term">Data Fit Term</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit-capacity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/gp-optimize-data-fit-capacity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="gp-optimise-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise006.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise010.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise016.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise021.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
</div>
<div id="gp-optimise-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-optimise&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-optimise-caption" class="caption-frame">
<p>Figure: Variation in the data fit term, the capacity term and the
negative log likelihood for different lengthscales.</p>
</div>
</div>
<h2 id="gene-expression-example">Gene Expression Example</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/della-gatta-gene-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/della-gatta-gene-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We now consider an example in gene expression. Gene expression is the
measurement of mRNA levels expressed in cells. These mRNA levels show
which genes are ‘switched on’ and producing data. In the example we will
use a Gaussian process to determine whether a given gene is active, or
we are merely observing a noise response.</p>
<h2 id="della-gatta-gene-data">Della Gatta Gene Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/della-gatta-gene-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/della-gatta-gene-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li>Given given expression levels in the form of a time series from
<span class="citation" data-cites="DellaGatta:direct08">Della Gatta et
al. (2008)</span>.</li>
</ul>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.della_gatta_TRP63_gene_expression(data_set<span class="op">=</span><span class="st">&#39;della_gatta&#39;</span>,gene_number<span class="op">=</span><span class="dv">937</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-data-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-data-caption" class="caption-frame">
<p>Figure: Gene expression levels over time for a gene from data
provided by <span class="citation"
data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>. We
would like to understand whether there is signal in the data, or we are
only observing noise.</p>
</div>
</div>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene
<span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and
Lawrence (2011)</span>.</li>
</ul>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Freddie Kalaitzis
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/freddie-kalaitzis.jpg" clip-path="url(#clip1)"/>
</svg>
</div>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="a-simple-approach-to-ranking-magnify" class="magnify"
onclick="magnifyFigure(&#39;a-simple-approach-to-ranking&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="a-simple-approach-to-ranking-caption" class="caption-frame">
<p>Figure: The example is taken from the paper “A Simple Approach to
Ranking Differentially Expressed Gene Expression Time Courses through
Gaussian Process Regression.” <span class="citation"
data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence
(2011)</span>.</p>
</div>
</div>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180"
class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
<p>Our first objective will be to perform a Gaussian process fit to the
data, we’ll do this using the <a
href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>m_full.kern.lengthscale<span class="op">=</span><span class="dv">50</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<p>Initialize the length scale parameter (which here actually represents
a <em>time scale</em> of the covariance function) to a reasonable value.
Default would be 1, but here we set it to 50 minutes, given points are
arriving across zero to 250 minutes.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">20</span>,<span class="dv">260</span>,<span class="dv">200</span>)[:,np.newaxis]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>yt_mean, yt_var <span class="op">=</span> m_full.predict(xt)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>yt_sd<span class="op">=</span>np.sqrt(yt_var)</span></code></pre></div>
<p>Now we plot the results using the helper function in
<code>mlai.plot</code>.</p>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-gp-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the time
scale parameter initialized to 50 minutes.</p>
</div>
</div>
<p>Now we try a model initialized with a longer length scale.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>m_full2 <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>m_full2.kern.lengthscale<span class="op">=</span><span class="dv">2000</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full2.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp2-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-gp2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-gp2-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the time
scale parameter initialized to 2000 minutes.</p>
</div>
</div>
<p>Now we try a model initialized with a lower noise.</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>m_full3 <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>m_full3.kern.lengthscale<span class="op">=</span><span class="dv">20</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>m_full3.likelihood.variance<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full3.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp3-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-gp3&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-gp3-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the
noise initialized low (standard deviation 0.1) and the time scale
parameter initialized to 20 minutes.</p>
</div>
</div>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
<div id="gp-multiple-optima000-magnify" class="magnify"
onclick="magnifyFigure(&#39;gp-multiple-optima000&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-multiple-optima000-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<!--

<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima001.svg" width="" style=" "></object>-->
<h2 id="example-prediction-of-malaria-incidence-in-uganda">Example:
Prediction of Malaria Incidence in Uganda</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/martin-mubangizi.png" clip-path="url(#clip2)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip3">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacecho
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/ricardo-andrade-pacheco.png" clip-path="url(#clip3)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/john-quinn.jpg" clip-path="url(#clip4)"/>
</svg>
</div>
<p>As an example of using Gaussian process models within the full
pipeline from data to decsion, we’ll consider the prediction of Malaria
incidence in Uganda. For the purposes of this study malaria reports come
in two forms, HMIS reports from health centres and Sentinel data, which
is curated by the WHO. There are limited sentinel sites and many HMIS
sites.</p>
<p>The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in
collaboration with John Quinn and Martin Mubangizi <span
class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>. John and Martin were
initally from the AI-DEV group from the University of Makerere in
Kampala and more latterly they were based at UN Global Pulse in Kampala.
You can see the work summarized on the UN Global Pulse <a
href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">disease
outbreaks project site here</a>.</p>
<ul>
<li>See <a href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">UN
Global Pulse Disease Outbreaks Site</a></li>
</ul>
<p>Malaria data is spatial data. Uganda is split into districts, and
health reports can be found for each district. This suggests that models
such as conditional random fields could be used for spatial modelling,
but there are two complexities with this. First of all, occasionally
districts split into two. Secondly, sentinel sites are a specific
location within a district, such as Nagongera which is a sentinel site
based in the Tororo district.</p>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uganda-districts-2006-magnify" class="magnify"
onclick="magnifyFigure(&#39;uganda-districts-2006&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uganda-districts-2006-caption" class="caption-frame">
<p>Figure: Ugandan districts. Data SRTM/NASA from <a
href="https://dds.cr.usgs.gov/srtm/version2_1"
class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.</p>
</div>
</div>
<div style="text-align:right">
<span class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>
</div>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Kapchorwa_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kapchorwa-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;kapchorwa-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kapchorwa-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kapchorwa District, home district of Stephen
Kiprotich.</p>
</div>
</div>
<p>Stephen Kiprotich, the 2012 gold medal winner from the London
Olympics, comes from Kapchorwa district, in eastern Uganda, near the
border with Kenya.</p>
<p>The common standard for collecting health data on the African
continent is from the Health management information systems (HMIS).
However, this data suffers from missing values <span class="citation"
data-cites="Gething:hmis06">(Gething et al., 2006)</span> and diagnosis
of diseases like typhoid and malaria may be confounded.</p>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="tororo-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;tororo-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="tororo-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Tororo district, where the sentinel site, Nagongera, is
located.</p>
</div>
</div>
<p><a
href="https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/">World
Health Organization Sentinel Surveillance systems</a> are set up “when
high-quality data are needed about a particular disease that cannot be
obtained through a passive system”. Several sentinel sites give accurate
assessment of malaria disease levels in Uganda, including a site in
Nagongera.</p>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sentinel-nagongera-magnify" class="magnify"
onclick="magnifyFigure(&#39;sentinel-nagongera&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sentinel-nagongera-caption" class="caption-frame">
<p>Figure: Sentinel and HMIS data along with rainfall and temperature
for the Nagongera sentinel station in the Tororo district.</p>
</div>
</div>
<p>In collaboration with the AI Research Group at Makerere we chose to
investigate whether Gaussian process models could be used to assimilate
information from these two different sources of disease informaton.
Further, we were interested in whether local information on rainfall and
temperature could be used to improve malaria estimates.</p>
<p>The aim of the project was to use WHO Sentinel sites, alongside
rainfall and temperature, to improve predictions from HMIS data of
levels of malaria.</p>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="mubende-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;mubende-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mubende-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Mubende District.</p>
</div>
</div>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="malaria-prediction-mubende-magnify" class="magnify"
onclick="magnifyFigure(&#39;malaria-prediction-mubende&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="malaria-prediction-mubende-caption" class="caption-frame">
<p>Figure: Prediction of malaria incidence in Mubende.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The project arose out of the Gaussian process summer school
held at Makerere in Kampala in 2013. The school led, in turn, to the
Data Science Africa initiative.</p>
</div>
</div>
<h2 id="early-warning-systems">Early Warning Systems</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kabarole-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kabarole district in Uganda.</p>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="kabarole-disease-over-time-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-disease-over-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-disease-over-time-caption" class="caption-frame">
<p>Figure: Estimate of the current disease situation in the Kabarole
district over time. Estimate is constructed with a Gaussian process with
an additive covariance funciton.</p>
</div>
</div>
<p>Health monitoring system for the Kabarole district. Here we have
fitted the reports with a Gaussian process with an additive covariance
function. It has two components, one is a long time scale component (in
red above) the other is a short time scale component (in blue).</p>
<p>Monitoring proceeds by considering two aspects of the curve. Is the
blue line (the short term report signal) above the red (which represents
the long term trend? If so we have higher than expected reports. If this
is the case <em>and</em> the gradient is still positive (i.e. reports
are going up) we encode this with a <em>red</em> color. If it is the
case and the gradient of the blue line is negative (i.e. reports are
going down) we encode this with an <em>amber</em> color. Conversely, if
the blue line is below the red <em>and</em> decreasing, we color
<em>green</em>. On the other hand if it is below red but increasing, we
color <em>yellow</em>.</p>
<p>This gives us an early warning system for disease. Red is a bad
situation getting worse, amber is bad, but improving. Green is good and
getting better and yellow good but degrading.</p>
<p>Finally, there is a gray region which represents when the scale of
the effect is small.</p>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-warning-system-map-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-warning-system-map&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-warning-system-map-caption" class="caption-frame">
<p>Figure: The map of Ugandan districts with an overview of the Malaria
situation in each district.</p>
</div>
</div>
<p>These colors can now be observed directly on a spatial map of the
districts to give an immediate impression of the current status of the
disease across the country.</p>
<h2 id="additive-covariance">Additive Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/add-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/add-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>An additive covariance function is derived from considering the
result of summing two Gaussian processes together. If the first Gaussian
process is <span class="math inline">\(g(\cdot)\)</span>, governed by
covariance <span class="math inline">\(k_g(\cdot, \cdot)\)</span> and
the second process is <span class="math inline">\(h(\cdot)\)</span>,
governed by covariance <span class="math inline">\(k_h(\cdot,
\cdot)\)</span> then the combined process <span
class="math inline">\(f(\cdot) = g(\cdot) + h(\cdot)\)</span> is
govererned by a covariance function, <span class="math display">\[
k_f(\mathbf{ x}, \mathbf{ x}^\prime) = k_g(\mathbf{ x}, \mathbf{
x}^\prime) + k_h(\mathbf{ x}, \mathbf{ x}^\prime)
\]</span></p>
<center>
<span class="math display">\[k_f(\mathbf{ x}, \mathbf{ x}^\prime) =
k_g(\mathbf{ x}, \mathbf{ x}^\prime) + k_h(\mathbf{ x}, \mathbf{
x}^\prime)\]</span>
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/add_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="add-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;add-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="add-covariance-plot-caption" class="caption-frame">
<p>Figure: An additive covariance function formed by combining a linear
and an exponentiated quadratic covariance functions.</p>
</div>
</div>
<h2 id="analysis-of-us-birth-rates">Analysis of US Birth Rates</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/bda-forecasting.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/bda-forecasting.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Aki Vehtari
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/aki-vehtari.jpg" clip-path="url(#clip5)"/>
</svg>
</div>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="bialik-friday-the-13th-magnify" class="magnify"
onclick="magnifyFigure(&#39;bialik-friday-the-13th&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="bialik-friday-the-13th-caption" class="caption-frame">
<p>Figure: This is a retrospective analysis of US births by Aki Vehtari.
The challenges of forecasting. Even with seasonal and weekly effects
removed there are significant effects on holidays, weekends, etc.</p>
</div>
</div>
<p>There’s a nice analysis of US birth rates by Gaussian processes with
additive covariances in <span class="citation"
data-cites="Gelman:bayesian13">Gelman et al. (2013)</span>. A
combination of covariance functions are used to take account of weekly
and yearly trends. The analysis is summarized on the cover of the
book.</p>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="bayesian-data-analysis-magnify" class="magnify"
onclick="magnifyFigure(&#39;bayesian-data-analysis&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="bayesian-data-analysis-caption" class="caption-frame">
<p>Figure: Two different editions of Bayesian Data Analysis <span
class="citation" data-cites="Gelman:bayesian13">(Gelman et al.,
2013)</span>.</p>
</div>
</div>
<h2 id="basis-function-covariance">Basis Function Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/basis-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/basis-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The fixed basis function covariance just comes from the properties of
a multivariate Gaussian, if we decide <span class="math display">\[
\mathbf{ f}=\boldsymbol{ \Phi}\mathbf{ w}
\]</span> and then we assume <span class="math display">\[
\mathbf{ w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right)
\]</span> then it follows from the properties of a multivariate Gaussian
that <span class="math display">\[
\mathbf{ f}\sim \mathcal{N}\left(\mathbf{0},\alpha\boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top\right)
\]</span> meaning that the vector of observations from the function is
jointly distributed as a Gaussian process and the covariance matrix is
<span class="math inline">\(\mathbf{K}= \alpha\boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top\)</span>, each element of the covariance
matrix can then be found as the inner product between two rows of the
basis funciton matrix.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> basis_cov</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> radial</span></code></pre></div>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\boldsymbol{ \phi}(\mathbf{ x})^\top \boldsymbol{ \phi}(\mathbf{
x}^\prime)\]</span>
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/basis_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="basis-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;basis-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="basis-covariance-plot-caption" class="caption-frame">
<p>Figure: A covariance function based on a non-linear basis given by
<span class="math inline">\(\boldsymbol{ \phi}(\mathbf{
x})\)</span>.</p>
</div>
</div>
<h2 id="brownian-covariance">Brownian Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/brownian-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/brownian-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> brownian_cov</span></code></pre></div>
<p>Brownian motion is also a Gaussian process. It follows a Gaussian
random walk, with diffusion occuring at each time point driven by a
Gaussian input. This implies it is both Markov and Gaussian. The
covariance function for Brownian motion has the form <span
class="math display">\[
k(t, t^\prime)=\alpha \min(t, t^\prime)
\]</span></p>
<center>
<span class="math display">\[k(t, t^\prime)=\alpha \min(t,
t^\prime)\]</span>
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/brownian_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="brownian-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;brownian-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="brownian-covariance-plot-caption" class="caption-frame">
<p>Figure: Brownian motion covariance function.</p>
</div>
</div>
<h2 id="mlp-covariance">MLP Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/mlp-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/mlp-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> mlp_cov</span></code></pre></div>
<p>The multi-layer perceptron (MLP) covariance, also known as the neural
network covariance or the arcsin covariance, is derived by considering
the infinite limit of a neural network.</p>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\arcsin\left(\frac{w \mathbf{ x}^\top \mathbf{ x}^\prime +
b}{\sqrt{\left(w \mathbf{ x}^\top \mathbf{ x}+ b + 1\right)\left(w
\left.\mathbf{ x}^\prime\right.^\top \mathbf{ x}^\prime + b +
1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/mlp_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="mlp-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;mlp-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mlp-covariance-plot-caption" class="caption-frame">
<p>Figure: The multi-layer perceptron covariance function. This is
derived by considering the infinite limit of a neural network with
probit activation functions.</p>
</div>
</div>
<h2 id="relu-covariance">RELU Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/relu-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/relu-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> relu_cov</span></code></pre></div>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha \arcsin\left(\frac{w \mathbf{ x}^\top \mathbf{ x}^\prime + b}
{\sqrt{\left(w \mathbf{ x}^\top \mathbf{ x}+ b + 1\right)
\left(w \left.\mathbf{ x}^\prime\right.^\top \mathbf{ x}^\prime + b +
1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="relu-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/relu_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/relu_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="relu-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;relu-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="relu-covariance-plot-caption" class="caption-frame">
<p>Figure: Rectified linear unit covariance function.</p>
</div>
</div>
<h2 id="sinc-covariance">Sinc Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/sinc-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/sinc-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Another approach to developing covariance function exploits Bochner’s
theorem <span class="citation" data-cites="Bochner:book59">Bochner
(1959)</span>. Bochner’s theorem tells us that any positve filter in
Fourier space implies has an associated Gaussian process with a
stationary covariance function. The covariance function is the
<em>inverse Fourier transform</em> of the filter applied in Fourier
space.</p>
<p>For example, in signal processing, <em>band limitations</em> are
commonly applied as an assumption. For example, we may believe that no
frequency above <span class="math inline">\(w=2\)</span> exists in the
signal. This is equivalent to a rectangle function being applied as a
the filter in Fourier space.</p>
<p>The inverse Fourier transform of the rectangle function is the <span
class="math inline">\(\text{sinc}(\cdot)\)</span> function. So the sinc
is a valid covariance function, and it represents <em>band limited</em>
signals.</p>
<p>Note that other covariance functions we’ve introduced can also be
interpreted in this way. For example, the exponentiated quadratic
covariance function can be Fourier transformed to see what the implied
filter in Fourier space is. The Fourier transform of the exponentiated
quadratic is an exponentiated quadratic, so the standard EQ-covariance
implies a EQ filter in Fourier space.</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> sinc_cov</span></code></pre></div>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\text{sinc}\left(\pi w r\right)\]</span>
</center>
<div class="figure">
<div id="sinc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/sinc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/sinc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="sinc-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;sinc-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sinc-covariance-plot-caption" class="caption-frame">
<p>Figure: Sinc covariance function.</p>
</div>
</div>
<h2 id="polynomial-covariance">Polynomial Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/poly-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/poly-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha(w \mathbf{ x}^\top\mathbf{ x}^\prime + b)^d\]</span>
</center>
<div class="figure">
<div id="polynomial-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/polynomial_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="polynomial-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;polynomial-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="polynomial-covariance-plot-caption" class="caption-frame">
<p>Figure: Polynomial covariance function.</p>
</div>
</div>
<h2 id="periodic-covariance">Periodic Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/periodic-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/periodic-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha\exp\left(\frac{-2\sin(\pi rw)^2}{\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="periodic-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/periodic_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/periodic_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="periodic-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;periodic-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="periodic-covariance-plot-caption" class="caption-frame">
<p>Figure: Periodic covariance function.</p>
</div>
</div>
<h2 id="linear-model-of-coregionalization-covariance">Linear Model of
Coregionalization Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/lmc-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/lmc-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<center>
<span class="math display">\[k(i, j, \mathbf{ x}, \mathbf{ x}^\prime) =
b_{i,j} k(\mathbf{ x}, \mathbf{ x}^\prime)\]</span>
</center>
<div class="figure">
<div id="lmc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/lmc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/lmc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="lmc-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;lmc-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="lmc-covariance-plot-caption" class="caption-frame">
<p>Figure: Linear model of coregionalization covariance function.</p>
</div>
</div>
<h2 id="intrinsic-coregionalization-model-covariance">Intrinsic
Coregionalization Model Covariance</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_kern/includes/icm-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_kern/includes/icm-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> icm_cov</span></code></pre></div>
<center>
<span class="math display">\[k(i, j, \mathbf{ x}, \mathbf{ x}^\prime) =
b_{i,j} k(\mathbf{ x}, \mathbf{ x}^\prime)\]</span>
</center>
<div class="figure">
<div id="icm-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/icm_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/icm_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="icm-covariance-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;icm-covariance-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="icm-covariance-plot-caption" class="caption-frame">
<p>Figure: Intrinsic coregionalization model covariance function.</p>
</div>
</div>
<h2 id="deep-gaussian-processes">Deep Gaussian Processes</h2>
<ul>
<li><em>Deep Gaussian Processes and Variational Propagation of
Uncertainty</em> <span class="citation"
data-cites="Damianou:thesis2015">Damianou (2015)</span></li>
</ul>
<h2 id="structure-of-priors">Structure of Priors</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/mackay-bathwater.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/mackay-bathwater.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Even in the early days of Gaussian processes in machine learning, it
was understood that we were throwing something fundamental away. This is
perhaps captured best by David MacKay in his 1997 NeurIPS tutorial on
Gaussian processes, where he asked “Have we thrown out the baby with the
bathwater?”. The quote below is from his summarization paper.</p>
<blockquote>
<p>According to the hype of 1987, neural networks were meant to be
intelligent models which discovered features and patterns in data.
Gaussian processes in contrast are simply smoothing devices. How can
Gaussian processes possibly replace neural networks? What is going
on?</p>
<p><span class="citation" data-cites="MacKay:gpintroduction98">MacKay
(n.d.)</span></p>
</blockquote>
<h2 id="deep-neural-network">Deep Neural Network</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install daft</span></code></pre></div>
<div class="figure">
<div id="deep-neural-network-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn2.svg" width="70%" style=" ">
</object>
</div>
<div id="deep-neural-network-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-neural-network&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-neural-network-caption" class="caption-frame">
<p>Figure: A deep neural network. Input nodes are shown at the bottom.
Each hidden layer is the result of applying an affine transformation to
the previous layer and placing through an activation function.</p>
</div>
</div>
<p>Mathematically, each layer of a neural network is given through
computing the activation function, <span
class="math inline">\(\phi(\cdot)\)</span>, contingent on the previous
layer, or the inputs. In this way the activation functions, are composed
to generate more complex interactions than would be possible with any
single layer. <span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{
h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{
h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
<h2 id="overfitting">Overfitting</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/overfitting-low-rank.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/overfitting-low-rank.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One potential problem is that as the number of nodes in two adjacent
layers increases, the number of parameters in the affine transformation
between layers, <span class="math inline">\(\mathbf{W}\)</span>,
increases. If there are <span class="math inline">\(k_{i-1}\)</span>
nodes in one layer, and <span class="math inline">\(k_i\)</span> nodes
in the following, then that matrix contains <span
class="math inline">\(k_i k_{i-1}\)</span> parameters, when we have
layer widths in the 1000s that leads to millions of parameters.</p>
<p>One proposed solution is known as <em>dropout</em> where only a
sub-set of the neural network is trained at each iteration. An
alternative solution would be to reparameterize <span
class="math inline">\(\mathbf{W}\)</span> with its <em>singular value
decomposition</em>. <span class="math display">\[
  \mathbf{W}= \mathbf{U}\boldsymbol{ \Lambda}\mathbf{V}^\top
  \]</span> or <span class="math display">\[
  \mathbf{W}= \mathbf{U}\mathbf{V}^\top
  \]</span> where if <span class="math inline">\(\mathbf{W}\in
\Re^{k_1\times k_2}\)</span> then <span
class="math inline">\(\mathbf{U}\in \Re^{k_1\times q}\)</span> and <span
class="math inline">\(\mathbf{V}\in \Re^{k_2\times q}\)</span>, i.e. we
have a low rank matrix factorization for the weights.</p>
<div class="figure">
<div id="low-rank-mapping-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//wisuvt.svg" width="80%" style=" ">
</object>
</div>
<div id="low-rank-mapping-magnify" class="magnify"
onclick="magnifyFigure(&#39;low-rank-mapping&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="low-rank-mapping-caption" class="caption-frame">
<p>Figure: Pictorial representation of the low rank form of the matrix
<span class="math inline">\(\mathbf{W}\)</span>.</p>
</div>
</div>
<p>In practice there is evidence that deep models seek these low rank
solutions where we expect better generalisation. See e.g. <span
class="citation" data-cites="Arora-convergence19">Arora et al.
(2019)</span>;<span class="citation"
data-cites="Jacot-deeplinear21">Jacot et al. (2021)</span>.</p>
<h2 id="bottleneck-layers-in-deep-neural-networks">Bottleneck Layers in
Deep Neural Networks</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-nn-bottleneck-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck2.svg" width="70%" style=" ">
</object>
</div>
<div id="deep-nn-bottleneck-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-nn-bottleneck&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-nn-bottleneck-caption" class="caption-frame">
<p>Figure: Inserting the bottleneck layers introduces a new set of
variables.</p>
</div>
</div>
<p>Including the low rank decomposition of <span
class="math inline">\(\mathbf{W}\)</span> in the neural network, we
obtain a new mathematical form. Effectively, we are adding additional
<em>latent</em> layers, <span class="math inline">\(\mathbf{
z}\)</span>, in between each of the existing hidden layers. In a neural
network these are sometimes known as <em>bottleneck</em> layers. The
network can now be written mathematically as <span
class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{U}_1 \mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \mathbf{ h}_{1}\\
  \mathbf{ h}_{2} &amp;= \phi\left(\mathbf{U}_2 \mathbf{ z}_{2}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \mathbf{ h}_{2}\\
  \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{U}_3 \mathbf{ z}_{3}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4^\top\mathbf{ h}_{3}.
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \phi\left(\mathbf{U}_1
\mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \phi\left(\mathbf{U}_2
\mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4 ^\top \mathbf{ z}_{3}
\end{align}
\]</span></p>
<h2 id="cascade-of-gaussian-processes">Cascade of Gaussian
Processes</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/cascade-of-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/cascade-of-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Now if we replace each of these neural networks with a Gaussian
process. This is equivalent to taking the limit as the width of each
layer goes to infinity, while appropriately scaling down the
outputs.</p>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{ f}_1\left(\mathbf{ x}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{ f}_2\left(\mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{ f}_3\left(\mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ f}_4\left(\mathbf{ z}_{3}\right)
\end{align}
\]</span></p>
<h1 id="deep-learning">Deep Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<!-- No slide titles in this context -->
<h2 id="deepface">DeepFace</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deep-face-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-face&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-face-caption" class="caption-frame">
<p>Figure: The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.</p>
</div>
</div>
<p>The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span> consists
of layers that deal with <em>translation</em> invariances, known as
convolutional layers. These layers are followed by three
locally-connected layers and two fully-connected layers. Color
illustrates feature maps produced at each layer. The neural network
includes more than 120 million parameters, where more than 95% come from
the local and fully connected layers.</p>
<h3 id="deep-learning-as-pinball">Deep Learning as Pinball</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-pinball-caption" class="caption-frame">
<p>Figure: Deep learning models are composition of simple functions. We
can think of a pinball machine as an analogy. Each layer of pins
corresponds to one of the layers of functions in the model. Input data
is represented by the location of the ball from left to right when it is
dropped in from the top. Output class comes from the position of the
ball as it leaves the pins at the bottom.</p>
</div>
</div>
<p>Sometimes deep learning models are described as being like the brain,
or too complex to understand, but one analogy I find useful to help the
gist of these models is to think of them as being similar to early pin
ball machines.</p>
<p>In a deep neural network, we input a number (or numbers), whereas in
pinball, we input a ball.</p>
<p>Think of the location of the ball on the left-right axis as a single
number. Our simple pinball machine can only take one number at a time.
As the ball falls through the machine, each layer of pins can be thought
of as a different layer of ‘neurons’. Each layer acts to move the ball
from left to right.</p>
<p>In a pinball machine, when the ball gets to the bottom it might fall
into a hole defining a score, in a neural network, that is equivalent to
the decision: a classification of the input object.</p>
<p>An image has more than one number associated with it, so it is like
playing pinball in a <em>hyper-space</em>.</p>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-initialization-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-initialization&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-initialization-caption" class="caption-frame">
<p>Figure: At initialization, the pins, which represent the parameters
of the function, aren’t in the right place to bring the balls to the
correct decisions.</p>
</div>
</div>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-trained-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-trained&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-trained-caption" class="caption-frame">
<p>Figure: After learning the pins are now in the right place to bring
the balls to the correct decisions.</p>
</div>
</div>
<p>Learning involves moving all the pins to be in the correct position,
so that the ball ends up in the right place when it’s fallen through the
machine. But moving all these pins in hyperspace can be difficult.</p>
<p>In a hyper-space you have to put a lot of data through the machine
for to explore the positions of all the pins. Even when you feed many
millions of data points through the machine, there are likely to be
regions in the hyper-space where no ball has passed. When future test
data passes through the machine in a new route unusual things can
happen.</p>
<p><em>Adversarial examples</em> exploit this high dimensional space. If
you have access to the pinball machine, you can use gradient methods to
find a position for the ball in the hyper space where the image looks
like one thing, but will be classified as another.</p>
<p>Probabilistic methods explore more of the space by considering a
range of possible paths for the ball through the machine. This helps to
make them more data efficient and gives some robustness to adversarial
examples.</p>
<p>Mathematically, a deep Gaussian process can be seen as a composite
<em>multivariate</em> function, <span class="math display">\[
  \mathbf{g}(\mathbf{ x})=\mathbf{ f}_5(\mathbf{ f}_4(\mathbf{
f}_3(\mathbf{ f}_2(\mathbf{ f}_1(\mathbf{ x}))))).
  \]</span> Or if we view it from the probabilistic perspective we can
see that a deep Gaussian process is specifying a factorization of the
joint density, the standard deep model takes the form of a Markov
chain.</p>
<p><span class="math display">\[
  p(\mathbf{ y}|\mathbf{ x})= p(\mathbf{ y}|\mathbf{ f}_5)p(\mathbf{
f}_5|\mathbf{ f}_4)p(\mathbf{ f}_4|\mathbf{ f}_3)p(\mathbf{
f}_3|\mathbf{ f}_2)p(\mathbf{ f}_2|\mathbf{ f}_1)p(\mathbf{
f}_1|\mathbf{ x})
  \]</span></p>
<div class="figure">
<div id="deep-markov-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov.svg" width="80%" style=" ">
</object>
</div>
<div id="deep-markov-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-markov&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-markov-caption" class="caption-frame">
<p>Figure: Probabilistically the deep Gaussian process can be
represented as a Markov chain. Indeed they can even be analyzed in this
way <span class="citation" data-cites="Dunlop:deep2017">(Dunlop et al.,
n.d.)</span>.</p>
</div>
</div>
<div class="figure">
<div id="deep-markov-vertical-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical.svg" width="7%" style=" ">
</object>
</div>
<div id="deep-markov-vertical-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-markov-vertical&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-markov-vertical-caption" class="caption-frame">
<p>Figure: More usually deep probabilistic models are written vertically
rather than horizontally as in the Markov chain.</p>
</div>
</div>
<h2 id="why-composition">Why Composition?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/process-composition.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/process-composition.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>If the result of composing many functions together is simply another
function, then why do we bother? The key point is that we can change the
class of functions we are modeling by composing in this manner. A
Gaussian process is specifying a prior over functions, and one with a
number of elegant properties. For example, the derivative process (if it
exists) of a Gaussian process is also Gaussian distributed. That makes
it easy to assimilate, for example, derivative observations. But that
also might raise some alarm bells. That implies that the <em>marginal
derivative distribution</em> is also Gaussian distributed. If that’s the
case, then it means that functions which occasionally exhibit very large
derivatives are hard to model with a Gaussian process. For example, a
function with jumps in.</p>
<p>A one off discontinuity is easy to model with a Gaussian process, or
even multiple discontinuities. They can be introduced in the mean
function, or independence can be forced between two covariance functions
that apply in different areas of the input space. But in these cases we
will need to specify the number of discontinuities and where they occur.
In otherwords we need to <em>parameterise</em> the discontinuities. If
we do not know the number of discontinuities and don’t wish to specify
where they occur, i.e. if we want a non-parametric representation of
discontinuities, then the standard Gaussian process doesn’t help.</p>
<h2 id="stochastic-process-composition">Stochastic Process
Composition</h2>
<p>The deep Gaussian process leads to <em>non-Gaussian</em> models, and
non-Gaussian characteristics in the covariance function. In effect, what
we are proposing is that we change the properties of the functions we
are considering by <em>composing stochastic processes</em>. This is an
approach to creating new stochastic processes from well known
processes.</p>
<p>Additionally, we are not constrained to the formalism of the chain.
For example, we can easily add single nodes emerging from some point in
the depth of the chain. This allows us to combine the benefits of the
graphical modelling formalism, but with a powerful framework for
relating one set of variables to another, that of Gaussian processes</p>
<div class="figure">
<div id="deep-markov-vertical-side-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical-side.svg" width="15%" style=" ">
</object>
</div>
<div id="deep-markov-vertical-side-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-markov-vertical-side&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-markov-vertical-side-caption" class="caption-frame">
<p>Figure: More generally we aren’t constrained by the Markov chain. We
can design structures that respect our belief about the underlying
conditional dependencies. Here we are adding a side note from the
chain.</p>
</div>
</div>
<h2 id="difficulty-for-probabilistic-approaches">Difficulty for
Probabilistic Approaches</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_dimred/includes/non-linear-difficulty.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/non-linear-difficulty.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The challenge for composition of probabilistic models is that you
need to propagate a probability densities through non linear mappings.
This allows you to create broader classes of probability density.
Unfortunately it renders the resulting densities
<em>intractable</em>.</p>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-3d-plot.svg" width="60%" style=" ">
</object>
</div>
<div id="nonlinear-mapping-3d-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;nonlinear-mapping-3d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nonlinear-mapping-3d-plot-caption" class="caption-frame">
<p>Figure: A two dimensional grid mapped into three dimensions to form a
two dimensional manifold.</p>
</div>
</div>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-2d-plot.svg" width="60%" style=" ">
</object>
</div>
<div id="non-linear-mapping-2d-plot-magnify" class="magnify"
onclick="magnifyFigure(&#39;non-linear-mapping-2d-plot&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="non-linear-mapping-2d-plot-caption" class="caption-frame">
<p>Figure: A one dimensional line mapped into two dimensions by two
separate independent functions. Each point can be mapped exactly through
the mappings.</p>
</div>
</div>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
<div id="gaussian-through-nonlinear-magnify" class="magnify"
onclick="magnifyFigure(&#39;gaussian-through-nonlinear&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gaussian-through-nonlinear-caption" class="caption-frame">
<p>Figure: A Gaussian density over the input of a non linear function
leads to a very non Gaussian output. Here the output is multimodal.</p>
</div>
</div>
<h2 id="standard-variational-approach-fails">Standard Variational
Approach Fails</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gplvm/includes/variational-bayes-gplvm-long.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gplvm/includes/variational-bayes-gplvm-long.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<ul>
<li>Standard variational bound has the form: <span
class="math display">\[
\mathcal{L}= \left\langle\log p(\mathbf{
y}|\mathbf{Z})\right\rangle_{q(\mathbf{Z})} + \text{KL}\left(
q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
<p>The standard variational approach would require the expectation of
<span class="math inline">\(\log p(\mathbf{ y}|\mathbf{Z})\)</span>
under <span class="math inline">\(q(\mathbf{Z})\)</span>. <span
class="math display">\[
  \begin{align}
  \log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{
y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{
f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log
\det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}}
-\frac{n}{2}\log 2\pi
  \end{align}
  \]</span> But this is extremely difficult to compute because <span
class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}\)</span> is
dependent on <span class="math inline">\(\mathbf{Z}\)</span> and it
appears in the inverse.</p>
<h2 id="variational-bayesian-gp-lvm">Variational Bayesian GP-LVM</h2>
<p>The alternative approach is to consider the collapsed variational
bound (used for low rank (sparse is a misnomer) Gaussian process
approximations. <span class="math display">\[
    p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{
f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{
u}
  \]</span> <span class="math display">\[
    p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int
\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
  \]</span> <span class="math display">\[
      \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq
\int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
  \]</span></p>
<p>To integrate across <span class="math inline">\(\mathbf{Z}\)</span>
we apply the lower bound to the inner integral. <span
class="math display">\[
    \begin{align}
    \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp;
\left\langle\sum_{i=1}^n\log  c_i\right\rangle_{q(\mathbf{Z})}\\ &amp;
+\left\langle\log\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)\right\rangle_{q(\mathbf{Z})}\\&amp;
+ \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
    \end{align}
  \]</span> * Which is analytically tractable for Gaussian <span
class="math inline">\(q(\mathbf{Z})\)</span> and some covariance
functions.</p>
<ul>
<li><p>Need expectations under <span
class="math inline">\(q(\mathbf{Z})\)</span> of: <span
class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{
u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i,
\mathbf{ u}}\right]
\]</span> and <span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{
u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log
2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f},
\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{
u}\right)^2
\]</span></p></li>
<li><p>This requires the expectations <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{
u}}\right\rangle_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{
u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{
f}}\right\rangle_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance
functions <span class="citation"
data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or
through sampling <span class="citation"
data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015;
Salimbeni and Deisenroth, 2017)</span>.</p></li>
</ul>
<p>Variational approximations aren’t the only approach to approximate
inference. The original work on deep Gaussian processes made use of MAP
approximations <span class="citation"
data-cites="Lawrence:hgplvm07">(Lawrence and Moore, 2007)</span>, which
couldn’t propagate the uncertainty through the model at the data points
but sustain uncertainty elsewhere. Since the variational approximation
was proposed researchers have also considered sampling approaches <span
class="citation" data-cites="Havasi:deepgp18">(Havasi et al.,
2018)</span> and expectation propagation <span class="citation"
data-cites="Bui:deep16">(Bui et al., 2016)</span>.</p>
<div class="figure">
<div id="neural-network-uncertainty-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepgp/neural-network-uncertainty.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="neural-network-uncertainty-magnify" class="magnify"
onclick="magnifyFigure(&#39;neural-network-uncertainty&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="neural-network-uncertainty-caption" class="caption-frame">
<p>Figure: Even the latest work on Bayesian neural networks has severe
problems handling uncertainty. In this example, <span class="citation"
data-cites="Izmailov:subspace19">(Izmailov et al., 2019)</span>, methods
even fail to interpolate through the data correctly or provide well
calibrated error bars in regions where data is observed.</p>
</div>
</div>
<p>The argument in the deep learning revolution is that deep
architectures allow us to develop an abstraction of the feature set
through model composition. Composing Gaussian processes is analytically
intractable. To form deep Gaussian processes we use a variational
approach to stack the models.</p>
<h2 id="stacked-pca">Stacked PCA</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-pca.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-pca.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="stack-pca-sample-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-4.svg" width="20%" style=" ">
</object>
</div>
<div id="stack-pca-sample-magnify" class="magnify"
onclick="magnifyFigure(&#39;stack-pca-sample&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="stack-pca-sample-caption" class="caption-frame">
<p>Figure: Composition of linear functions just leads to a new linear
function. Here you see the result of multiple affine transformations
applied to a square in two dimensions.</p>
</div>
</div>
<p>Stacking a series of linear functions simply leads to a new linear
function. The use of multiple linear function merely changes the
covariance of the resulting Gaussian. If <span class="math display">\[
\mathbf{Z}\sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right)
\]</span> and the <span class="math inline">\(i\)</span>th hidden layer
is a multivariate linear transformation defined by <span
class="math inline">\(\mathbf{W}_i\)</span>, <span
class="math display">\[
\mathbf{Y}= \mathbf{Z}\mathbf{W}_1 \mathbf{W}_2 \dots \mathbf{W}_\ell
\]</span> then the rules of multivariate Gaussians tell us that <span
class="math display">\[
\mathbf{Y}\sim \mathcal{N}\left(\mathbf{0},\mathbf{W}_\ell\dots
\mathbf{W}_1 \mathbf{W}^\top_1 \dots \mathbf{W}^\top_\ell\right).
\]</span> So the model can be replaced by one where we set <span
class="math inline">\(\mathbf{V}= \mathbf{W}_\ell\dots \mathbf{W}_2
\mathbf{W}_1\)</span>. So is such a model trivial? The answer is that it
depends. There are two cases in which such a model remaisn interesting.
Firstly, if we make intermediate observations stemming from the chain.
So, for example, if we decide that, <span class="math display">\[
\mathbf{Z}_i = \mathbf{W}_i \mathbf{Z}_{i-1}
\]</span> and set <span class="math inline">\(\mathbf{Z}_{0} =
\mathbf{X}\sim \mathcal{N}\left(\mathbf{0},\mathbf{I}\right)\)</span>,
then the matrices <span class="math inline">\(\mathbf{W}\)</span>
inter-relate a series of jointly Gaussian observations in an interesting
way, stacking the full data matrix to give <span class="math display">\[
\mathbf{Z}= \begin{bmatrix}
\mathbf{Z}_0 \\
\mathbf{Z}_1 \\
\vdots \\
\mathbf{Z}_\ell
\end{bmatrix}
\]</span> we can obtain <span class="math display">\[\mathbf{Z}\sim
\mathcal{N}\left(\mathbf{0},\begin{bmatrix}
\mathbf{I}&amp; \mathbf{W}^\top_1 &amp;
\mathbf{W}_1^\top\mathbf{W}_2^\top &amp; \dots &amp; \mathbf{V}^\top \\
\mathbf{W}_1 &amp; \mathbf{W}_1 \mathbf{W}_1^\top &amp; \mathbf{W}_1
\mathbf{W}_1^\top \mathbf{W}_2^\top &amp; \dots &amp; \mathbf{W}_1
\mathbf{V}^\top \\
\mathbf{W}_2 \mathbf{W}_1 &amp; \mathbf{W}_2 \mathbf{W}_1
\mathbf{W}_1^\top &amp; \mathbf{W}_2 \mathbf{W}_1 \mathbf{W}_1^\top
\mathbf{W}_2^\top &amp; \dots &amp; \mathbf{W}_2 \mathbf{W}_1
\mathbf{V}^\top \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{V}&amp; \mathbf{V}\mathbf{W}_1^\top  &amp;
\mathbf{V}\mathbf{W}_1^\top \mathbf{W}_2^\top&amp; \dots &amp;
\mathbf{V}\mathbf{V}^\top
\end{bmatrix}\right)\]</span> which is a highly structured Gaussian
covariance with hierarchical dependencies between the variables <span
class="math inline">\(\mathbf{Z}_i\)</span>.</p>
<h2 id="stacked-gp">Stacked GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/stacked-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="stack-gp-sample-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-4.svg" width="20%" style=" ">
</object>
</div>
<div id="stack-gp-sample-magnify" class="magnify"
onclick="magnifyFigure(&#39;stack-gp-sample&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="stack-gp-sample-caption" class="caption-frame">
<p>Figure: Stacking Gaussian process models leads to non linear mappings
at each stage. Here we are mapping from two dimensions to two dimensions
in each layer.</p>
</div>
</div>
<p>Note that once the box has folded over on itself, it cannot be
unfolded. So a feature that is generated near the top of the model
cannot be removed further down the model.</p>
<p>This folding over effect happens in low dimensions. In higher
dimensions it is less common.</p>
<p>Observation of this effect at a talk in Cambridge was one of the
things that caused David Duvenaud (and collaborators) to consider the
behavior of deeper Gaussian process models <span class="citation"
data-cites="Duvenaud:pathologies14">(Duvenaud et al., 2014)</span>.</p>
<p>Such folding over in the latent spaces necessarily forces the density
to be non-Gaussian. Indeed, since folding-over is avoided as we increase
the dimensionality of the latent spaces, such processes become more
Gaussian. If we take the limit of the latent space dimensionality as it
tends to infinity, the entire deep Gaussian process returns to a
standard Gaussian process, with a covariance function given as a deep
kernel (such as those described by <span class="citation"
data-cites="Cho:deep09">Cho and Saul (2009)</span>).</p>
<p>Further analysis of these deep networks has been conducted by <span
class="citation" data-cites="Dunlop:deep2017">Dunlop et al.
(n.d.)</span>, who use analysis of the deep network’s stationary density
(treating it as a Markov chain across layers), to explore the nature of
the implied process prior for a deep GP.</p>
<p>Both of these works, however, make constraining assumptions on the
form of the Gaussian process prior at each layer (e.g. same covariance
at each layer). In practice, the form of this covariance can be learnt
and the densities described by the deep GP are more general than those
mentioned in either of these papers.</p>
<h2 id="stacked-gps-video-by-david-duvenaud">Stacked GPs (video by David
Duvenaud)</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-pathologies.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/deep-pathologies.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="visualization-deep-gp-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/XhIvygQYFFQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="visualization-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;visualization-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="visualization-deep-gp-caption" class="caption-frame">
<p>Figure: Visualization of mapping of a two dimensional space through a
deep Gaussian process.</p>
</div>
</div>
<p>David Duvenaud also created a YouTube video to help visualize what
happens as you drop through the layers of a deep GP.</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian
Process Framework in Python</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Gaussian processes are a flexible tool for non-parametric analysis
with uncertainty. The GPy software was started in Sheffield to provide a
easy to use interface to GPs. One which allowed the user to focus on the
modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify"
onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing
Gaussian process models in Python. It is designed for teaching and
modelling. We welcome contributions which can be made through the GitHub
repository <a href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian
process models in python. This allows GPs to be combined with a wide
variety of software libraries.</p>
<p>The software itself is available on <a
href="https://github.com/SheffieldML/GPy">GitHub</a> and the team
welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language,
i.e., you specify the model rather than the algorithm. As well as a
large range of covariance functions the software allows for non-Gaussian
likelihoods, multivariate outputs, dimensionality reduction and
approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a
href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<p>This notebook depends on PyDeepGP. This library can be installed via
pip.</p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>SheffieldML<span class="op">/</span>PyDeepGP.git</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install mlai</span></code></pre></div>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Late bind setup methods to DeepGP object</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> initialize</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> staged_optimize</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> posterior_sample</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> visualize</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai.deepgp_tutorial <span class="im">import</span> visualize_pinball</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.initialize<span class="op">=</span>initialize</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.staged_optimize<span class="op">=</span>staged_optimize</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.posterior_sample<span class="op">=</span>posterior_sample</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.visualize<span class="op">=</span>visualize</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>deepgp.DeepGP.visualize_pinball<span class="op">=</span>visualize_pinball</span></code></pre></div>
<h2 id="deep-gp-fit">Deep GP Fit</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/olympic-marathon-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/olympic-marathon-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Let’s see if a deep Gaussian process can help here. We will construct
a deep Gaussian process with one hidden layer (i.e. one Gaussian process
feeding into another).</p>
<p>Build a Deep GP with an additional hidden layer (one dimensional) to
fit the model.</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span></code></pre></div>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>hidden <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP([y.shape[<span class="dv">1</span>],hidden,x.shape[<span class="dv">1</span>]],Y<span class="op">=</span>yhat, X<span class="op">=</span>x, inits<span class="op">=</span>[<span class="st">&#39;PCA&#39;</span>,<span class="st">&#39;PCA&#39;</span>], </span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>[GPy.kern.RBF(hidden,ARD<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>                           GPy.kern.RBF(x.shape[<span class="dv">1</span>],ARD<span class="op">=</span><span class="va">True</span>)], <span class="co"># the kernels for each layer</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">50</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Call the initalization</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>m.initialize()</span></code></pre></div>
<p>Now optimize the model.</p>
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    layer.likelihood.variance.constrain_positive(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">True</span>,max_iters<span class="op">=</span><span class="dv">10000</span>)</span></code></pre></div>
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>m.staged_optimize(messages<span class="op">=</span>(<span class="va">True</span>,<span class="va">True</span>,<span class="va">True</span>))</span></code></pre></div>
<h2 id="olympic-marathon-data-deep-gp">Olympic Marathon Data Deep
GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp.svg" width="100%" style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-caption" class="caption-frame">
<p>Figure: Deep GP fit to the Olympic marathon data. Error bars now
change as the prediction evolves.</p>
</div>
</div>
<h2 id="olympic-marathon-data-deep-gp-1">Olympic Marathon Data Deep
GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-samples.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-samples-caption"
class="caption-frame">
<p>Figure: Point samples run through the deep Gaussian process show the
distribution of output locations.</p>
</div>
</div>
<h2 id="fitted-gp-for-each-layer">Fitted GP for each layer</h2>
<p>Now we explore the GPs the model has used to fit each layer. First of
all, we look at the hidden layer.</p>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-0.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-layer-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-layer-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-layer-0-caption"
class="caption-frame">
<p>Figure: The mapping from input to the latent layer is broadly, with
some flattening as time goes on. Variance is high across the input
range.</p>
</div>
</div>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-1.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-layer-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-layer-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-layer-1-caption"
class="caption-frame">
<p>Figure: The mapping from the latent layer to the output layer.</p>
</div>
</div>
<h2 id="olympic-marathon-pinball-plot">Olympic Marathon Pinball
Plot</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-pinball.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-deep-gp-pinball-caption"
class="caption-frame">
<p>Figure: A pinball plot shows the movement of the ‘ball’ as it passes
through each layer of the Gaussian processes. Mean directions of
movement are shown by lines. Shading gives one standard deviation of
movement position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. There is
some grouping of later points towards the right in the first layer,
which also injects a large amount of uncertainty. Due to flattening of
the curve in the second layer towards the right the uncertainty is
reduced in final output.</p>
</div>
</div>
<p>The pinball plot shows the flow of any input ball through the deep
Gaussian process. In a pinball plot a series of vertical parallel lines
would indicate a purely linear function. For the olypmic marathon data
we can see the first layer begins to shift from input towards the right.
Note it also does so with some uncertainty (indicated by the shaded
backgrounds). The second layer has less uncertainty, but bunches the
inputs more strongly to the right. This input layer of uncertainty,
followed by a layer that pushes inputs to the right is what gives the
heteroschedastic noise.</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">1</span>,x.shape[<span class="dv">1</span>]]</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i)]</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>yhat, X<span class="op">=</span>x, </span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>                  inits<span class="op">=</span>inits, </span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels, <span class="co"># the kernels for each layer</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">20</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>m.initialize()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>m.staged_optimize()</span></code></pre></div>
<h2 id="della-gatta-gene-data-deep-gp">Della Gatta Gene Data Deep
GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/della-gatta-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/della-gatta-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="della-gatta-gene-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-caption" class="caption-frame">
<p>Figure: Deep Gaussian process fit to the Della Gatta gene expression
data.</p>
</div>
</div>
<h2 id="della-gatta-gene-data-deep-gp-1">Della Gatta Gene Data Deep
GP</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-samples.svg" width style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-samples-caption"
class="caption-frame">
<p>Figure: Deep Gaussian process samples fitted to the Della Gatta gene
expression data.</p>
</div>
</div>
<h2 id="della-gatta-gene-data-latent-1">Della Gatta Gene Data Latent
1</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-0.svg" width="50%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-layer-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-layer-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-layer-0-caption"
class="caption-frame">
<p>Figure: Gaussian process mapping from input to latent layer for the
della Gatta gene expression data.</p>
</div>
</div>
<h2 id="della-gatta-gene-data-latent-2">Della Gatta Gene Data Latent
2</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-1.svg" width="50%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-layer-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-layer-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-layer-1-caption"
class="caption-frame">
<p>Figure: Gaussian process mapping from latent to output layer for the
della Gatta gene expression data.</p>
</div>
</div>
<h2 id="tp53-gene-pinball-plot">TP53 Gene Pinball Plot</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
<div id="della-gatta-gene-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;della-gatta-gene-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="della-gatta-gene-deep-gp-pinball-caption"
class="caption-frame">
<p>Figure: A pinball plot shows the movement of the ‘ball’ as it passes
through each layer of the Gaussian processes. Mean directions of
movement are shown by lines. Shading gives one standard deviation of
movement position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. Pinball
plot of the della Gatta gene expression data.</p>
</div>
</div>
<h2 id="step-function">Step Function</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/step-function-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/step-function-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Next we consider a simple step function data set.</p>
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>num_low<span class="op">=</span><span class="dv">25</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>num_high<span class="op">=</span><span class="dv">25</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>gap <span class="op">=</span> <span class="op">-</span><span class="fl">.1</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>noise<span class="op">=</span><span class="fl">0.0001</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.vstack((np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span>gap<span class="op">/</span><span class="fl">2.0</span>, num_low)[:, np.newaxis],</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>              np.linspace(gap<span class="op">/</span><span class="fl">2.0</span>, <span class="dv">1</span>, num_high)[:, np.newaxis]))</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.vstack((np.zeros((num_low, <span class="dv">1</span>)), np.ones((num_high,<span class="dv">1</span>))))</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y<span class="op">-</span>offset)<span class="op">/</span>scale</span></code></pre></div>
<h2 id="step-function-data">Step Function Data</h2>
<div class="figure">
<div id="step-function-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/step-function.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-data-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-data-caption" class="caption-frame">
<p>Figure: Simulation study of step function data artificially
generated. Here there is a small overlap between the two lines.</p>
</div>
</div>
<h2 id="step-function-data-gp">Step Function Data GP</h2>
<p>We can fit a Gaussian process to the step function data using
<code>GPy</code> as follows.</p>
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<p>Where <code>GPy.models.GPRegression()</code> gives us a standard GP
regression model with exponentiated quadratic covariance function.</p>
<p>The model is optimized using <code>m_full.optimize()</code> which
calls an L-BGFS gradient based solver in python.</p>
<div class="figure">
<div id="step-function-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/step-function-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the step function data. Note the
large error bars and the over-smoothing of the discontinuity. Error bars
are shown at two standard deviations.</p>
</div>
</div>
<p>The resulting fit to the step function data shows some challenges. In
particular, the over smoothing at the discontinuity. If we know how many
discontinuities there are, we can parameterize them in the step
function. But by doing this, we form a semi-parametric model. The
parameters indicate how many discontinuities are, and where they are.
They can be optimized as part of the model fit. But if new, unforeseen,
discontinuities arise when the model is being deployed in practice,
these won’t be accounted for in the predictions.</p>
<h2 id="step-function-data-deep-gp">Step Function Data Deep GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/step-function-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/step-function-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>First we initialize a deep Gaussian process with three latent layers
(four layers total). Within each layer we create a GP with an
exponentiated quadratic covariance (<code>GPy.kern.RBF</code>).</p>
<p>At each layer we use 20 inducing points for the variational
approximation.</p>
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>,x.shape[<span class="dv">1</span>]]</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i)]</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>yhat, X<span class="op">=</span>x, </span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>                  inits<span class="op">=</span>inits, </span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels, <span class="co"># the kernels for each layer</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">20</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p>Once the model is constructed we initialize the parameters, and
perform the staged optimization which starts by optimizing variational
parameters with a low noise and proceeds to optimize the whole
model.</p>
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>m.initialize()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>m.staged_optimize()</span></code></pre></div>
<p>We plot the output of the deep Gaussian process fitted to the step
data as follows.</p>
<p>The deep Gaussian process does a much better job of fitting the data.
It handles the discontinuity easily, and error bars drop to smaller
values in the regions of data.</p>
<div class="figure">
<div id="step-function-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-caption" class="caption-frame">
<p>Figure: Deep Gaussian process fit to the step function data.</p>
</div>
</div>
<h2 id="step-function-data-deep-gp-1">Step Function Data Deep GP</h2>
<p>The samples of the model can be plotted with the helper function from
<code>mlai.plot</code>, <code>model_sample</code></p>
<p>The samples from the model show that the error bars, which are
informative for Gaussian outputs, are less informative for this model.
They make clear that the data points lie, in output mainly at 0 or 1, or
occasionally in between.</p>
<div class="figure">
<div id="step-function-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-samples-caption" class="caption-frame">
<p>Figure: Samples from the deep Gaussian process model for the step
function fit.</p>
</div>
</div>
<p>The visualize code allows us to inspect the intermediate layers in
the deep GP model to understand how it has reconstructed the step
function.</p>
<div class="figure">
<div id="step-function-deep-gp-mappings-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-2.svg" width="60%" style=" ">
</object>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-3.svg" width="60%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-mappings-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp-mappings&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-mappings-caption" class="caption-frame">
<p>Figure: From top to bottom, the Gaussian process mapping function
that makes up each layer of the resulting deep Gaussian process.</p>
</div>
</div>
<p>A pinball plot can be created for the resulting model to understand
how the input is being translated to the output across the different
layers.</p>
<div class="figure">
<div id="step-function-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
<div id="step-function-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;step-function-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="step-function-deep-gp-pinball-caption" class="caption-frame">
<p>Figure: Pinball plot of the deep GP fitted to the step function data.
Each layer of the model pushes the ‘ball’ towards the left or right,
saturating at 1 and 0. This causes the final density to be be peaked at
0 and 1. Transitions occur driven by the uncertainty of the mapping in
each layer.</p>
</div>
</div>
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.mcycle()</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>scale<span class="op">=</span>np.sqrt(y.var())</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>offset<span class="op">=</span>y.mean()</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<h2 id="motorcycle-helmet-data">Motorcycle Helmet Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/motorcycle-helmet-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/motorcycle-helmet-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="motorcycle-helment-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/motorcycle-helmet.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helment-data-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helment-data&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helment-data-caption" class="caption-frame">
<p>Figure: Motorcycle helmet data. The data consists of acceleration
readings on a motorcycle helmet undergoing a collision. The data
exhibits heteroschedastic (time varying) noise levles and
non-stationarity.</p>
</div>
</div>
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<h2 id="motorcycle-helmet-data-gp">Motorcycle Helmet Data GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_gp/includes/motorcycle-helmet-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_gp/includes/motorcycle-helmet-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="motorcycle-helmet-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/motorcycle-helmet-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the motorcycle helmet accelerometer
data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-deep-gp">Motorcycle Helmet Data Deep
GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/motorcycle-helmet-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/motorcycle-helmet-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb56"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span></code></pre></div>
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">1</span>, x.shape[<span class="dv">1</span>]]</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i)]</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>yhat, X<span class="op">=</span>x, </span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>                  inits<span class="op">=</span>inits, </span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels, <span class="co"># the kernels for each layer</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">20</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>m.initialize()</span></code></pre></div>
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>m.staged_optimize(iters<span class="op">=</span>(<span class="dv">1000</span>,<span class="dv">1000</span>,<span class="dv">10000</span>), messages<span class="op">=</span>(<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>))</span></code></pre></div>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-caption" class="caption-frame">
<p>Figure: Deep Gaussian process fit to the motorcycle helmet
accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-deep-gp-1">Motorcycle Helmet Data Deep
GP</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-samples-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-samples-caption"
class="caption-frame">
<p>Figure: Samples from the deep Gaussian process as fitted to the
motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-latent-1">Motorcycle Helmet Data Latent
1</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-layer-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-layer-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-layer-0-caption"
class="caption-frame">
<p>Figure: Mappings from the input to the latent layer for the
motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-data-latent-2">Motorcycle Helmet Data Latent
2</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-layer-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-layer-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-layer-1-caption"
class="caption-frame">
<p>Figure: Mappings from the latent layer to the output layer for the
motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="motorcycle-helmet-pinball-plot">Motorcycle Helmet Pinball
Plot</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
<div id="motorcycle-helmet-deep-gp-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;motorcycle-helmet-deep-gp-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="motorcycle-helmet-deep-gp-pinball-caption"
class="caption-frame">
<p>Figure: Pinball plot for the mapping from input to output layer for
the motorcycle helmet accelerometer data.</p>
</div>
</div>
<h2 id="robot-wireless-data">Robot Wireless Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/robot-wireless-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/robot-wireless-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The robot wireless data is taken from an experiment run by Brian
Ferris at University of Washington. It consists of the measurements of
WiFi access point signal strengths as Brian walked in a loop. It was
published at IJCAI in 2007 <span class="citation"
data-cites="Ferris:wifi07">(Ferris et al., 2007)</span>.</p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb60"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>pods.datasets.robot_wireless()</span></code></pre></div>
<p>The ground truth is recorded in the data, the actual loop is given in
the plot below.</p>
<h2 id="robot-wireless-ground-truth">Robot Wireless Ground Truth</h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
<div id="robot-wireless-ground-truth-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-ground-truth&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-ground-truth-caption" class="caption-frame">
<p>Figure: Ground truth movement for the position taken while recording
the multivariate time-course of wireless access point signal
strengths.</p>
</div>
</div>
<p>We will ignore this ground truth in making our predictions, but see
if the model can recover something similar in one of the latent
layers.</p>
<h2 id="robot-wifi-data">Robot WiFi Data</h2>
<p>One challenge with the data is that the signal strength ‘drops out’.
This is because the device only tracks a limited number of wifi access
points, when one of the access points falls outside the track, the value
disappears (in the plot below it reads -0.5). The data is missing, but
it is not missing at random because the implication is that the wireless
access point must be weak to have dropped from the list of those that
are tracked.</p>
<div class="figure">
<div id="robot-wireless-data-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-dim-1.svg" width="60%" style=" ">
</object>
</div>
<div id="robot-wireless-data-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-data-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-data-dim-1-caption" class="caption-frame">
<p>Figure: Output dimension 1 from the robot wireless data. This plot
shows signal strength changing over time.</p>
</div>
</div>
<h2 id="gaussian-process-fit-to-robot-wireless-data">Gaussian Process
Fit to Robot Wireless Data</h2>
<p>Perform a Gaussian process fit on the data using GPy.</p>
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></span></code></pre></div>
<h2 id="robot-wifi-data-gp">Robot WiFi Data GP</h2>
<div class="figure">
<div id="robot-wireless-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/robot-wireless-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
<div id="robot-wireless-gp-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-gp-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-gp-dim-1-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Robot Wireless dimension 1.</p>
</div>
</div>
<div class="sourceCode" id="cb62"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [y.shape[<span class="dv">1</span>], <span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">2</span>, x.shape[<span class="dv">1</span>]]</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>inits <span class="op">=</span> [<span class="st">&#39;PCA&#39;</span>]<span class="op">*</span>(<span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> []</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> layers[<span class="dv">1</span>:]:</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    kernels <span class="op">+=</span> [GPy.kern.RBF(i, ARD<span class="op">=</span><span class="va">True</span>)]</span></code></pre></div>
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP(layers,Y<span class="op">=</span>y, X<span class="op">=</span>x, inits<span class="op">=</span>inits, </span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>kernels,</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">50</span>, back_constraint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>m.initialize()</span></code></pre></div>
<div class="sourceCode" id="cb64"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>m.staged_optimize(messages<span class="op">=</span>(<span class="va">True</span>,<span class="va">True</span>,<span class="va">True</span>))</span></code></pre></div>
<h2 id="robot-wifi-data-deep-gp">Robot WiFi Data Deep GP</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/robot-wireless-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/robot-wireless-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="robot-wireless-deep-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
<div id="robot-wireless-deep-gp-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-deep-gp-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-deep-gp-dim-1-caption" class="caption-frame">
<p>Figure: Fit of the deep Gaussian process to dimension 1 of the robot
wireless data.</p>
</div>
</div>
<h2 id="robot-wifi-data-deep-gp-1">Robot WiFi Data Deep GP</h2>
<div class="figure">
<div id="robot-wireless-deep-gp-samples-dim-1-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-samples-dim-1.svg" width="80%" style=" ">
</object>
</div>
<div id="robot-wireless-deep-gp-samples-dim-1-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-deep-gp-samples-dim-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-deep-gp-samples-dim-1-caption"
class="caption-frame">
<p>Figure: Samples from the deep Gaussian process fit to dimension 1 of
the robot wireless data.</p>
</div>
</div>
<h2 id="robot-wifi-data-latent-space">Robot WiFi Data Latent Space</h2>
<div class="figure">
<div id="robot-wireless-latent-space-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-latent-space.svg" width="60%" style=" ">
</object>
</div>
<div id="robot-wireless-latent-space-magnify" class="magnify"
onclick="magnifyFigure(&#39;robot-wireless-latent-space&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="robot-wireless-latent-space-caption" class="caption-frame">
<p>Figure: Inferred two dimensional latent space from the model for the
robot wireless data.</p>
</div>
</div>
<h2 id="high-five-motion-capture-data">‘High Five’ Motion Capture
Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/high-five-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/high-five-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Motion capture data from the CMU motion capture data base <span
class="citation" data-cites="CMU-mocap03">(CMU Motion Capture Lab,
2003)</span>. It contains two subjects approaching each other and
executing a ‘high five’. The subjects are number 10 and 11 and their
motion numbers are 21.</p>
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb66"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.cmu_mocap_high_five()</span></code></pre></div>
<p>The data dictionary contains the keys ‘Y1’ and ‘Y2’, which represent
the motions of the two different subjects. Their skeleton files are
included in the keys ‘skel1’ and ‘skel2’.</p>
<div class="sourceCode" id="cb67"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&#39;Y1&#39;</span>].shape</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&#39;Y2&#39;</span>].shape</span></code></pre></div>
<p>The data was used in the hierarchical GP-LVM paper <span
class="citation" data-cites="Lawrence:hgplvm07">(Lawrence and Moore,
2007)</span> in an experiment that was also recreated in the Deep
Gaussian process paper <span class="citation"
data-cites="Damianou:deepgp13">(Damianou and Lawrence, 2013)</span>.</p>
<div class="sourceCode" id="cb68"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data[<span class="st">&#39;citation&#39;</span>])</span></code></pre></div>
<p>And extra information about the data is included, as standard, under
the keys <code>info</code> and <code>details</code>.</p>
<div class="sourceCode" id="cb69"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data[<span class="st">&#39;info&#39;</span>])</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data[<span class="st">&#39;details&#39;</span>])</span></code></pre></div>
<h2 id="shared-lvm">Shared LVM</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/high-five-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/high-five-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="shared-latent-variable-model-graph-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//shared.svg" width="60%" style=" ">
</object>
</div>
<div id="shared-latent-variable-model-graph-magnify" class="magnify"
onclick="magnifyFigure(&#39;shared-latent-variable-model-graph&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="shared-latent-variable-model-graph-caption"
class="caption-frame">
<p>Figure: Shared latent variable model structure. Here two related data
sets are brought together with a set of latent variables that are
partially shared and partially specific to one of the data sets.</p>
</div>
</div>
<div class="figure">
<div id="deep-gp-high-five-figure" class="figure-frame">
<p><img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deep-gp-high-five2.png" width="80%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
</div>
<div id="deep-gp-high-five-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-gp-high-five&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-gp-high-five-caption" class="caption-frame">
<p>Figure: Latent spaces of the ‘high five’ data. The structure of the
model is automatically learnt. One of the latent spaces is coordinating
how the two figures walk together, the other latent spaces contain
latent variables that are specific to each of the figures
separately.</p>
</div>
</div>
<h2 id="subsample-of-the-mnist-data">Subsample of the MNIST Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We will look at a sub-sample of the MNIST digit data set.</p>
<p>First load in the MNIST data set from scikit learn. This can take a
little while because it’s large to download.</p>
<div class="sourceCode" id="cb70"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span></code></pre></div>
<div class="sourceCode" id="cb71"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> fetch_openml(<span class="st">&#39;mnist_784&#39;</span>)</span></code></pre></div>
<p>Sub-sample the dataset to make the training faster.</p>
<div class="sourceCode" id="cb72"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb73"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>]</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>N_per_digit <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> []</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> []</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> digits:</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> mnist[<span class="st">&#39;data&#39;</span>][mnist[<span class="st">&#39;target&#39;</span>]<span class="op">==</span><span class="bu">str</span>(d)]</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    Y.append(imgs.loc[np.random.permutation(imgs.index)[:N_per_digit]])</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    labels.append(np.ones(N_per_digit)<span class="op">*</span>d)</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.vstack(Y).astype(np.float64)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.hstack(labels)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>Y <span class="op">/=</span> <span class="dv">255</span></span></code></pre></div>
<h2 id="fitting-a-deep-gp-to-a-the-mnist-digits-subsample">Fitting a
Deep GP to a the MNIST Digits Subsample</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/mnist-digits-subsample-deep-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepgp/includes/mnist-digits-subsample-deep-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip6">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Zhenwen Dai
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/zhenwen-dai.jpg" clip-path="url(#clip6)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip7">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/andreas-damianou.png" clip-path="url(#clip7)"/>
</svg>
</div>
<p>We now look at the deep Gaussian processes’ capacity to perform
unsupervised learning.</p>
<h2 id="fit-a-deep-gp">Fit a Deep GP</h2>
<p>We’re going to fit a Deep Gaussian process model to the MNIST data
with two hidden layers. Each of the two Gaussian processes (one from the
first hidden layer to the second, one from the second hidden layer to
the data) has an exponentiated quadratic covariance.</p>
<div class="sourceCode" id="cb74"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepgp</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span></code></pre></div>
<div class="sourceCode" id="cb75"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>num_latent <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>num_hidden_2 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> deepgp.DeepGP([Y.shape[<span class="dv">1</span>],num_hidden_2,num_latent],</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>                  Y,</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>                  kernels<span class="op">=</span>[GPy.kern.RBF(num_hidden_2,ARD<span class="op">=</span><span class="va">True</span>), </span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>                           GPy.kern.RBF(num_latent,ARD<span class="op">=</span><span class="va">False</span>)], </span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>                  num_inducing<span class="op">=</span><span class="dv">50</span>, back_constraint<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>                  encoder_dims<span class="op">=</span>[[<span class="dv">200</span>],[<span class="dv">200</span>]])</span></code></pre></div>
<h2 id="initialization">Initialization</h2>
<p>Just like deep neural networks, there are some tricks to
intitializing these models. The tricks we use here include some early
training of the model with model parameters constrained. This gives the
variational inducing parameters some scope to tighten the bound for the
case where the noise variance is small and the variances of the Gaussian
processes are around 1.</p>
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>m.obslayer.likelihood.variance[:] <span class="op">=</span> Y.var()<span class="op">*</span><span class="fl">0.01</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    layer.kern.variance.fix(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    layer.likelihood.variance.fix(warning<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p>We now we optimize for a hundred iterations with the constrained
model.</p>
<div class="sourceCode" id="cb77"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">False</span>,max_iters<span class="op">=</span><span class="dv">100</span>)</span></code></pre></div>
<p>Now we remove the fixed constraint on the kernel variance parameters,
but keep the noise output constrained, and run for a further 100
iterations.</p>
<div class="sourceCode" id="cb78"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    layer.kern.variance.constrain_positive(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">False</span>,max_iters<span class="op">=</span><span class="dv">100</span>)</span></code></pre></div>
<p>Finally we unconstrain the layer likelihoods and allow the full model
to be trained for 1000 iterations.</p>
<div class="sourceCode" id="cb79"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> m.layers:</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    layer.likelihood.variance.constrain_positive(warning<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>m.optimize(messages<span class="op">=</span><span class="va">True</span>,max_iters<span class="op">=</span><span class="dv">10000</span>)</span></code></pre></div>
<h2 id="visualize-the-latent-space-of-the-top-layer">Visualize the
latent space of the top layer</h2>
<p>Now the model is trained, let’s plot the mean of the posterior
distributions in the top latent layer of the model.</p>
<div class="figure">
<div id="mnist-digits-subsample-latent-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-latent.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-latent-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-latent&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-latent-caption" class="caption-frame">
<p>Figure: Latent space for the deep Gaussian process learned through
unsupervised learning and fitted to a subset of the MNIST digits
subsample.</p>
</div>
</div>
<h2 id="visualize-the-latent-space-of-the-intermediate-layer">Visualize
the latent space of the intermediate layer</h2>
<p>We can also visualize dimensions of the intermediate layer. First the
lengthscale of those dimensions is given by</p>
<div class="sourceCode" id="cb80"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>m.obslayer.kern.lengthscale</span></code></pre></div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-1-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-2-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-3-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-4-0.svg" width="60%" style=" ">
</object>
</div>
<div id="mnist-digits-subsample-hidden-1-0-magnify" class="magnify"
onclick="magnifyFigure(&#39;mnist-digits-subsample-hidden-1-0&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mnist-digits-subsample-hidden-1-0-caption"
class="caption-frame">
<p>Figure: Visualisation of the intermediate layer, plot of dimension 1
vs dimension 0.</p>
</div>
</div>
<h2 id="generate-from-model">Generate From Model</h2>
<p>Now we can take a look at a sample from the model, by drawing a
Gaussian random sample in the latent space and propagating it through
the model.</p>
<div class="sourceCode" id="cb81"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>t<span class="op">=</span>np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, rows<span class="op">*</span>cols)[:, <span class="va">None</span>]</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>,lengthscale<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> kern.K(t, t)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.multivariate_normal(np.zeros(rows<span class="op">*</span>cols), cov, num_latent).T</span></code></pre></div>
<div class="figure">
<div id="digit-samples-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/digit-samples-deep-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="digit-samples-deep-gp-magnify" class="magnify"
onclick="magnifyFigure(&#39;digit-samples-deep-gp&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="digit-samples-deep-gp-caption" class="caption-frame">
<p>Figure: These digits are produced by taking a tour of the two
dimensional latent space (as described by a Gaussian process sample) and
mapping the tour into the data space. We visualize the mean of the
mapping in the images.</p>
</div>
</div>
<h2 id="deep-health">Deep Health</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_health/includes/deep-health-model.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/deep-health-model.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-health-model-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deep-health.svg" width="70%" style=" ">
</object>
</div>
<div id="deep-health-model-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-health-model&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-health-model-caption" class="caption-frame">
<p>Figure: The deep health model uses different layers of abstraction in
the deep Gaussian process to represent information about diagnostics and
treatment to model interelationships between a patients different data
modalities.</p>
</div>
</div>
<p>From a machine learning perspective, we’d like to be able to
interrelate all the different modalities that are informative about the
state of the disease. For deep health, the notion is that the state of
the disease is appearing at the more abstract levels, as we descend the
model, we express relationships between the more abstract concept, that
sits within the physician’s mind, and the data we can measure.</p>
<h2 id="conclusions">Conclusions</h2>
<p>The probabilistic modelling community has evolved in an era where the
assumption was that ambiguous conclusions are best shared with a
(trained) professional through probabilities. Recent advances in
generative AI offer the possibility of machines that have a better
understanding of human subjective ambiguities and therefore machines
that can summarise information in a way that can be interogated rather
than just through a series of numbers.</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>book: <a
href="https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248">The
Atomic Human</a></li>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Ananthanarayanan-cat09" class="csl-entry" role="listitem">
Ananthanarayanan, R., Esser, S.K., Simon, H.D., Modha, D.S., 2009. The
cat is out of the bag: Cortical simulations with <span
class="math inline">\(10^9\)</span> neurons, <span
class="math inline">\(10^{13}\)</span> synapses, in: Proceedings of the
Conference on High Performance Computing Networking, Storage and
Analysis - SC ’09. <a
href="https://doi.org/10.1145/1654059.1654124">https://doi.org/10.1145/1654059.1654124</a>
</div>
<div id="ref-Andrade:consistent14" class="csl-entry" role="listitem">
Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.
Consistent mapping of government malaria records across a changing
territory delimitation. Malaria Journal 13. <a
href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>
</div>
<div id="ref-Arora-convergence19" class="csl-entry" role="listitem">
Arora, S., Cohen, N., Golowich, N., Hu, W., 2019. <a
href="https://openreview.net/forum?id=SkMQg3C5K7">A convergence analysis
of gradient descent for deep linear neural networks</a>, in:
International Conference on Learning Representations.
</div>
<div id="ref-Bochner:book59" class="csl-entry" role="listitem">
Bochner, S., 1959. <a
href="http://books.google.co.uk/books?id=-vU02QewWK8C">Lectures on
<span>F</span>ourier integrals</a>. Princeton University Press.
</div>
<div id="ref-Boltzmann-warmetheorie77" class="csl-entry"
role="listitem">
Boltzmann, L., n.d. Über die <span>B</span>eziehung zwischen dem zweiten
<span>H</span>auptsatze der mechanischen <span>W</span>armetheorie und
der <span>W</span>ahrscheinlichkeitsrechnung, respective den
<span>S</span>ätzen über das wärmegleichgewicht. Sitzungberichte der
Kaiserlichen Akademie der Wissenschaften. Mathematisch-Naturwissen
Classe. Abt. II LXXVI, 373–435.
</div>
<div id="ref-Bui:deep16" class="csl-entry" role="listitem">
Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., Turner, R.,
2016. <a href="http://proceedings.mlr.press/v48/bui16.html">Deep
<span>G</span>aussian processes for regression using approximate
expectation propagation</a>, in: Balcan, M.F., Weinberger, K.Q. (Eds.),
Proceedings of the 33rd International Conference on Machine Learning,
Proceedings of Machine Learning Research. PMLR, New York, New York, USA,
pp. 1472–1481.
</div>
<div id="ref-Cabrera-realworld23" class="csl-entry" role="listitem">
Cabrera, C., Paleyes, A., Thodoroff, P., Lawrence, N.D., 2023. <a
href="https://arxiv.org/abs/2302.04810">Real-world machine learning
systems: A survey from a data-oriented architecture perspective</a>.
</div>
<div id="ref-Cho:deep09" class="csl-entry" role="listitem">
Cho, Y., Saul, L.K., 2009. <a
href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel
methods for deep learning</a>, in: Bengio, Y., Schuurmans, D., Lafferty,
J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural
Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.
</div>
<div id="ref-CMU-mocap03" class="csl-entry" role="listitem">
CMU Motion Capture Lab, 2003. The <span>CMU</span> mocap database.
</div>
<div id="ref-Coales-yellow14" class="csl-entry" role="listitem">
Coales, J.F., Kane, S.J., 2014. The <span>“yellow peril”</span> and
after. IEEE Control Systems Magazine 34, 65–69. <a
href="https://doi.org/10.1109/MCS.2013.2287387">https://doi.org/10.1109/MCS.2013.2287387</a>
</div>
<div id="ref-Damianou:thesis2015" class="csl-entry" role="listitem">
Damianou, A., 2015. Deep <span>G</span>aussian processes and variational
propagation of uncertainty (PhD thesis). University of Sheffield.
</div>
<div id="ref-Damianou:deepgp13" class="csl-entry" role="listitem">
Damianou, A., Lawrence, N.D., 2013. Deep <span>G</span>aussian
processes. pp. 207–215.
</div>
<div id="ref-Damianou:variational15" class="csl-entry" role="listitem">
Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference
for latent variables and uncertain inputs in <span>G</span>aussian
processes. Journal of Machine Learning Research 17.
</div>
<div id="ref-DellaGatta:direct08" class="csl-entry" role="listitem">
Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D.,
Missero, C., Bernardo, D. di, 2008. Direct targets of the TRP63
transcription factor revealed by a combination of gene expression
profiling and reverse engineering. Genome Research 18, 939–948. <a
href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a>
</div>
<div id="ref-Dunlop:deep2017" class="csl-entry" role="listitem">
Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. <a
href="http://jmlr.org/papers/v19/18-015.html">How deep are deep
<span>G</span>aussian processes?</a> Journal of Machine Learning
Research 19, 1–46.
</div>
<div id="ref-Duvenaud:pathologies14" class="csl-entry" role="listitem">
Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding
pathologies in very deep networks.
</div>
<div id="ref-Eddington:nature29" class="csl-entry" role="listitem">
Eddington, A.S., 1929. The nature of the physical world. Dent (London).
<a
href="https://doi.org/10.2307/2180099">https://doi.org/10.2307/2180099</a>
</div>
<div id="ref-Ferris:wifi07" class="csl-entry" role="listitem">
Ferris, B.D., Fox, D., Lawrence, N.D., 2007. <span>WiFi-SLAM</span>
using <span>G</span>aussian process latent variable models, in: Veloso,
M.M. (Ed.), Proceedings of the 20th International Joint Conference on
Artificial Intelligence (IJCAI 2007). pp. 2480–2485.
</div>
<div id="ref-Gelman:bayesian13" class="csl-entry" role="listitem">
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin,
D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.
</div>
<div id="ref-Gething:hmis06" class="csl-entry" role="listitem">
Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,
Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data
from health management information systems in <span>A</span>frica using
space–time geostatistics. PLoS Medicine 3. <a
href="https://doi.org/10.1371/journal.pmed.0030271">https://doi.org/10.1371/journal.pmed.0030271</a>
</div>
<div id="ref-Havasi:deepgp18" class="csl-entry" role="listitem">
Havasi, M., Hernández-Lobato, J.M., Murillo-Fuentes, J.J., 2018. <a
href="http://papers.nips.cc/paper/7979-inference-in-deep-gaussian-processes-using-stochastic-gradient-hamiltonian-monte-carlo.pdf">Inference
in deep <span>G</span>aussian processes using stochastic gradient
<span>H</span>amiltonian <span>M</span>onte <span>C</span>arlo</a>, in:
Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,
Garnett, R. (Eds.), Advances in Neural Information Processing Systems
31. Curran Associates, Inc., pp. 7506–7516.
</div>
<div id="ref-Heider:interpersonal58" class="csl-entry" role="listitem">
Heider, F., 1958. The psychology of interpersonal relations. John Wiley.
</div>
<div id="ref-Heider-experimental44" class="csl-entry" role="listitem">
Heider, F., Simmel, M., 1944. An experimental study of apparent
behavior. The American Journal of Psychology 57, 243–259. <a
href="https://doi.org/10.2307/1416950">https://doi.org/10.2307/1416950</a>
</div>
<div id="ref-Joseph-origins21" class="csl-entry" role="listitem">
Henrich, J., Muthukrishna, M., 2021. The origins and psychology of human
cooperation. Annual Review of Psychology 72, 207–240. <a
href="https://doi.org/10.1146/annurev-psych-081920-042106">https://doi.org/10.1146/annurev-psych-081920-042106</a>
</div>
<div id="ref-Huang-inner22" class="csl-entry" role="listitem">
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng,
A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T.,
Brown, N., Luu, L., Levine, S., Hausman, K., ichter, brian, 2023. <a
href="https://proceedings.mlr.press/v205/huang23c.html">Inner monologue:
Embodied reasoning through planning with language models</a>, in: Liu,
K., Kulic, D., Ichnowski, J. (Eds.), Proceedings of the 6th Conference
on Robot Learning, Proceedings of Machine Learning Research. PMLR, pp.
1769–1782.
</div>
<div id="ref-Ioffe:batch15" class="csl-entry" role="listitem">
Ioffe, S., Szegedy, C., 2015. <a
href="http://proceedings.mlr.press/v37/ioffe15.html">Batch
normalization: Accelerating deep network training by reducing internal
covariate shift</a>, in: Bach, F., Blei, D. (Eds.), Proceedings of the
32nd International Conference on Machine Learning, Proceedings of
Machine Learning Research. PMLR, Lille, France, pp. 448–456.
</div>
<div id="ref-Izmailov:subspace19" class="csl-entry" role="listitem">
Izmailov, P., Maddox, W.J., Kirichenko, P., Garipov, T., Vetrov, D.P.,
Wilson, A.G., 2019. <a href="http://arxiv.org/abs/1907.07504">Subspace
inference for bayesian deep learning</a>. CoRR abs/1907.07504.
</div>
<div id="ref-Jacot-deeplinear21" class="csl-entry" role="listitem">
Jacot, A., Ged, F., Gabriel, F., Şimşek, B., Hongler, C., 2021. <a
href="https://arxiv.org/abs/2106.15933">Deep linear networks dynamics:
Low-rank biases induced by initialization scale and L2
regularization</a>.
</div>
<div id="ref-Kalaitzis:simple11" class="csl-entry" role="listitem">
Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking
differentially expressed gene expression time courses through
<span>Gaussian</span> process regression. BMC Bioinformatics 12. <a
href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a>
</div>
<div id="ref-Lawrence-atomic24" class="csl-entry" role="listitem">
Lawrence, N.D., 2024. The atomic human: Understanding ourselves in the
age of AI. Allen Lane.
</div>
<div id="ref-Lawrence:embodiment17" class="csl-entry" role="listitem">
Lawrence, N.D., 2017. <a href="https://arxiv.org/abs/1705.07996">Living
together: Mind and machine intelligence</a>. arXiv.
</div>
<div id="ref-Lawrence:hgplvm07" class="csl-entry" role="listitem">
Lawrence, N.D., Moore, A.J., 2007. Hierarchical <span>G</span>aussian
process latent variable models. pp. 481–488.
</div>
<div id="ref-MacKay:gpintroduction98" class="csl-entry" role="listitem">
MacKay, D.J.C., n.d. Introduction to <span>G</span>aussian processes.
pp. 133–166.
</div>
<div id="ref-MacKay:bayesian92" class="csl-entry" role="listitem">
MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis).
California Institute of Technology.
</div>
<div id="ref-Mackay-behind91" class="csl-entry" role="listitem">
MacKay, D.M., 1991. Behind the eye. Basil Blackwell.
</div>
<div id="ref-McCulloch-neuron43" class="csl-entry" role="listitem">
McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas
immanent in nervous activity. Bulletin of Mathematical Biophysics 5,
115–133. <a
href="https://doi.org/10.1007/BF02478259">https://doi.org/10.1007/BF02478259</a>
</div>
<div id="ref-Mikhailov:hydrodynamica05" class="csl-entry"
role="listitem">
Mikhailov, G.K., n.d. Daniel bernoulli, hydrodynamica (1738).
</div>
<div id="ref-Mubangizi:malaria14" class="csl-entry" role="listitem">
Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,
N.D., 2014. Malaria surveillance with multiple data sources using
<span>Gaussian</span> process models, in: 1st International Conference
on the Use of Mobile <span>ICT</span> in Africa.
</div>
<div id="ref-Neal:bayesian94" class="csl-entry" role="listitem">
Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis).
Dept. of Computer Science, University of Toronto.
</div>
<div id="ref-Pearl:causality95" class="csl-entry" role="listitem">
Pearl, J., 1995. From <span>B</span>ayesian networks to causal networks,
in: Gammerman, A. (Ed.), Probabilistic Reasoning and
<span>B</span>ayesian Belief Networks. Alfred Waller, pp. 1–31.
</div>
<div id="ref-Reed-information98" class="csl-entry" role="listitem">
Reed, C., Durlach, N.I., 1998. Note on information transfer rates in
human communication. Presence Teleoperators &amp; Virtual Environments
7, 509–518. <a
href="https://doi.org/10.1162/105474698565893">https://doi.org/10.1162/105474698565893</a>
</div>
<div id="ref-Salimbeni:doubly2017" class="csl-entry" role="listitem">
Salimbeni, H., Deisenroth, M., 2017. <a
href="http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf">Doubly
stochastic variational inference for deep <span>G</span>aussian
processes</a>, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H.,
Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural
Information Processing Systems 30. Curran Associates, Inc., pp.
4591–4602.
</div>
<div id="ref-Kim-translation15" class="csl-entry" role="listitem">
Sharp, K., Matschinsky, F., 2015. Translation of <span>L</span>udwig
<span>B</span>oltzmann’s paper <span>“on the relationship between the
second fundamental theorem of the mechanical theory of heat and
probability calculations regarding the conditions for thermal
equilibrium.”</span> Entropy 17, 1971–2009. <a
href="https://doi.org/10.3390/e17041971">https://doi.org/10.3390/e17041971</a>
</div>
<div id="ref-Steele:predictive12" class="csl-entry" role="listitem">
Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson,
E., Avital, I., Stojadinovic, A., 2012. Using machine-learned
<span>B</span>ayesian belief networks to predict perioperative risk of
clostridium difficile infection following colon surgery. Interact J Med
Res 1, e6. <a
href="https://doi.org/10.2196/ijmr.2131">https://doi.org/10.2196/ijmr.2131</a>
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Admiralty-gunnery45" class="csl-entry" role="listitem">
The Admiralty, 1945. <a href="https://www.maritime.org/doc/br224/">The
gunnery pocket book, b.r. 224/45</a>.
</div>
<div id="ref-Thompson-juries89" class="csl-entry" role="listitem">
Thompson, W.C., 1989. <a href="http://www.jstor.org/stable/1191906">Are
juries competent to evaluate statistical evidence?</a> Law and
Contemporary Problems 52, 9–41.
</div>
<div id="ref-Tipping:probpca99" class="csl-entry" role="listitem">
Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component
analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a
href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>
</div>
<div id="ref-Wiener-exprodigy53" class="csl-entry" role="listitem">
Wiener, N., 1953. Ex-prodigy: My childhood and youth. mitp, Cambridge,
MA.
</div>
<div id="ref-Wiener:yellow49" class="csl-entry" role="listitem">
Wiener, N., 1949. The extrapolation, interpolation and smoothing of
stationary time series with engineering applications. wiley.
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>the challenge of understanding what information pertains
to is known as knowledge representation.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The quote is reported in the <em>Manchester
Guardian</em> on 29th June 1892. See also <a
href="https://www.york.ac.uk/depts/maths/histstat/lies.htm"
class="uri">https://www.york.ac.uk/depts/maths/histstat/lies.htm</a>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Although Twain attributes Disraeli in this way there’s
<a
href="https://en.wikipedia.org/wiki/Lies,_damned_lies,_and_statistics">no
record of him having said this.</a>.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>In classical statistics we often interpret these
parameters, <span class="math inline">\(\beta\)</span>, whereas in
machine learning we are normally more interested in the result of the
prediction, and less in the prediction. Although this is changing with
more need for accountability. In honour of this I normally use <span
class="math inline">\(\boldsymbol{\beta}\)</span> when I care about the
value of these parameters, and <span class="math inline">\(\mathbf{
w}\)</span> when I care more about the quality of the prediction.<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

