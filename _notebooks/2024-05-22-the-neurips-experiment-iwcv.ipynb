{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting the Revisit of the 2014 NeurIPS Experiment\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge\n",
    "\n",
    "### 2024-05-22"
   ],
   "id": "93a2536d-c2ca-4758-9ff3-b46ec7a92006"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In 2014, along with Corinna Cortes, I was Program Chair of\n",
    "the Neural Information Processing Systems conference. At the time, when\n",
    "wondering about innovations for the conference, Corinna and I decided it\n",
    "would be interesting to test the consistency of reviewing. With this in\n",
    "mind, we randomly selected 10% of submissions and had them reviewed by\n",
    "two independent committees. In this talk I will review the construction\n",
    "of the experiment, explain how the NeurIPS review process worked and\n",
    "talk about what I felt the implications for reviewing were, vs what the\n",
    "community reaction was. The talk was originally given in 2021 when the\n",
    "long term impact of papers were measured by seven years of citations.\n",
    "Here we augment the results with citations from today, 2024, nearly a\n",
    "decade after papers were published."
   ],
   "id": "b5dc4eca-bd8b-4754-ab7c-ad788500aa8b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ],
   "id": "8f5762ab-eeb9-42b8-be9e-c9ee75bc5bfc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ],
   "id": "88213275-0ff2-4e22-bc7e-b678831d1697"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The NIPS experiment was an experiment to determine the consistency of\n",
    "the review process. After receiving papers, we selected 10% that would\n",
    "be independently rereviewed. The idea was to determine how consistent\n",
    "the decisions between the two sets of independent papers would be. In\n",
    "2014 NIPS received 1678 submissions and we selected 170 for the\n",
    "experiment. These papers are referred to below as ‘duplicated papers’.\n",
    "\n",
    "To run the experiment, we created two separate committees within the\n",
    "NIPS program committee. The idea was that the two separate committees\n",
    "would review each duplicated paper independently and results compared."
   ],
   "id": "1f084908-62b8-4bcc-b9fe-745689df62bf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeurIPS in Numbers\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-in-numbers.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-in-numbers.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In 2014 the NeurIPS conference had 1474 active reviewers (up from 1133\n",
    "in 2013), 92 area chairs (up from 67 in 2013) and two program chairs,\n",
    "Corinna Cortes and me.\n",
    "\n",
    "The conference received 1678 submissions and presented 414 accepted\n",
    "papers, of which 20 were presented as talks in the single-track session,\n",
    "62 were presented as spotlights and 331 papers were presented as\n",
    "posters. Of the 1678 submissions, 19 papers were rejected without\n",
    "review."
   ],
   "id": "32624ccb-60ea-427d-84b4-5ab52b2c9e2f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NeurIPS Experiment\n",
    "\n",
    "The objective of the NeurIPS experiment was to determine how consistent\n",
    "the process of peer review is. One way of phrasing this question is to\n",
    "ask: what would happen to submitted papers in the conference if the\n",
    "process was independently rerun?\n",
    "\n",
    "For the 2014 conference, to explore this question, we selected\n",
    "$\\approx 10\\%$ of submitted papers to be reviewed twice, by independent\n",
    "committees. This led to 170 papers being selected from the conference\n",
    "for dual reviewing. For these papers the program committee was divided\n",
    "into two. Reviewers were placed randomly on one side of the committee or\n",
    "the other. For Program Chairs we also engaged in some manual selection\n",
    "to ensure we had expert coverage in all the conference areas on both\n",
    "side of the committee."
   ],
   "id": "4c350da5-f98a-41a2-8c21-4fae87433b01"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline for NeurIPS\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-timeline.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-timeline.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Chairing a conference starts with recruitment of the program committee,\n",
    "which is usually done in a few stages. The primary task is to recruit\n",
    "the area chairs. We sent out our program committee invites in three\n",
    "waves.\n",
    "\n",
    "-   17/02/2014\n",
    "-   08/03/2014\n",
    "-   09/04/2014\n",
    "\n",
    "By recruiting area chairs first, you can involve them in recruiting\n",
    "reviewers. We requested names of reviewers from ACs in two waves.\n",
    "\n",
    "-   25/03/2014\n",
    "-   11/04/2014\n",
    "\n",
    "In 2014, this wasn’t enough to obtain the requisite number of reviewers,\n",
    "so we used additional approaches. These included lists of previous\n",
    "NeurIPS authors. For each individual we were looking for at least two\n",
    "previously-published papers from NeurIPS and other leading leading ML\n",
    "venues like ICML, AISTATS, COLT, UAI etc.. We made extensive use of\n",
    "[DBLP](https://dblp.uni-trier.de/) for verifying each potential\n",
    "reviewer’s publication track record.\n",
    "\n",
    "-   14/04/2014\n",
    "-   28/04/2014\n",
    "-   09/05/2014\n",
    "-   10/06/2014 (note this is after deadline … lots of area chairs asked\n",
    "    for reviewers after the deadline!). We invited them en-masse.\n",
    "\n",
    "-   06/06/2014 Submission Deadline\n",
    "-   12/06/2014 Bidding Open for Area Chairs (this was *delayed* by CMT\n",
    "    issues)\n",
    "-   17/06/2014 Bidding Open for Reviewers\n",
    "-   01/07/2014 Start Reviewing\n",
    "-   21/07/2014 Reviewing deadline\n",
    "-   04/08/2014 Reviews to Authors\n",
    "-   11/08/2014 Author Rebuttal Due\n",
    "-   25/08/2014 Teleconferences Begin\n",
    "-   30/08/2014 Teleconferences End\n",
    "-   1/09/2014 Preliminary Decisions Made\n",
    "-   9/09/2014 Decisions Sent to Authors"
   ],
   "id": "f2575bf8-f1fb-4163-bae7-3b2851d52824"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Scoring and Reviewer Instructions\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/paper-scoring.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/paper-scoring.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The instructions to reviewers for the 2014 conference are still\n",
    "available [online\n",
    "here](https://nips.cc/Conferences/2014/PaperInformation/ReviewerInstructions).\n",
    "\n",
    "To keep quality of reviews high, we tried to keep load low. We didn’t\n",
    "assign any reviewer more than 5 papers, most reviewers received 4\n",
    "papers."
   ],
   "id": "1aeb35c6-16c3-4526-9526-4a2d9d467053"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation\n",
    "\n",
    "Reviewers give a score of between 1 and 10 for each paper. The program\n",
    "committee will interpret the numerical score in the following way:\n",
    "\n",
    "-   10: Top 5% of accepted NIPS papers, a seminal paper for the ages.\n",
    "\n",
    "    I will consider not reviewing for NIPS again if this is rejected.\n",
    "\n",
    "-   9: Top 15% of accepted NIPS papers, an excellent paper, a strong\n",
    "    accept.\n",
    "\n",
    "    I will fight for acceptance.\n",
    "\n",
    "-   8: Top 50% of accepted NIPS papers, a very good paper, a clear\n",
    "    accept.\n",
    "\n",
    "    I vote and argue for acceptance.\n",
    "\n",
    "-   7: Good paper, accept.\n",
    "\n",
    "    I vote for acceptance, although would not be upset if it were\n",
    "    rejected.\n",
    "\n",
    "-   6: Marginally above the acceptance threshold.\n",
    "\n",
    "    I tend to vote for accepting it, but leaving it out of the program\n",
    "    would be no great loss.\n",
    "\n",
    "-   5: Marginally below the acceptance threshold.\n",
    "\n",
    "    I tend to vote for rejecting it, but having it in the program would\n",
    "    not be that bad.\n",
    "\n",
    "-   4: An OK paper, but not good enough. A rejection.\n",
    "\n",
    "    I vote for rejecting it, although would not be upset if it were\n",
    "    accepted.\n",
    "\n",
    "-   3: A clear rejection.\n",
    "\n",
    "    I vote and argue for rejection.\n",
    "\n",
    "-   2: A strong rejection. I’m surprised it was submitted to this\n",
    "    conference.\n",
    "\n",
    "    I will fight for rejection.\n",
    "\n",
    "-   1: Trivial or wrong or known. I’m surprised anybody wrote such a\n",
    "    paper.\n",
    "\n",
    "    I will consider not reviewing for NIPS again if this is accepted.\n",
    "\n",
    "Reviewers should NOT assume that they have received an unbiased sample\n",
    "of papers, nor should they adjust their scores to achieve an artificial\n",
    "balance of high and low scores. Scores should reflect absolute judgments\n",
    "of the contributions made by each paper."
   ],
   "id": "c4b41e07-6db5-48fd-86a5-d0f522f18507"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact Score\n",
    "\n",
    "The impact score was an innovation introduce in 2013 by Ghahramani and\n",
    "Welling that we retained for 2014. Quoting from the instructions to\n",
    "reviewers:\n",
    "\n",
    "> Independently of the Quality Score above, this is your opportunity to\n",
    "> identify papers that are very different, original, or otherwise\n",
    "> potentially impactful for the NIPS community.\n",
    ">\n",
    "> There are two choices:\n",
    ">\n",
    "> 2: This work is different enough from typical submissions to\n",
    "> potentially have a major impact on a subset of the NIPS community.\n",
    ">\n",
    "> 1: This work is incremental and unlikely to have much impact even\n",
    "> though it may be technically correct and well executed.\n",
    ">\n",
    "> Examples of situations where the impact and quality scores may point\n",
    "> in opposite directions include papers which are technically strong but\n",
    "> unlikely to generate much follow-up research, or papers that have some\n",
    "> flaw (e.g. not enough evaluation, not citing the right literature) but\n",
    "> could lead to new directions of research."
   ],
   "id": "2114c6aa-9127-4eac-9191-60fb226529ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Score\n",
    "\n",
    "Reviewers also give a confidence score between 1 and 5 for each paper.\n",
    "The program committee will interpret the numerical score in the\n",
    "following way:\n",
    "\n",
    "5: The reviewer is absolutely certain that the evaluation is correct and\n",
    "very familiar with the relevant literature.\n",
    "\n",
    "4: The reviewer is confident but not absolutely certain that the\n",
    "evaluation is correct. It is unlikely but conceivable that the reviewer\n",
    "did not understand certain parts of the paper, or that the reviewer was\n",
    "unfamiliar with a piece of relevant literature.\n",
    "\n",
    "3: The reviewer is fairly confident that the evaluation is correct. It\n",
    "is possible that the reviewer did not understand certain parts of the\n",
    "paper, or that the reviewer was unfamiliar with a piece of relevant\n",
    "literature. Mathematics and other details were not carefully checked.\n",
    "\n",
    "2: The reviewer is willing to defend the evaluation, but it is quite\n",
    "likely that the reviewer did not understand central parts of the paper.\n",
    "\n",
    "1: The reviewer’s evaluation is an educated guess. Either the paper is\n",
    "not in the reviewer’s area, or it was extremely difficult to understand."
   ],
   "id": "4f4e61c3-02dc-400f-95b0-cbc008863a50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation\n",
    "\n",
    "All NIPS papers should be good scientific papers, regardless of their\n",
    "specific area. We judge whether a paper is good using four criteria; a\n",
    "reviewer should comment on all of these, if possible:\n",
    "\n",
    "-   Quality\n",
    "\n",
    "    Is the paper technically sound? Are claims well-supported by\n",
    "    theoretical analysis or experimental results? Is this a complete\n",
    "    piece of work, or merely a position paper? Are the authors careful\n",
    "    (and honest) about evaluating both the strengths and weaknesses of\n",
    "    the work?\n",
    "\n",
    "-   Clarity\n",
    "\n",
    "    Is the paper clearly written? Is it well-organized? (If not, feel\n",
    "    free to make suggestions to improve the manuscript.) Does it\n",
    "    adequately inform the reader? (A superbly written paper provides\n",
    "    enough information for the expert reader to reproduce its results.)\n",
    "\n",
    "-   Originality\n",
    "\n",
    "    Are the problems or approaches new? Is this a novel combination of\n",
    "    familiar techniques? Is it clear how this work differs from previous\n",
    "    contributions? Is related work adequately referenced? We recommend\n",
    "    that you check the proceedings of recent NIPS conferences to make\n",
    "    sure that each paper is significantly different from papers in\n",
    "    previous proceedings. Abstracts and links to many of the previous\n",
    "    NIPS papers are available from http://books.nips.cc\n",
    "\n",
    "-   Significance\n",
    "\n",
    "Are the results important? Are other people (practitioners or\n",
    "researchers) likely to use these ideas or build on them? Does the paper\n",
    "address a difficult problem in a better way than previous research? Does\n",
    "it advance the state of the art in a demonstrable way? Does it provide\n",
    "unique data, unique conclusions on existing data, or a unique\n",
    "theoretical or pragmatic approach?"
   ],
   "id": "dcb5052f-01ef-4151-9feb-50bd160cce22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speculation\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-speculation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-speculation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "With the help of [Nicolo Fusi](http://nicolofusi.com/), [Charles\n",
    "Twardy](http://blog.scicast.org/tag/charles-twardy/) and the entire\n",
    "Scicast team we launched [a Scicast\n",
    "question](https://scicast.org/#!/questions/1083/trades/create/power) a\n",
    "week before the results were revealed. The comment thread for that\n",
    "question already had [an amount of interesting\n",
    "comment](https://scicast.org/#!/questions/1083/comments/power) before\n",
    "the conference. Just for informational purposes before we began\n",
    "reviewing Corinna forecast this figure would be 25% and I forecast it\n",
    "would be 20%. The box plot summary of predictions from Scicast is below.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/scicast-forecast.png\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>Summary forecast from those that responded to a scicast\n",
    "question about how consistent the decision making was.</i>"
   ],
   "id": "d29c1c0e-2694-445f-9847-374e988fe364"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeurIPS Experiment Results\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-results.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-results.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The results of the experiment were as follows. From 170 papers 4 had to\n",
    "be withdrawn or were rejected without completing the review process, for\n",
    "the remainder, the ‘confusion matrix’ for the two committee’s decisions\n",
    "is in Table .\n",
    "\n",
    "Table: Table showing the results from the two committees as a confusion\n",
    "matrix. Four papers were rejected or withdrawn without review.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td colspan=\"2\">\n",
    "</td>\n",
    "<td colspan=\"2\">\n",
    "\n",
    "Committee 1\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "Accept\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "Reject\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"2\">\n",
    "\n",
    "Committee 2\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "Accept\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "22\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "22\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "Reject\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "21\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "101\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<img class=\"img-button\" src=\"{{ '/assets/images/Magnify_Large.svg' | relative_url }}\" style=\"width:1.5ex\">"
   ],
   "id": "94f17b56-7275-40f1-b203-ffcc52e93841"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the Table\n",
    "\n",
    "There are a few ways of summarizing the numbers in this table as percent\n",
    "or probabilities. First, the inconsistency, the proportion of decisions\n",
    "that were not the same across the two committees. The decisions were\n",
    "inconsistent for 43 out of 166 papers or 0.259 as a proportion. This\n",
    "number is perhaps a natural way of summarizing the figures if you are\n",
    "submitting your paper and wish to know an estimate of what the\n",
    "probability is that your paper would have different decisions according\n",
    "to the different committees. Secondly, the accept precision: if you are\n",
    "attending the conference and looking at any given paper, then you might\n",
    "want to know the probability that the paper would have been rejected in\n",
    "an independent rerunning of the conference. We can estimate this for\n",
    "Committee 1’s conference as 22/(22 + 22) = 0.5 (50%) and for Committee\n",
    "2’s conference as 21/(22+21) = 0.49 (49%). Averaging the two estimates\n",
    "gives us 49.5%. Finally, the reject precision: if your paper was\n",
    "rejected from the conference, you might like an estimate of the\n",
    "probability that the same paper would be rejected again if the review\n",
    "process had been independently rerun. That estimate is 101/(22+101) =\n",
    "0.82 (82%) for Committee 1 and 101/(21+101)=0.83 (83%) for Committee 2,\n",
    "or on average 82.5%. A final quality estimate might be the ratio of\n",
    "consistent accepts to consistent rejects, or the agreed accept rate,\n",
    "22/123 = 0.18 (18%).\n",
    "\n",
    "-   *inconsistency*: 43/166 = **0.259**\n",
    "    -   proportion of decisions that were not the same\n",
    "-   *accept precision* $0.5 \\times 22/44$ + $0.5 \\times 21/43$ =\n",
    "    **0.495**\n",
    "    -   probability any accepted paper would be rejected in a rerunning\n",
    "-   *reject precision* = $0.5\\times 101/(22+101)$ +\n",
    "    $0.5\\times 101/(21 + 101)$ = **0.175**\n",
    "    -   probability any rejected paper would be rejected in a rerunning\n",
    "-   *agreed accept rate* = 22/101 = **0.218**\n",
    "-   ratio between agreed accepted papers and agreed rejected papers."
   ],
   "id": "d967a822-1562-4641-b905-4e7407f00a49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaction After Experiment\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-reaction.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-reaction.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "There seems to have been a lot of discussion of the result, both at the\n",
    "conference and on bulletin boards since. Such discussion is to be\n",
    "encouraged, and for ease of memory, it is worth pointing out that the\n",
    "approximate proportions of papers in each category can be nicely divided\n",
    "in to eighths as follows. Accept-Accept 1 in 8 papers, Accept-Reject 3\n",
    "in 8 papers, Reject-Reject, 5 in 8 papers. This makes the statistics\n",
    "we’ve computed above: inconsistency 1 in 4 (25%) accept precision 1 in 2\n",
    "(50%) reject precision 5 in 6 (83%) and agreed accept rate of 1 in 6\n",
    "(20%). This compares with the accept rate of 1 in 4.\n",
    "\n",
    "-   Public reaction after experiment [documented\n",
    "    here](http://inverseprobability.com/2015/01/16/blogs-on-the-nips-experiment/)\n",
    "\n",
    "-   [Open Data\n",
    "    Science](http://inverseprobability.com/2014/07/01/open-data-science/)\n",
    "    (see Heidelberg Meeting)\n",
    "\n",
    "-   NIPS was run in a very open way.\n",
    "    [Code](https://github.com/sods/conference) and [blog\n",
    "    posts](http://inverseprobability.com/2014/12/16/the-nips-experiment/)\n",
    "    all available!\n",
    "\n",
    "-   Reaction triggered by [this blog\n",
    "    post](http://blog.mrtz.org/2014/12/15/the-nips-experiment.html).\n",
    "\n",
    "Much of the discussion speculates on the number of consistent accepts in\n",
    "the process (using the main conference accept rate as a proxy). It\n",
    "therefore produces numbers that don’t match ours above. This is because\n",
    "the computed accept rate of the individual committees is different from\n",
    "that of the main conference. This could be due to a bias for the\n",
    "duplicated papers, or statistical sampling error. We look at these\n",
    "questions below. First, to get the reader primed for thinking about\n",
    "these numbers we discuss some context for placing these numbers."
   ],
   "id": "dcb71880-d0ee-42f7-838f-996c0c84bbf4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS 2021 Experiment\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-2021-experiment.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-2021-experiment.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The experiment was repeated in 2021 with broadly the same set up\n",
    "(Beygelzimer et al., 2023), only a much larger conference. The\n",
    "conference had increased in size by five times to over 9,000 submissions\n",
    "and 9,000 programme committee members.\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/neurips-2021-snippet-1.png\" style=\"width:\">\n",
    "\n",
    "Figure: <i>Snippet from the NeurIPS 2021 experiment paper explaining the\n",
    "change in circumstances.</i>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/neurips-2021-snippet-2.png\" style=\"width:\">\n",
    "\n",
    "Figure: <i>Snippet from the NeurIPS 2021 experiment paper explaining the\n",
    "results.</i>"
   ],
   "id": "4ce9fa1b-0ee9-4dfb-8edf-7a8386bbbbb4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Random Committee @ 25%\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-random-committee.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-random-committee.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The first context we can place around the numbers is what would have\n",
    "happened at the ‘Random Conference’ where we simply accept a quarter of\n",
    "papers at random. In this NIPS the expected numbers of accepts would\n",
    "then have been given as in Table .\n",
    "\n",
    "Table: Table shows the expected values for the confusion matrix if the\n",
    "committee was making decisions totally at random.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td colspan=\"2\">\n",
    "</td>\n",
    "<td colspan=\"2\">\n",
    "\n",
    "Committee 1\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "Accept\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "Reject\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td rowspan=\"2\">\n",
    "\n",
    "Committee 2\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "Accept\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "10.4 (1 in 16)\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "31.1 (3 in 16)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "Reject\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "31.1 (3 in 16)\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "93.4 (9 in 16)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<img class=\"img-button\" src=\"{{ '/assets/images/Magnify_Large.svg' | relative_url }}\" style=\"width:1.5ex\">\n",
    "\n",
    "And for this set up we would expect *inconsistency* of 3 in 8 (37.5%)\n",
    "*accept precision* of 1 in 4 (25%) and a *reject precision* of 3 in 4\n",
    "(75%) and a *agreed accept rate* of 1 in 10 (10%). The actual committee\n",
    "made improvements on these numbers, the accept precision was markedly\n",
    "better with 50%: twice as many consistent accept decisions were made\n",
    "than would be expected if the process had been performed at random and\n",
    "only around two thirds as many inconsistent decisions were made as would\n",
    "have been expected if decisions were made at random. However, we should\n",
    "treat all these figures with some skepticism until we’ve performed some\n",
    "estimate of the uncertainty associated with them."
   ],
   "id": "0d6dac04-cab0-410d-a819-81b60423f47f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats for Random Committee\n",
    "\n",
    "-   For random committee we expect:\n",
    "    -   *inconsistency* of 3 in 8 (37.5%)\n",
    "    -   *accept precision* of 1 in 4 (25%)\n",
    "    -   *reject precision* of 3 in 4 (75%) and a\n",
    "    -   *agreed accept rate* of 1 in 10 (10%).\n",
    "\n",
    "Actual committee’s accept precision markedly better with 50% accept\n",
    "precision."
   ],
   "id": "c43097b8-2ae8-452c-b566-f6b09d72185c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty: Accept Rate\n",
    "\n",
    "To get a handle on the uncertainty around these numbers we’ll start by\n",
    "making use of the\n",
    "<a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\" target=\"_blank\">binomial\n",
    "distribution</a>. First, let’s explore the fact that for the overall\n",
    "conference the accept rate was around 23%, but for the duplication\n",
    "committees the accept rate was around 25%. If we assume decisions are\n",
    "made according to a binomial distribution, then is the accept rate for\n",
    "the duplicated papers too high?\n",
    "\n",
    "Note that for all our accept probability statistics we used as a\n",
    "denominator the number of papers that were initially sent for review,\n",
    "rather than the number where a final decision was made by the program\n",
    "committee. These numbers are different because some papers are withdrawn\n",
    "before the program committee makes its decision. Most commonly this\n",
    "occurs after authors have seen their preliminary reviews: for NIPS 2014\n",
    "we provided preliminary reviews that included paper scores. So for the\n",
    "official accept probability we use the 170 as denominator. The accept\n",
    "probabilities were therefore 43 out of 170 papers (25.3%) for Committee\n",
    "1 and 44 out of 170 (25.8%) for Committee 2. This compares with the\n",
    "overall conference accept rate for papers outside the duplication\n",
    "process of 349 out of 1508 (23.1%).\n",
    "\n",
    "If the true underlying probability of an accept were 0.23, independent\n",
    "of the paper, then the probability of generating accepts for any subset\n",
    "of the papers would be given by a binomial distribution. Combining\n",
    "across the two committees for the duplicated papers, we see that 87\n",
    "papers in total were recommended for accept out of a total of 340\n",
    "trials. out of 166 trials would be given by a binomial distribution as\n",
    "depicted below."
   ],
   "id": "32101330-fd1c-498e-abef-5c016b264e79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "from IPython.display import HTML"
   ],
   "id": "20d4b151-c889-498a-89ee-d34017e55278"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cmtutils.plot as plot\n",
    "import mlai as ma"
   ],
   "id": "77daedeb-f2be-4cf9-9291-6012003b6351"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = binom(340, 0.23)\n",
    "x = np.arange(60, 120)\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.bar(x, rv.pmf(x))\n",
    "display(HTML('<h3>Number of Accepted Papers for p = 0.23</h3>'))\n",
    "ax.axvline(87,linewidth=4, color='red')\n",
    "ma.write_figure(filename=\"uncertainty-accept-rate.svg\", directory=\"./neurips\")"
   ],
   "id": "ea476f4b-45a5-43e9-8a11-93522ddbdbf5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/uncertainty-accept-rate.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Number of accepted papers for $p=0.23$.</i>\n",
    "\n",
    "From the plot, we can see that whilst the accept rate was slightly\n",
    "higher for duplicated papers it doesn’t seem that we can say that it was\n",
    "statistically significant that it was higher, it falls well within the\n",
    "probability mass of the Binomial.\n",
    "\n",
    "Note that Area Chairs knew which papers were duplicates, whereas\n",
    "reviewers did not. Whilst we stipulated that duplicate papers should not\n",
    "be any given special treatment, we cannot discount the possibility that\n",
    "Area Chairs may have given slightly preferential treatment to duplicate\n",
    "papers."
   ],
   "id": "ed9682a3-c0c5-48dd-92d0-7b3fdbb94004"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty: Accept Precision\n",
    "\n",
    "For the accept precision, if we assume that accept decisions were drawn\n",
    "according to a binomial, then the distribution for consistent accepts is\n",
    "also binomial. Our best estimate of its parameter is 22/166 = 0.13\n",
    "(13%). If we had a binomial distribution with these parameters, then the\n",
    "distribution of consistent accepts would be as follows."
   ],
   "id": "a1baf3e5-3949-44bf-9ee6-0cfcec5576dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = binom(166, 0.13)\n",
    "x = np.arange(10, 30)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.bar(x, rv.pmf(x))\n",
    "display(HTML('<h3>Number of Consistent Accepts given p=0.13</h3>'))\n",
    "ax.axvline(22,linewidth=4, color='red') \n",
    "ma.write_figure(filename=\"uncertainty-accept-precision.svg\", directory=\"./neurips\")"
   ],
   "id": "3a8d37b2-f853-495e-9e66-a8f2217be4af"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/uncertainty-accept-rate.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Number of consistent accepts given $p=0.13$.</i>\n",
    "\n",
    "We see immediately that there is a lot of uncertainty around this\n",
    "number, for the scale of the experiment as we have it. This suggests a\n",
    "more complex analysis is required to extract our estimates with\n",
    "uncertainty."
   ],
   "id": "41d47087-f57b-4a84-a663-59023b84b692"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Analysis\n",
    "\n",
    "Before we start the analysis, it’s important to make some statements\n",
    "about the aims of our modelling here. We will make some simplifying\n",
    "modelling assumptions for the sake of a model that is understandable. We\n",
    "are looking to get a handle on the uncertainty associated with some of\n",
    "the probabilities associated with the NIPS experiment. [Some preliminary\n",
    "analyses have already been conducted on\n",
    "blogs](http://inverseprobability.com/2015/01/16/blogs-on-the-nips-experiment/).\n",
    "Those analyses don’t have access to information like paper scores etc.\n",
    "For that reason we also leave out such information in this preliminary\n",
    "analysis. We will focus only on the summary results from the experiment:\n",
    "how many papers were consistently accepted, consistently rejected, or\n",
    "had inconsistent decisions. For the moment we disregard the information\n",
    "we have about paper scores.\n",
    "\n",
    "In our analysis there are three possible outcomes for each paper:\n",
    "consistent accept, inconsistent decision and consistent reject. So, we\n",
    "need to perform the analysis with the [multinomial\n",
    "distribution](http://en.wikipedia.org/wiki/Multinomial_distribution).\n",
    "The multinomial is parameterized by the probabilities of the different\n",
    "outcomes. These are our parameters of interest; we would like to\n",
    "estimate these probabilities alongside their uncertainties. To make a\n",
    "Bayesian analysis we place a prior density over these probabilities,\n",
    "then we update the prior with the observed data, that gives us a\n",
    "posterior density, giving us an uncertainty associated with these\n",
    "probabilities."
   ],
   "id": "94d99893-7af9-400e-b4ab-df2b6ea99e6f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Density\n",
    "\n",
    "Choice of prior for the multinomial is typically straightforward, the\n",
    "[Dirichlet density](http://en.wikipedia.org/wiki/Dirichlet_distribution)\n",
    "is [conjugate](http://en.wikipedia.org/wiki/Conjugate_prior) and has the\n",
    "additional advantage that its parameters can be set to ensure it is\n",
    "*uninformative*, i.e. uniform across the domain of the prior.\n",
    "Combination of a multinomial likelihood and a Dirichlet prior is not\n",
    "new, and in this domain if we were to consider the mean the posterior\n",
    "density only, then the approach is known as [Laplace\n",
    "smoothing](http://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "\n",
    "For our model we are assuming for our prior that the probabilities are\n",
    "drawn from a Dirichlet as follows, $$\n",
    "p \\sim \\text{Dir}(\\alpha_1, \\alpha_2, \\alpha_3),\n",
    "$$ with $\\alpha_1=\\alpha_2=\\alpha_3=1$. The Dirichlet density is\n",
    "conjugate to the [multinomial\n",
    "distribution](http://en.wikipedia.org/wiki/Multinomial_distribution),\n",
    "and we associate three different outcomes with the multinomial. For each\n",
    "of the 166 papers we expect to have a consistent accept (outcome 1), an\n",
    "inconsistent decision (outcome 2) or a consistent reject (outcome 3). If\n",
    "the counts four outcome 1, 2 and 3 are represented by $k_1$, $k_2$ and\n",
    "$k_3$ and the associated probabilities are given by $p_1$, $p_2$ and\n",
    "$p_3$ then our model is, Due to the conjugacy the posterior is tractable\n",
    "and easily computed as a Dirichlet (see e.g. [Gelman et\n",
    "al](http://www.stat.columbia.edu/~gelman/book/)), where the parameters\n",
    "of the Dirichlet are given by the original vector from the Dirichlet\n",
    "prior plus the counts associated with each outcome. $$\n",
    "\\mathbf{p}|\\mathbf{k}, \\boldsymbol{\\alpha} \\sim \\text{Dir}(\\boldsymbol{\\alpha} + \\mathbf{k})\n",
    "$$ The mean probability for each outcome is then given by, $$\n",
    "\\bar{p}_i = \\frac{\\alpha_i+k_i}{\\sum_{j=1}^3(\\alpha_j + k_j)}.\n",
    "$$ and the variance is $$\n",
    "\\mathrm{Var}[p_i] = \\frac{(\\alpha_i+k_i) (\\alpha_0-\\alpha_i + n + k_i)}{(\\alpha_0+n)^2 (\\alpha_0+n+1)},\n",
    "$$ where $n$ is the number of trials (166 in our case) and\n",
    "$\\alpha_0 = \\sum_{i=1}^3\\alpha_i$. This allows us to compute the\n",
    "expected value of the probabilities and their variances under the\n",
    "posterior as follows."
   ],
   "id": "24aa6bd7-571e-4d8d-ba3b-bf5dcd9026e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mean_var(k, alpha):\n",
    "    \"\"\"Compute the mean and variance of the Dirichlet posterior.\"\"\"\n",
    "    alpha_0 = alpha.sum()\n",
    "    n = k.sum()\n",
    "    m = (k + alpha)\n",
    "    m /= m.sum()\n",
    "    v = (alpha+k)*(alpha_0 - alpha + n + k)/((alpha_0+n)**2*(alpha_0+n+1))\n",
    "    return m, v\n",
    "\n",
    "k = np.asarray([22, 43, 101])\n",
    "alpha = np.ones((3,))\n",
    "m, v = posterior_mean_var(k, alpha)\n",
    "outcome = ['consistent accept', 'inconsistent decision', 'consistent reject']\n",
    "for i in range(3):\n",
    "    display(HTML(\"<h4>Probability of \" + outcome[i] +' ' + str(m[i]) +  \"+/-\" + str(2*np.sqrt(v[i])) + \"</h4>\"))"
   ],
   "id": "f1977183-bdd2-4ee5-91ac-43b5477fc65d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a probability of consistent accept as $0.136 \\pm 0.06$, the\n",
    "probability of inconsistent decision as $0.260 \\pm 0.09$ and probability\n",
    "of consistent reject as $0.60 \\pm 0.15$. Recall that if we’d selected\n",
    "papers at random (with accept rate of 1 in 4) then these values would\n",
    "have been 1 in 16 (0.0625), 3 in 8 (0.375) and 9 in 16 (0.5625).\n",
    "\n",
    "The other values we are interested in are the accept precision, reject\n",
    "precision and the agreed accept rate. Computing the probability density\n",
    "for these statistics is complex: it involves [Ratio\n",
    "Distributions](http://en.wikipedia.org/wiki/Ratio_distribution).\n",
    "However, we can use Monte Carlo to estimate the expected accept\n",
    "precision, reject precision, and agreed accept rate as well as their\n",
    "variances. We can use these results to give us error bars and histograms\n",
    "of these statistics."
   ],
   "id": "0c039a2f-4fa5-45cb-90b4-0d4954acf5c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precisions(k, alpha, num_samps):\n",
    "    \"\"\"Helper function to sample from the posterior distibution of accept, \n",
    "    reject and inconsistent probabilities and compute other statistics of interest \n",
    "    from the samples.\"\"\"\n",
    "\n",
    "    k = np.random.dirichlet(k+alpha, size=num_samps)\n",
    "    # Factors of 2 appear because inconsistent decisions \n",
    "    # are being accounted for across both committees.\n",
    "    ap = 2*k[:, 0]/(2*k[:, 0]+k[:, 1])\n",
    "    rp = 2*k[:, 2]/(k[:, 1]+2*k[:, 2])\n",
    "    aa = k[:, 0]/(k[:, 0]+k[:, 2])\n",
    "    return ap, rp, aa\n",
    "\n",
    "ap, rp, aa = sample_precisions(k, alpha, 10000)\n",
    "print(ap.mean(), '+/-', 2*np.sqrt(ap.var()))\n",
    "print(rp.mean(), '+/-', 2*np.sqrt(rp.var()))\n",
    "print(aa.mean(), '+/-', 2*np.sqrt(aa.var()))"
   ],
   "id": "71d81035-61f7-49c3-96a3-7f8a5b07dd00"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving an accept precision of $0.51 \\pm 0.13$, a reject precision of\n",
    "$0.82 \\pm 0.05$ and an agreed accept rate of $0.18 \\pm 0.07$. Note that\n",
    "the ‘random conference’ values of 1 in 4 for accept precision and 3 in 4\n",
    "for reject decisions are outside the two standard deviation error bars.\n",
    "If it is preferred medians and percentiles could also be computed from\n",
    "the samples above, but as we will see when we histogram the results the\n",
    "densities look broadly symmetric, so this is unlikely to have much\n",
    "effect."
   ],
   "id": "63b6e47f-3a34-44f0-b388-0b1f70f1a421"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Monte Carlo Results\n",
    "\n",
    "Just to ensure that the error bars are reflective of the underlying\n",
    "densities we histogram the Monte Carlo results for accept precision,\n",
    "reject precision and agreed accept below. Shown on each histogram is a\n",
    "line representing the result we would get for the ‘random committee’."
   ],
   "id": "0c936d47-8789-496d-9968-fbc9e5416d20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "_ = ax[0].hist(ap, 20)\n",
    "_ = ax[0].set_title('Accept Precision')\n",
    "ax[0].axvline(0.25, linewidth=4, color=\"r\")\n",
    "_ = ax[1].hist(rp, 20)\n",
    "_ = ax[1].set_title('Reject Precision')\n",
    "ax[1].axvline(0.75, linewidth=4, color=\"r\")\n",
    "_ = ax[2].hist(aa, 20)\n",
    "_ = ax[2].set_title('Agreed Accept Rate')\n",
    "_ = ax[2].axvline(0.10, linewidth=4, color=\"r\")\n",
    "ma.write_figure(filename=\"random-committee-outcomes-vs-true.svg\", directory=\"./neurips\")"
   ],
   "id": "e8bf340f-f536-45a9-937f-6b505f9af3bc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/random-committee-outcomes-vs-true.svg\" class=\"\" width=\"90%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Different statistics for the random committee oucomes versus\n",
    "the observed committee outcomes.</i>"
   ],
   "id": "2a23d2aa-80f1-40ae-a324-942d67fa52e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Choice and Prior Values\n",
    "\n",
    "In the analysis above we’ve minimized the modeling choices: we made use\n",
    "of a Bayesian analysis to capture the uncertainty in counts that can be\n",
    "arising from statistical sampling error. To this end we chose an\n",
    "uninformative prior over these probabilities. However, one might argue\n",
    "that the prior should reflect something more about the underlying\n",
    "experimental structure: for example, we *know* that if the committees\n",
    "made their decisions independently it is unlikely that we’d obtain an\n",
    "inconsistency figure much greater than 37.5% because that would require\n",
    "committees to explicitly collude to make inconsistent decisions: the\n",
    "random conference is the worst case. Due to the accept rate, we also\n",
    "expect a larger number of reject decisions than reject. This also isn’t\n",
    "captured in our prior. Such questions move us into the realms of\n",
    "modeling the process, rather than performing a sensitivity analysis.\n",
    "However, if we wish to model the decision process as a whole, we have a\n",
    "lot more information available, and we should make use of it. The\n",
    "analysis above is intended to exploit our randomized experiment to\n",
    "explore how inconsistent we expect two committees to be. It focusses on\n",
    "that single question; it doesn’t attempt to give answers on what the\n",
    "reasons for that inconsistency are and how it may be reduced. The\n",
    "additional maths was needed only to give a sense of the uncertainty in\n",
    "the figures. That uncertainty arises due to the limited number of papers\n",
    "in the experiment.\n",
    "\n",
    "<!--include{_neurips/includes/neurips-experiment.md}-->"
   ],
   "id": "21ca8886-eff1-4dbe-86ef-aae0a1d9bdb2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewer Calibration\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-reviewer-calibration.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-reviewer-calibration.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Calibration of reviewers is the process where different interpretations\n",
    "of the reviewing scale are addressed. The tradition of calibration goes\n",
    "at least as far back as John Platt’s Program Chairing, and included a\n",
    "Bayesian model by Ge, Welling and Ghahramani at NeurIPS 2013."
   ],
   "id": "ec5a06f4-5641-4d33-94a4-55fddfed2f3f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewer Calibration Model\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/reviewer-calibration-model.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/reviewer-calibration-model.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In this note book we deal with reviewer calibration. Our assumption is\n",
    "that the score from the $j$th reviwer for the $i$th paper is given by $$\n",
    "y_{i,j} = f_i + b_j + \\epsilon_{i, j}\n",
    "$$ where $f_i$ is the ‘objective quality’ of paper $i$ and $b_j$ is an\n",
    "offset associated with reviewer $j$. $\\epsilon_{i,j}$ is a subjective\n",
    "quality estimate which reflects how a specific reviewer’s opinion\n",
    "differs from other reviewers (such differences in opinion may be due to\n",
    "differing expertise or perspective). The underlying ‘objective quality’\n",
    "of the paper is assumed to be the same for all reviewers and the\n",
    "reviewer offset is assumed to be the same for all papers.\n",
    "\n",
    "If we have $n$ papers and $m$ reviewers, then this implies $n$ + $m$ +\n",
    "$nm$ values need to be estimated. Naturally this is too many, and we can\n",
    "start by assuming that the subjective quality is drawn from a normal\n",
    "density with variance $\\sigma^2$ $$\n",
    "\\epsilon_{i, j} \\sim N(0, \\sigma^2 \\mathbf{I})\n",
    "$$ which reduces us to $n$ + $m$ + 1 parameters. Further we can assume\n",
    "that the objective quality is also normally distributed with mean $\\mu$\n",
    "and variance $\\alpha_f$, $$\n",
    "f_i \\sim N(\\mu, \\alpha_f)\n",
    "$$ this now reduces us to $m$+3 parameters. However, we only have\n",
    "approximately $4m$ observations (4 papers per reviewer) so parameters\n",
    "may still not be that well determined (particularly for those reviewers\n",
    "that have only one review). We, therefore, finally, assume that reviewer\n",
    "offset is normally distributed with zero mean, $$\n",
    "b_j \\sim N(0, \\alpha_b),\n",
    "$$ leaving us only four parameters: $\\mu$, $\\sigma^2$, $\\alpha_f$ and\n",
    "$\\alpha_b$. Combined together these three assumptions imply that $$\n",
    "\\mathbf{y} \\sim N(\\mu \\mathbf{1}, \\mathbf{K}),\n",
    "$$ where $\\mathbf{y}$ is a vector of stacked scores $\\mathbf{1}$ is the\n",
    "vector of ones and the elements of the covariance function are given by\n",
    "$$\n",
    "k(i,j; k,l) = \\delta_{i,k} \\alpha_f + \\delta_{j,l} \\alpha_b + \\delta_{i, k}\\delta_{j,l} \\sigma^2,\n",
    "$$ where $i$ and $j$ are the index of first paper and reviewer and $k$\n",
    "and $l$ are the index of second paper and reviewer. The mean is easily\n",
    "estimated by maximum likelihood and is given as the mean of all scores.\n",
    "\n",
    "It is convenient to reparametrize slightly into an overall scale\n",
    "$\\alpha_f$, and normalized variance parameters, $$\n",
    "k(i,j; k,l) = \\alpha_f\\left(\\delta_{i,k}  + \\delta_{j,l} \\frac{\\alpha_b}{\\alpha_f} + \\delta_{i, k}\\delta_{j,l} \\frac{\\sigma^2}{\\alpha_f}\\right)\n",
    "$$ which we rewrite to give two ratios: offset/signal ratio,\n",
    "$\\hat{\\alpha}_b$ and noise/signal $\\hat{\\sigma}^2$ ratio. $$\n",
    "k(i,j; k,l) = \\alpha_f\\left(\\delta_{i,k}  + \\delta_{j,l} \\hat{\\alpha}_b + \\delta_{i, k}\\delta_{j,l} \\hat{\\sigma}^2\\right)\n",
    "$$ The advantage of this parameterization is it allows us to optimize\n",
    "$\\alpha_f$ directly (with a fixed-point equation) and it will be very\n",
    "well determined. This leaves us with two free parameters, that we can\n",
    "explore on the grid. It is in these parameters that we expect the\n",
    "remaining underdetermindness of the model. We expect $\\alpha_f$ to be\n",
    "well determined because the negative log likelihood is now $$\n",
    "\\frac{|\\mathbf{y}|}{2}\\log\\alpha_f + \\frac{1}{2}\\log  \\left|\\hat{\\mathbf{K}}\\right| + \\frac{1}{2\\alpha_f}\\mathbf{y}^\\top \\hat{\\mathbf{K}}^{-1} \\mathbf{y},\n",
    "$$ where $|\\mathbf{y}|$ is the length of $\\mathbf{y}$ (i.e. the number\n",
    "of reviews) and $\\hat{\\mathbf{K}}=\\alpha_f^{-1}\\mathbf{K}$ is the scale\n",
    "normalized covariance. This negative log likelihood is easily minimized\n",
    "to recover $$\n",
    "\\alpha_f = \\frac{1}{|\\mathbf{y}|} \\mathbf{y}^\\top \\hat{\\mathbf{K}}^{-1} \\mathbf{y}.\n",
    "$$ A Bayesian analysis of this parameter is possible with gamma priors,\n",
    "but it would merely show that this parameter is extremely well\n",
    "determined (the degrees of freedom parameter of the associated\n",
    "Student-$t$ marginal likelihood scales will the number of reviews, which\n",
    "will be around $|\\mathbf{y}| \\approx 6,000$ in our case.\n",
    "\n",
    "So, we propose to proceed as follows. Set the mean from the reviews\n",
    "($\\mu$) and then choose a two-dimensional grid of parameters for\n",
    "reviewer offset and diversity. For each parameter choice, optimize to\n",
    "find $\\alpha_f$ and then evaluate the liklihood. Worst case this will\n",
    "require us inverting $\\hat{\\mathbf{K}}$, but if the reviewer paper\n",
    "groups are disconnected, it can be done a lot quicker. Next stage is to\n",
    "load in the reviews for analysis."
   ],
   "id": "a937a36c-e275-4e18-b26a-d5f69407da8f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/reviewer-calibration-fit-model.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/reviewer-calibration-fit-model.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "683eaf44-18e4-48fd-a0cc-8bc61c24ee48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmtutils as cu\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import GPy\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.linalg import solve_triangular "
   ],
   "id": "99bb6e5f-4b9f-4cb2-8a51-0c2bf2c28de8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2014-09-06'"
   ],
   "id": "e1257b1d-85fb-4d46-af75-d9db7a387c57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Data"
   ],
   "id": "d8829ac9-bb05-4fc3-9e62-a6a2407aa490"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = date + '_reviews.xls'\n",
    "reviews = cu.CMT_Reviews_read(filename=filename)\n",
    "papers = list(sorted(set(reviews.reviews.index), key=int))\n",
    "reviews.reviews = reviews.reviews.loc[papers]"
   ],
   "id": "d7e14e3f-28cd-4d78-9944-2061f03e11eb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood solution for $\\mu$ is simply the mean quality of\n",
    "the papers, this is easily computed."
   ],
   "id": "58c4d36b-78eb-4b64-a9a2-d5443f9d6f4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = reviews.reviews.Quality.mean()\n",
    "print(\"Mean value, mu = \", mu)"
   ],
   "id": "d09a6341-8e00-4142-b286-f02e8371aaa6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We take the reviews, which are indexed by the paper number, and create a\n",
    "new data frame, that indexes by paper id and email combined. From these\n",
    "reviews we tokenize the `PaperID` and the `Email` to extract two\n",
    "matrices that can be used in creation of covariance matrices. We also\n",
    "create a target vector which is the mean centred vector of scores."
   ],
   "id": "f6be4309-ccb8-421a-b660-1dcdabdab5bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reviews.reviews.reset_index()\n",
    "r.rename(columns={'ID':'PaperID'}, inplace=True)\n",
    "r.index = r.PaperID + '_' + r.Email\n",
    "X1 = pd.get_dummies(r.PaperID)\n",
    "X1 = X1[sorted(X1.columns, key=int)]\n",
    "X2 = pd.get_dummies(r.Email)\n",
    "X2 = X2[sorted(X2.columns, key=str.lower)]\n",
    "y = reviews.reviews.Quality - mu"
   ],
   "id": "25550939-55d0-4871-bea7-1de0abff2655"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Model in GPy\n",
    "\n",
    "Having reduced the model to two parameters, I was hopeful I could set\n",
    "parameters broadly by hand. My initial expectation was that `alpha_b`\n",
    "and `sigma2` would both be less than 1, but some playing with parameters\n",
    "showed this wasn’t the case. Rather than waste further time, I decided\n",
    "to use our [`GPy` Software](https://github.com/SheffieldML/GPy) (see\n",
    "below) to find a maximum likelihood solution for the parameters.\n",
    "\n",
    "Model construction firstly involves constructing covariance functions\n",
    "for the model and concatenating `X1` and `X2` to a new input matrix `X`."
   ],
   "id": "f4d78480-8caa-48c3-9571-5524747b9819"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X1.join(X2)\n",
    "kern1 = GPy.kern.Linear(input_dim=len(X1.columns), active_dims=np.arange(len(X1.columns)))\n",
    "kern1.name = 'K_f'\n",
    "kern2 = GPy.kern.Linear(input_dim=len(X2.columns), active_dims=np.arange(len(X1.columns), len(X.columns)))\n",
    "kern2.name = 'K_b'"
   ],
   "id": "755b29e1-ed64-46f3-aed6-5734ee5c2c3b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the covariance function is used to create a Gaussian process\n",
    "regression model with `X` as input and `y` as target. The covariance\n",
    "function is given by $\\mathbf{K}_f + \\mathbf{K}_b$."
   ],
   "id": "075b4857-0ebf-466e-ab66-8c757820d58d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X, y.to_numpy()[:, np.newaxis], kern1+kern2)\n",
    "model.optimize()"
   ],
   "id": "ed0ec0b4-71a0-4269-963c-2093f7c5efca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the parameters of the result."
   ],
   "id": "ed2e2658-d5f1-461e-beec-6ee1e0986d02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(model.log_likelihood())"
   ],
   "id": "01ef4b33-f1c3-40ca-b5e6-5e89550a62ec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Name : GP regression\n",
    "        Objective : 10071.679092815619\n",
    "        Number of Parameters : 3\n",
    "        Number of Optimization Parameters : 3\n",
    "        Updates : True\n",
    "        Parameters:\n",
    "          GP_regression.           |               value  |  constraints  |  priors\n",
    "          sum.K_f.variances        |  1.2782303448777643  |      +ve      |        \n",
    "          sum.K_b.variances        |  0.2400098787580176  |      +ve      |        \n",
    "          Gaussian_noise.variance  |  1.2683656892796749  |      +ve      |        \n",
    "        -10071.679092815619"
   ],
   "id": "613ea795-3bc6-4083-aa4c-1dd14264b2cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Model Without GPy\n",
    "\n",
    "The answer from the GPy solution is introduced here, alongside the code\n",
    "where the covariance matrices are explicitly created (above they are\n",
    "created using GPy’s high level code for kernel matrices, which may be\n",
    "less clear on the details)."
   ],
   "id": "189c3029-fb6c-4ebf-ac3f-74a32d8df04b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameter values to ML solutions given by GPy.\n",
    "alpha_f = model.sum.K_f.variances\n",
    "alpha_b = model.sum.K_b.variances/alpha_f\n",
    "sigma2 = model.Gaussian_noise.variance/alpha_f"
   ],
   "id": "45e76d91-f074-457b-845b-f347ecd528de"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the covariance functions based on the tokenized paper IDs\n",
    "and emails."
   ],
   "id": "c42c1c84-62e4-4650-bf25-bcb15394e055"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_f = np.dot(X1, X1.T)\n",
    "K_b = alpha_b*np.dot(X2, X2.T)\n",
    "K = K_f + K_b + sigma2*np.eye(X2.shape[0])\n",
    "Kinv, L, Li, logdet = GPy.util.linalg.pdinv(K) # since we have GPy loaded in use their positive definite inverse.\n",
    "y = reviews.reviews.Quality - mu\n",
    "alpha = np.dot(Kinv, y)\n",
    "yTKinvy = np.dot(y, alpha)\n",
    "alpha_f = yTKinvy/len(y)"
   ],
   "id": "e1c94b63-9ccf-409d-9636-c8d47afff945"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have removed the data mean, the log likelihood we are\n",
    "interested in is the likelihood of a multivariate Gaussian with\n",
    "covariance $\\mathbf{K}$ and mean zero. This is computed below."
   ],
   "id": "c5236f43-0053-4924-a79b-288cc08b9334"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = 0.5*len(y)*np.log(2*np.pi*alpha_f) + 0.5*logdet + 0.5*yTKinvy/alpha_f \n",
    "print(\"negative log likelihood: \", ll)"
   ],
   "id": "5f4a82be-549f-459a-bbdf-bf8323e9f08f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Quality Prediction\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/review-quality-prediction.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/review-quality-prediction.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Now we wish to predict the bias corrected scores for the papers. That\n",
    "involves considering a variable $s_{i,j} = f_i + e_{i,j}$ which is the\n",
    "score with the bias removed. That variable has a covariance matrix,\n",
    "$\\mathbf{K}_s=\\mathbf{K}_f + \\sigma^2 \\mathbf{I}$ and a cross covariance\n",
    "between $\\mathbf{y}$ and $\\mathbf{s}$ is also given by $\\mathbf{K}_s$.\n",
    "This means we can compute the posterior distribution of the scores as\n",
    "follows:"
   ],
   "id": "a328b608-4c75-4231-9606-2930818af1d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and covariance of quality scores\n",
    "K_s = K_f + np.eye(K_f.shape[0])*sigma2\n",
    "s = pd.Series(np.dot(K_s, alpha) + mu, index=X1.index)\n",
    "covs = alpha_f*(K_s - np.dot(K_s, np.dot(Kinv, K_s)))"
   ],
   "id": "88f21c07-80c6-4482-a43d-922c54a32ad5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulations for Probability of Acceptance\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/paper-acceptance-monte-carlo.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/paper-acceptance-monte-carlo.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "We can now sample from this posterior distribution of bias-adjusted\n",
    "scores jointly, to get a set of scores for all papers. For this set of\n",
    "scores, we can perform a ranking and accept the top 400 papers. This\n",
    "gives us a sampled conference. If we do that 1,000 times then we can see\n",
    "how many times each paper was accepted to get a probability of\n",
    "acceptance."
   ],
   "id": "70810f18-1b8c-4b73-a702-69834bb6395c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_accepts = 420 # 440 because of the 10% replication"
   ],
   "id": "bea90a3b-a358-4759-97c0-931066ea1dcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place this in a separate box, because sampling can take a while.\n",
    "samples = 1000\n",
    "score = np.random.multivariate_normal(mean=s, cov=covs, size=samples).T\n",
    "# Use X1 which maps papers to paper/reviewer pairings to get the average score for each paper.\n",
    "paper_score = pd.DataFrame(np.dot(np.diag(1./X1.sum(0)), np.dot(X1.T, score)), index=X1.columns)"
   ],
   "id": "281d3148-0aee-46f1-80f4-27e0bbe907ea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the probability of acceptance for each of the sampled\n",
    "rankings."
   ],
   "id": "471f70e8-fecd-460a-8d73-1c1783d177e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_accept = ((paper_score>paper_score.quantile(1-(float(number_accepts)/paper_score.shape[0]))).sum(1)/1000)\n",
    "prob_accept.name = 'AcceptProbability'"
   ],
   "id": "84220b0c-8db2-4314-b9d9-a9bb20865106"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the probability of accepts, we can decide on the boundaries\n",
    "of the grey area. These are set in `lower` and `upper`. The grey area is\n",
    "those papers that will be debated most heavily during the\n",
    "teleconferences between program chairs and area chairs."
   ],
   "id": "25aa61f6-588c-4676-a40e-64c54ec5d964"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower=0.1\n",
    "upper=0.9\n",
    "grey_area = ((prob_accept>lower) & (prob_accept<upper))\n",
    "print('Number of papers in grey area:', grey_area.sum())"
   ],
   "id": "df8a2862-47af-4e0e-be44-3ed028e738f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cmtutils.plot as plot"
   ],
   "id": "1331e6a4-da23-4409-89a3-a2e05a563bc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "print('Expected Papers Accepted:', prob_accept.sum())\n",
    "_ = prob_accept.hist(bins=40, ax=ax)\n",
    "ma.write_figure(directory=\"./neurips\", filename=\"probability-of-accept.svg\")"
   ],
   "id": "39f6e17b-b593-499c-8e2f-84f287506804"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/probability-of-accept.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Histogram of the probability of accept as estimated by the\n",
    "Monte Carlo simulation across all papers submitted to NeurIPS 2014.</i>"
   ],
   "id": "8f72fa54-5e4a-49f4-8a23-e8a68ad08903"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Sanity Checking Plots\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/calibration-sanity-checks.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/calibration-sanity-checks.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Here is the histogram of the reviewer scores after calibration."
   ],
   "id": "56d4a91d-31bc-48e9-adb4-52e467affb6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "s.hist(bins=100, ax=ax)\n",
    "_ = ax.set_title('Calibrated Reviewer Scores')\n",
    "ma.write_figure(directory=\"./neurips\", filename=\"calibrated-reviewer-scores.svg\")"
   ],
   "id": "fd553193-4f8d-41e0-bdba-5a06ddd908b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/calibrated-reviewer-scores.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Histogram of updated reviewer scores after the calibration\n",
    "process is applied.</i>"
   ],
   "id": "703db191-7d9e-46e1-abcf-03ddd51d0c0e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustments to Reviewer Scores\n",
    "\n",
    "We can also compute the posterior distribution for the adjustments to\n",
    "the reviewer scores."
   ],
   "id": "f046323d-7a0d-46a4-ba86-856c12d4b425"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and covariance of review biases\n",
    "b = pd.Series(np.dot(K_b, alpha), index=X2.index)\n",
    "covb = alpha_f*(K_b - np.dot(K_b, np.dot(Kinv, K_b)))"
   ],
   "id": "87f60322-d4f2-423a-88e4-e86cf3404aed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_bias = pd.Series(np.dot(np.diag(1./X2.sum(0)), np.dot(X2.T, b)), index=X2.columns, name='ReviewerBiasMean')\n",
    "reviewer_bias_std = pd.Series(np.dot(np.diag(1./X2.sum(0)), np.dot(X2.T, np.sqrt(np.diag(covb)))), index=X2.columns, name='ReviewerBiasStd')"
   ],
   "id": "a9c70eaf-3e33-4c01-89ff-05308e46dfef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a histogram of the mean adjustment for the reviewers."
   ],
   "id": "11185b2e-5548-4bb6-a94a-202182fb76fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "reviewer_bias.hist(bins=100, ax=ax)\n",
    "_ = ax.set_title('Reviewer Calibration Adjustments Histogram')\n",
    "ma.write_figure(directory=\"./neurips\", filename=\"reviewer-calibration-adjustments.svg\")"
   ],
   "id": "9a5108db-b7d6-42fc-8773-502c240276b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/reviewer-calibration-adjustments.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Histogram of individual offsets associated with the reviewers\n",
    "as estimated by the model.</i>\n",
    "\n",
    "Export a version of the bias scores for use in CMT."
   ],
   "id": "dd5a7d70-5480-4317-8b46-e2bdc484ceeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_export = pd.DataFrame(data={'Quality Score - Does the paper deserves to be published?':reviewer_bias, \n",
    "                   'Impact Score - Independently of the Quality Score above, this is your opportunity to identify papers that are very different, original, or otherwise potentially impactful for the NIPS community.':pd.Series(np.zeros(len(reviewer_bias)), index=reviewer_bias.index),\n",
    "                    'Confidence':pd.Series(np.zeros(len(reviewer_bias)), index=reviewer_bias.index)})\n",
    "cols = bias_export.columns.tolist()\n",
    "cols = [cols[2], cols[1], cols[0]]\n",
    "bias_export = bias_export[cols]\n",
    "#bias_export.to_csv(os.path.join(cu.cmt_data_directory, 'reviewer_bias.csv'), sep='\\t', header=True, index_label='Reviewer Email')"
   ],
   "id": "a0f1d41c-deff-40ea-bfeb-14b649f29705"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "As a sanity check Corinna suggested it makes sense to plot the average\n",
    "raw score for the papers vs the probability of accept, just to ensure\n",
    "nothing weird is going on. To clarify the plot, I’ve actually plotted\n",
    "raw score vs log odds of accept."
   ],
   "id": "5c529eac-7656-4ffc-888b-50038140bc28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_score = pd.Series(np.dot(np.diag(1./X1.sum(0)), np.dot(X1.T, r.Quality)), index=X1.columns)\n",
    "prob_accept[prob_accept==0] = 1/(10*samples)\n",
    "prob_accept[prob_accept==1] = 1-1/(10*samples)"
   ],
   "id": "cb43f623-7f9c-46b6-95a9-8c2eb60854be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(raw_score, np.log(prob_accept)- np.log(1-prob_accept), 'rx')\n",
    "ax.set_title('Raw Score vs Log odds of accept')\n",
    "ax.set_xlabel('raw score')\n",
    "_ = ax.set_ylabel('log odds of accept')\n",
    "ma.write_figure(directory=\"./neurips\", filename=\"raw-score-vs-log-odds.svg\")"
   ],
   "id": "fc61b398-4113-403c-bae7-edf896e82cc9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/raw-score-vs-log-odds.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Scatter plot of the raw paper score against the log\n",
    "probability of paper acceptance, as estimated by Monte Carlo\n",
    "simulation.</i>"
   ],
   "id": "b677441b-878c-45ba-a7ca-23619839d8cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibraton Quality Sanity Checks"
   ],
   "id": "48806419-c2b0-4482-9af6-01946b933724"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.name = 'CalibratedQuality'\n",
    "r = r.join(s)"
   ],
   "id": "e1f3c5af-c863-45af-9e9d-6bed459e757e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at a scatter plot of the review quality vs the\n",
    "calibrated quality."
   ],
   "id": "714473a0-b7b9-407d-bbd1-de422a13f012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.plt as plt\n",
    "import cmtutils.plot as plot"
   ],
   "id": "a7d7abc3-791a-46f5-83a1-a6940e4a6eb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(r.Quality, r.CalibratedQuality, 'r.', markersize=10)\n",
    "ax.set_xlim([0, 11])\n",
    "ax.set_xlabel('original review score')\n",
    "_ = ax.set_ylabel('calibrated review score')\n",
    "ma.write_figure(directory=\"./neurips\", filename=\"calibrated-review-score-vs-original-score.svg\")"
   ],
   "id": "8f18a277-286b-461b-9fef-2e8ed8ecb53e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/calibrated-review-score-vs-original-score.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Scatter plot of the calibrated review scores against the\n",
    "original review scores.</i>"
   ],
   "id": "f2fb3e4b-a96c-4e5f-8b05-efb3017d11bc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of Duplicate Papers\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/calibration-correlation-of-duplicate-papers.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/calibration-correlation-of-duplicate-papers.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "For NeurIPS 2014 we experimented with duplicate papers: we pushed papers\n",
    "through the system twice, exposing them to different subsets of the\n",
    "reviewers. The first thing we’ll look at is the duplicate papers.\n",
    "Firstly, we identify them by matching on title."
   ],
   "id": "65eaf6ff-61ce-4a32-bf37-6d93ebf94ed6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = date + '_paper_list.xls'\n",
    "papers = cu.CMT_Papers_read(filename=filename)\n",
    "duplicate_list = []\n",
    "for ID, title in papers.papers.Title.iteritems():\n",
    "    if int(ID)>1779 and int(ID) != 1949:\n",
    "        pair = list(papers.papers[papers.papers['Title'].str.contains(papers.papers.Title[ID].strip())].index)\n",
    "        pair.sort(key=int)\n",
    "        duplicate_list.append(pair)"
   ],
   "id": "4456b767-6b6d-4b1a-8594-dad68b53fec0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the correlation coefficients for the duplicated papers\n",
    "for the average impact and quality scores."
   ],
   "id": "edc7d21f-32c1-46d8-898d-966e0314f676"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = []\n",
    "calibrated_quality = []\n",
    "accept = []\n",
    "impact = []\n",
    "confidence = []\n",
    "for duplicate_pair in duplicate_list:\n",
    "    quality.append([np.mean(r[r.PaperID==duplicate_pair[0]].Quality), np.mean(r[r.PaperID==duplicate_pair[1]].Quality)])\n",
    "    calibrated_quality.append([np.mean(r[r.PaperID==duplicate_pair[0]].CalibratedQuality), np.mean(r[r.PaperID==duplicate_pair[1]].CalibratedQuality)])\n",
    "    impact.append([np.mean(r[r.PaperID==duplicate_pair[0]].Impact), np.mean(r[r.PaperID==duplicate_pair[1]].Impact)])\n",
    "    confidence.append([np.mean(r[r.PaperID==duplicate_pair[0]].Conf), np.mean(r[r.PaperID==duplicate_pair[1]].Conf)])\n",
    "quality = np.array(quality)\n",
    "calibrated_quality = np.array(calibrated_quality)\n",
    "impact = np.array(impact)\n",
    "confidence = np.array(confidence)\n",
    "quality_cor = np.corrcoef(quality.T)[0, 1]\n",
    "calibrated_quality_cor = np.corrcoef(calibrated_quality.T)[0, 1]\n",
    "impact_cor = np.corrcoef(impact.T)[0, 1]\n",
    "confidence_cor = np.corrcoef(confidence.T)[0, 1]\n",
    "print(\"Quality correlation: \", quality_cor)\n",
    "print(\"Calibrated Quality correlation: \", calibrated_quality_cor)\n",
    "print(\"Impact correlation: \", impact_cor)\n",
    "print(\"Confidence correlation: \", confidence_cor)"
   ],
   "id": "5e201129-54cc-4d71-b75f-6f9f80688b13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Quality correlation:  0.54403674862622\n",
    "        Calibrated Quality correlation:  0.5455958618174274\n",
    "        Impact correlation:  0.26945269236041036\n",
    "        Confidence correlation:  0.3854251559444674"
   ],
   "id": "5ff2593b-b28e-4fef-8c6e-18068970f824"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plots\n",
    "\n",
    "To visualize the quality score correlation, we plot the group 1 papers\n",
    "against the group 2 papers. Here we add a small amount of jitter to\n",
    "ensure points to help visualize points that would otherwise fall on the\n",
    "same position."
   ],
   "id": "35a9cb68-28f5-4c8e-9a15-e65d93309827"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.plot(quality[:, 0]+np.random.randn(quality.shape[0])*0.06125, quality[:, 1]+np.random.randn(quality.shape[0])*0.06125, 'r.', markersize=10)\n",
    "lims = [1.5, 8.5]\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "ax.plot(lims, lims, 'r-')\n",
    "_ = ax.set_title(Correlation: {cor:.2g}'.format(cor=quality_cor))\n",
    "ma.write_figure(directory=\"./neurips\",\n",
    "                filename=\"quality-correlation.svg\")"
   ],
   "id": "3b2fb84b-065e-445c-a314-48b5b22aade8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/quality-correlation.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Correlation between reviewer scores across the duplicated\n",
    "committees (scores have jitter added to prevent too many points sitting\n",
    "on top of each other).</i>\n",
    "\n",
    "Similarly for the calibrated quality of the papers."
   ],
   "id": "245f12bd-be80-4537-bc6c-6da5d442ffe8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.plot(calibrated_quality[:, 0]+np.random.randn(calibrated_quality.shape[0])*0.06125, calibrated_quality[:, 1]+np.random.randn(calibrated_quality.shape[0])*0.06125, 'r.', markersize=10)\n",
    "lims = [1.5, 8.5]\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "ax.plot(lims, lims, 'r-')\n",
    "_ = ax.set_title('Correlation: {cor:.2g}'.format(cor=calibrated_quality_cor))\n",
    "ma.write_figure(directory=\"./neurips\",\n",
    "                filename=\"calibrated-quality-correlation.svg\")"
   ],
   "id": "90fd46bd-8e7b-4626-83a2-6ea4643f4920"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/calibrated-quality-correlation.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Correlation between calibrated reviewer scores across the two\n",
    "independent committees.</i>"
   ],
   "id": "ce3ce048-aea9-4b8a-bc8e-106123617c79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Laplace smoothing to accept probabilities before incorporating them.\n",
    "revs = r.join((prob_accept+0.0002)/1.001, on='PaperID').join(reviewer_bias, on='Email').join(papers.papers['Number Of Discussions'], on='PaperID').join(reviewer_bias_std, on='Email').sort_values(by=['AcceptProbability','PaperID', 'CalibratedQuality'], ascending=False)\n",
    "revs.set_index(['PaperID'], inplace=True)\n",
    "def len_comments(x):\n",
    "    return len(x.Comments)\n",
    "revs['comment_length']=revs.apply(len_comments, axis=1)\n",
    "# Save the computed information to disk\n",
    "#revs.to_csv(os.path.join(cu.cmt_data_directory, date + '_processed_reviews.csv'), encoding='utf-8')"
   ],
   "id": "caceee11-cd87-42f3-999e-e9df3055e1a4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conference Simulation\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-simulation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-simulation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Given the realization that roughly 50% of the score seems to be\n",
    "‘subjective’ and 50% of the score seems to be ‘objective’, then we can\n",
    "simulate the conference and see what it does for the accept precision\n",
    "for different probability of accept.\n",
    "\n",
    "To explore the effect of the subjective scoring on the accept precision\n",
    "we construct a simple simulation that scores hypothetical papers with\n",
    "random values drawn from a Gaussian density. Each paper has an\n",
    "underlying objective score (shared across the hypothetical reviewers),\n",
    "and then alongside it there are Gaussian variables drawn independently\n",
    "at random to represent the subjectivity of the hypothetical reviewers.\n",
    "\n",
    "Each paper is rated by two independent committees, and the papers are\n",
    "reordered to accept the top $x$% where $x$ is our chosen accept rate. We\n",
    "can then use sample based estimates for the resulting accept precision.\n",
    "\n",
    "In these experiments the scores are taken to be 50% subjective and 50%\n",
    "objective, in line with the results we see from the NeurIPS 2014\n",
    "calibration model. We vary the number of reviewers in the simulation to\n",
    "see the effect of increasing reviewers on the accept precision."
   ],
   "id": "68406658-06e6-474c-9ba7-9d42c0a6521b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "id": "7f453280-3cfe-4ca7-831b-f54d54528d03"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the experiment `samples` number of times, here we’ve set this\n",
    "to be 100000. The subjectivity portion gives how much of the scores for\n",
    "each paper is subjective."
   ],
   "id": "9db9968c-4e99-4c6c-8540-ecdf81e3979b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_papers = 100000\n",
    "subjectivity_portion = 0.5"
   ],
   "id": "151e3ade-0333-43c9-b027-0242ef9ff1a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_rates = [0.05, 0.1, 0.15, 0.2, 0.25, \n",
    "                      0.3, 0.35, 0.4, 0.45, 0.5, \n",
    "                      0.55, 0.6, 0.65, 0.7, 0.75, \n",
    "                      0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "all_accepts = []\n",
    "for num_reviewers in range(1,7):\n",
    "    consistent_accepts = []\n",
    "    for accept_rate in accept_rates:\n",
    "        objective = (1-subjectivity_portion)*np.random.randn(num_papers) \n",
    "        subjective_0 = subjectivity_portion*np.random.randn(num_papers, num_reviewers).mean(1)\n",
    "        subjective_1 = subjectivity_portion*np.random.randn(num_papers, num_reviewers).mean(1)\n",
    "        score_0 = objective + subjective_0    \n",
    "        score_1 = objective + subjective_1\n",
    "\n",
    "        accept_0 = score_0.argsort()[:int(num_papers*accept_rate)]\n",
    "        accept_1 = score_1.argsort()[:int(num_papers*accept_rate)]\n",
    "\n",
    "        consistent_accept = len(set(accept_0).intersection(set(accept_1)))\n",
    "        consistent_accepts.append(consistent_accept/(num_papers*accept_rate))\n",
    "        print('Percentage consistently accepted: {prop}'.format(prop=consistent_accept/(num_papers*accept_rate)))\n",
    "\n",
    "    all_accepts.append(consistent_accepts)\n",
    "all_accepts = np.array(all_accepts)\n",
    "consistent_accepts = np.array(consistent_accepts)\n",
    "accept_rate = np.array(accept_rate)"
   ],
   "id": "0e91b3f4-0968-4c2a-b9c4-392e2f314d78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import mlai.plot as plot\n",
    "from cycler import cycler\n",
    "monochrome = (cycler('color', ['k']) * cycler('linestyle', ['-', '--', ':']) * cycler('marker', ['^','o', 's']))"
   ],
   "id": "58d9f44c-a15f-4061-9046-72a9602c92b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.set_prop_cycle(monochrome)\n",
    "\n",
    "ax.plot(accept_rates, accept_rates, \"k-\", linewidth=2)\n",
    "ax.plot(accept_rates, all_accepts.T, markersize=7)\n",
    "ax.legend(['random', '1 reviewer', '2 reviewers', '3 reviewers', '4 reviewers', '5 reviewers', '6 reviewers'])\n",
    "ax.set_xlabel(\"accept rate\")\n",
    "ax.set_ylabel(\"accept precision\")\n",
    "ax.axvline(0.23)\n",
    "ax.grid(True)\n",
    "mlai.write_figure(filename=\"accept-precision-vs-accept-rate.svg\",\n",
    "                  directory=\"./neurips/\")"
   ],
   "id": "8c6f62fa-3fe2-4860-a21e-ae544350c7bc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/accept-precision-vs-accept-rate.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Plot of the accept rate vs the accept precision for the\n",
    "conference for 50% subjectivity and different numbers of reviewers. The\n",
    "grey line gives the NeurIPS accept rate for 2014 of 23%.</i>\n",
    "\n",
    "In Figure we see the change in accept precision as we vary accept rate\n",
    "and number of reviewers for a conference where reviewers are 50%\n",
    "subjective."
   ],
   "id": "e8516c92-c8a1-489f-8975-fb0d1f301543"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.set_prop_cycle(monochrome)\n",
    "ax.plot(accept_rates, (all_accepts-accept_rates).T)\n",
    "ax.legend(['1 reviewer', '2 reviewers', '3 reviewers', '4 reviewers', '5 reviewers', '6 reviewers'])\n",
    "ax.set_xlabel(\"accept rate\")\n",
    "ax.set_ylabel(\"(accept precision)-(accept rate)\")\n",
    "mlai.write_figure(filename=\"gain-in-consistency.svg\",\n",
    "                  directory=\"./neurips/\")"
   ],
   "id": "1ac60b7e-792a-47ae-a634-0aca1b27aba7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/gain-in-consistency.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Plot of the accept rate vs gain in consistency over a random\n",
    "conference for 50% subjectivity.</i>\n",
    "\n",
    "Figure shows the accept rate against the gain in accept precision we\n",
    "have over the random committee."
   ],
   "id": "66f3b19d-33b4-4fe7-bb01-0d2542cf1d07"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do Rejected Papers Go?\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/where-do-the-rejected-papers-go.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/where-do-the-rejected-papers-go.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "One facet that we can explore is what the final fate of papers that are\n",
    "rejected by the conference is.\n",
    "\n",
    "Of the 1,678 papers submitted to NeurIPS 2014, only 414 were presented\n",
    "at the final conference. Here we trace the fate of the rejected papers,\n",
    "we searched Semantic Scholar for evidence of all 1,264 rejected papers.\n",
    "We looked for papers with similar titles and where the NeurIPS\n",
    "submission’s contact author was also in the author list. We were able to\n",
    "track down 680 papers.\n",
    "\n",
    "This code analyzes those 680 papers extracting their final publication\n",
    "venue using the Semantic Scholar API."
   ],
   "id": "10d9c9f9-c58e-4092-a59e-d266b8b9a5a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cmtutils"
   ],
   "id": "16537268-99ed-4990-8a03-1c3207a6ccee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmtutils.nipsy as nipsy"
   ],
   "id": "0caf67e1-6e8b-484e-aae7-fe00e15d103b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ],
   "id": "1408b800-efc0-41e7-9e4c-fe3a029e57b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(nipsy.review_store, nipsy.outlet_name_mapping), 'r') as f:\n",
    "    mapping = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "date = \"2021-06-11\"\n",
    "\n",
    "citations = nipsy.load_citation_counts(date=date)\n",
    "decisions = nipsy.load_decisions()\n",
    "nipsy.augment_decisions(decisions)\n",
    "joindf = nipsy.join_decisions_citations(decisions, citations)\n",
    "\n",
    "joindf['short_venue'] = joindf.venue.replace(mapping)"
   ],
   "id": "8c4fafa7-3012-4292-9fb9-3e31e6f56091"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/where-do-neurips-papers-go.html\" width=\"600\" height=\"450\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "\n",
    "Figure: <i>Sankey diagram showing the flow of NeurIPS papers through the\n",
    "system from submission to eventual publication.</i>\n",
    "\n",
    "Of the 680 papers 177 were only found on arXiv, 76 were found as PDFs\n",
    "online without a publication venue and 427 were published in other\n",
    "venues. The outlets that received ten or more papers from this group\n",
    "were AAAI (72 papers), AISTATS (57 papers), ICML (33 papers), CVPR (17\n",
    "papers), Later NeurIPS (15 papers), JMLR (14 papers), IJCAI (14 papers),\n",
    "ICLR (13 papers), UAI (11 papers). Opinion about quality of these\n",
    "different outlets will vary from individual, but from our perspective\n",
    "all of these outlets are \\`top-tier’ for machine learning and related\n",
    "areas. Other papers appeared at less prestigious outlets, and citation\n",
    "scores were also recored for papers that remained available only on\n",
    "ArXiv. Note that there is likely a bias towards outlets that have a\n",
    "submission deadline shortly after NeurIPS decisions are public,\n",
    "e.g. submission deadline for AAAI 2015 was six days after NeurIPS\n",
    "decisions were sent to authors. AISTATS has a submission deadline one\n",
    "month after.\n",
    "\n",
    "A Sankey diagram showing where papers submitted to the conference ended\n",
    "up is shown below."
   ],
   "id": "b597280c-c40c-4a87-9384-622674946e44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ],
   "id": "2e9e91ee-d6f5-4961-8016-5fd5d84783b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_to_show = 3\n",
    "\n",
    "label = ['submitted', 'oral', 'spotlight', 'poster', 'reject', '/dev/null']\n",
    "x = [0.1, 0.3, 0.3, 0.3, 0.3, 0.5]\n",
    "y = [0.4, 0.95, 0.9, 0.85, 0.3, 0.01]\n",
    "source = [0, 0, 0, 0, 4]\n",
    "target = [1, 2, 3, 4, 5]\n",
    "value = [(joindf['Status']=='Oral').sum(),\n",
    "         (joindf['Status']=='Spotlight').sum(), \n",
    "         (joindf['Status']=='Poster').sum(),\n",
    "         (joindf['Status']=='Reject').sum(),\n",
    "        joindf.loc[joindf.reject]['venue'].isna().sum()]\n",
    "\n",
    "venue_counts = joindf.loc[joindf.reject]['short_venue'].value_counts()\n",
    "venue_show = venue_counts[venue_counts>=thresh_to_show]\n",
    "target_val = target[-1]\n",
    "for venue,count in venue_show.items():\n",
    "    target_val += 1\n",
    "    value.append(count)\n",
    "    source.append(4)\n",
    "    label.append(venue)\n",
    "    target.append(target_val)\n",
    "    if venue=='ArXiv':\n",
    "        y.append(.15)\n",
    "        x.append(0.75)\n",
    "    \n",
    "    elif venue == 'None':\n",
    "        y.append(.20)\n",
    "        x.append(0.75)\n",
    "\n",
    "    else: \n",
    "        y.append(.27)\n",
    "        x.append(0.8)\n",
    "    \n",
    "\n",
    "    \n",
    "value.append(venue_counts[venue_counts<thresh_to_show].sum())\n",
    "source.append(4)\n",
    "label.append('other')\n",
    "target.append(target_val+1)\n",
    "x.append(0.85)\n",
    "y.append(1.0)\n",
    "\n",
    "link = dict(source = source, target = target, value = value)\n",
    "node = dict(label=label,\n",
    "            x = x,\n",
    "            y = y,\n",
    "            pad=12)\n",
    "data=go.Sankey(arrangement = \"snap\",\n",
    "                 link = link,\n",
    "                 node = node)"
   ],
   "id": "11042191-ba59-4446-b6ac-0ba335e41dd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(template=\"plotly_dark\")\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(\".\", \"neurips\", \"where-do-neurips-papers-go.dark.html\"))"
   ],
   "id": "a4afbfdb-193c-4426-b13d-20f0a0676962"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data,\n",
    "                layout = go.Layout(width=600,\n",
    "                height=450))\n",
    "fig.update_layout(template=\"plotly\", font=dict(\n",
    "        family=\"sans serif\",\n",
    "        size=14,\n",
    "        color=\"Black\"\n",
    "    ))\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(\".\", \"neurips\", \"where-do-neurips-papers-go.html\"))\n",
    "fig.write_image(os.path.join(\".\", \"neurips\", \"where-do-neurips-papers-go.svg\"))"
   ],
   "id": "44adc560-4ff5-4293-a759-57d6fd4af1db"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/where-do-neurips-papers-go.svg\" class=\"450\" width=\"600\" style=\"vertical-align:middle;\"><iframe src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/where-do-neurips-papers-go.html\" width=\"600\" height=\"450\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "\n",
    "Figure: <i>Sankey diagram showing the flow of NeurIPS papers through the\n",
    "system from submission to eventual publication.</i>"
   ],
   "id": "99ff58fb-36aa-4091-b811-38c73e53b2dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Papers Ten Years On\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/impact-of-papers-ten-years-on.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/impact-of-papers-ten-years-on.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Now we look at the actual impact of the papers published using the\n",
    "Semantic Scholar data base for tracking citations."
   ],
   "id": "ce926ff5-0518-4989-ac12-6767b28413c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ],
   "id": "4e9bd03d-4eea-4b76-9697-63b4c50f03f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmtutils as cu\n",
    "import cmtutils.nipsy as nipsy\n",
    "import cmtutils.plot as plot"
   ],
   "id": "20b0ef77-e978-4dbf-806b-bab59509114a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "3a9bcfff-d4b8-49ab-8084-b116e1b95e3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = cu.Papers()"
   ],
   "id": "69112522-1435-425f-a034-db94288b5dd4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<https://proceedings.neurips.cc/paper/2014>"
   ],
   "id": "9034a0ca-79b5-4ba1-8905-26aa937b6b40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_IMPACTS = False # Set to True to download impacts from Semantic Scholar"
   ],
   "id": "af1837af-2b6a-468a-853a-a3ab980bbaf1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact of the different papers is downloaded from Semantic scholar\n",
    "using their REST API. This can take some time, and they also throttle\n",
    "the calls. At the moment the code below deosn’t handle the throttling\n",
    "correctly. However, you it will load the cached version of of citations\n",
    "scores from the given date."
   ],
   "id": "7a20d2a2-b745-49f6-938f-e1703847502e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPDATE_IMPACTS:\n",
    "    from datetime import datetime\n",
    "    date=datetime.today().strftime('%Y-%m-%d')\n",
    "else:\n",
    "    date = \"2024-05-22\""
   ],
   "id": "f579ddae-9ca7-4620-b3a6-b6db195de7a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun to download impacts from Semantic Scholar\n",
    "if UPDATE_IMPACTS:\n",
    "    semantic_ids = nipsy.load_semantic_ids()\n",
    "    citations_dict = citations.to_dict(orient='index')\n",
    "    # Need to be a bit cleverer here. Semantic scholar will throttle this call.\n",
    "    sscholar = nipsy.download_citation_counts(citations_dict=citations_dict, semantic_ids=semantic_ids)\n",
    "    citations = pd.DataFrame.from_dict(citations_dict, orient=\"index\") \n",
    "    citations.to_pickle(date + '-semantic-scholar-info.pickle')\n",
    "else: \n",
    "    citations = nipsy.load_citation_counts(date=date)"
   ],
   "id": "b537106e-3f9e-4ac3-8685-23bae90cc720"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final decision sheet provides information about what happened to all\n",
    "of the papers."
   ],
   "id": "58e53462-5ecb-45a2-b3b5-0da7ceb20ab8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = nipsy.load_decisions()\n",
    "nipsy.augment_decisions(decisions)"
   ],
   "id": "c607e728-fbc7-4d49-ace3-374b50b417d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is joined with the citation information to provide our main ability\n",
    "to understand the impact of these papers."
   ],
   "id": "c68d4bf1-900c-49bd-8869-ca9615b356f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joindf = nipsy.join_decisions_citations(decisions, citations)"
   ],
   "id": "25cf6a24-62d6-4719-afe7-6be217056900"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of Quality Scores and Citation\n",
    "\n",
    "Our first study will be to check the correlation between quality scores\n",
    "of papers and how many times that the papers have been cited in\n",
    "practice. In the plot below, rejected papers are given as crosses,\n",
    "accepted papers are given as dots. We include all papers, whether\n",
    "published in a venue or just available through ArXiv or other preprint\n",
    "servers. We show the published/non-published quality scores and\n",
    "$\\log_{10}(1+\\text{citations})$ for all papers in the plot below. In the\n",
    "plot we are showing each point corrupted by some Laplacian noise and\n",
    "also removing axes. The idea is to give a sense of the distribution\n",
    "rather than reveal the score of a particular paper."
   ],
   "id": "858e8e67-44b3-480e-b627-e11f6c4824ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai as ma"
   ],
   "id": "4a225cc2-e802-4385-bdc5-c74ff6c423f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_calibrated_quality\"\n",
    "filter_col = \"all\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ax.set_xticks([])\n",
    "ma.write_figure(filename=date + \"-citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ],
   "id": "aca4de1c-71f0-4935-b921-21716bad14be"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2021-06-11-citations-vs-average-calibrated-quality-all.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation date from 2021. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average calibrated quality\n",
    "score for all papers. To prevent reidentification of individual papers\n",
    "quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2024-05-22-citations-vs-average-calibrated-quality-all.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation date from 2024. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average calibrated quality\n",
    "score for all papers. To prevent reidentification of individual papers\n",
    "quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "The correlation seems strong, but of course, we are looking at papers\n",
    "which were accepted and rejected by the conference. This is dangerous,\n",
    "as it is quite likely that presentation at the conference may provide\n",
    "some form of lift to the papers’ numbers of citations. So, the right\n",
    "thing to do is to look at the groups separately.\n",
    "\n",
    "Looking at the accepted papers only shows a very different picture.\n",
    "There is very little correlation between accepted papers’ quality scores\n",
    "and the number of citations they receive."
   ],
   "id": "d692c3a3-fbbe-476d-a1f7-dba83d36d83c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_calibrated_quality\"\n",
    "filter_col = \"accept\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=date + \"-citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ],
   "id": "0599e20a-fd1a-4c65-be62-de064511147f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2021-06-11-citations-vs-average-calibrated-quality-accept.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation date from 2021. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average calibrated quality\n",
    "score for accepted papers. To prevent reidentification of individual\n",
    "papers quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2024-05-22-citations-vs-average-calibrated-quality-accept.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation data from 2024. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average calibrated quality\n",
    "score for accepted papers. To prevent reidentification of individual\n",
    "papers quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "Conversely, looking at rejected papers only, we do see a slight trend,\n",
    "with higher scoring papers achieving more citations on average. This,\n",
    "combined with the lower average number of citations in the rejected\n",
    "paper group, alongside their lower average scores, explains the\n",
    "correlation we originally observed."
   ],
   "id": "f2da02ff-54cc-41a4-b2e6-5d64685d02b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_calibrated_quality\"\n",
    "filter_col = \"reject\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=date + \"-citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ],
   "id": "596dce2a-c232-4af5-82d0-d14b3adfba44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2021-06-11-citations-vs-average-calibrated-quality-reject.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation data from 2021. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average calibrated quality\n",
    "score for rejected papers. To prevent reidentification of individual\n",
    "papers quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2024-05-22-citations-vs-average-calibrated-quality-reject.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation data from 2024. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average calibrated quality\n",
    "score for rejected papers. To prevent reidentification of individual\n",
    "papers quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "Welling and Ghahramani introduced an “impact” score in NeurIPS 2013, we\n",
    "might expect the impact score to show correlation. And indeed, despite\n",
    "the lower range of the score (a reviewer can score either 1 or 2) we do\n",
    "see *some* correlation, although it is relatively weak."
   ],
   "id": "a39414a1-b5df-4e85-8eb0-487ec29d7ec8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"average_impact\"\n",
    "filter_col = \"accept\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=date + \"-citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ],
   "id": "99c60eae-da3d-40b9-bd7f-40a9d24204ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2021-06-11-citations-vs-average-impact-accept.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation data from 2021. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average impact score for\n",
    "accepted papers. To prevent reidentification of individual papers\n",
    "quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2024-05-22-citations-vs-average-impact-accept.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation data from 2024. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average impact score for\n",
    "accepted papers. To prevent reidentification of individual papers\n",
    "quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "Finally, we also looked at correlation between the *confidence* score\n",
    "and the impact. Here correlation is somewhat stronger. Why should\n",
    "confidence be an indicator of higher citations? A plausible explanation\n",
    "is that there is confounder driving both variables. For example, it\n",
    "might be that papers which are easier to understand (due to elegance of\n",
    "the idea, or quality of exposition) inspire greater reviewer confidence\n",
    "and increase the number of citations."
   ],
   "id": "1a0d0253-3589-42a7-9046-f150fd3d2357"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'average_confidence'\n",
    "filter_col = \"accept\"\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.log_one_citations(column, joindf, filt=joindf[filter_col], ax=ax)\n",
    "ma.write_figure(filename=date + \"-citations-vs-{col}-{filt}.svg\".format(filt=filter_col, col=column.replace(\"_\", \"-\")),\n",
    "                   directory=\"./neurips\")"
   ],
   "id": "3abca863-5727-4e62-a0d2-f467fe7aa0ae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2021-06-11-citations-vs-average-confidence-accept.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation data from 2021. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average confidence score for\n",
    "accepted papers. To prevent reidentification of individual papers\n",
    "quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>\n",
    "\n",
    "<img src=\"https://inverseprobability.com/talks/../slides/diagrams//neurips/2024-05-22-citations-vs-average-confidence-accept.svg\" class=\"\" width=\"70%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Citation data from 2024. Scatter plot of\n",
    "$\\log_{10}(1+\\text{citations})$ against the average confidence score for\n",
    "accepted papers. To prevent reidentification of individual papers\n",
    "quality scores and citation count, each point is corrupted by\n",
    "differentially private noise in the plot (correlation is computed before\n",
    "adding differentially private noise).</i>"
   ],
   "id": "d1c99ef9-6db0-4946-b233-51710efbead1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_index(df):\n",
    "    n = len(df.index)\n",
    "    return df.index[np.random.randint(n, size=n)]"
   ],
   "id": "cf9f94f5-842b-4fce-97c7-503e3a5d8c95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\"average_quality\", \"average_impact\", \"average_confidence\"]:\n",
    "    cor = []\n",
    "    for i in range(1000):\n",
    "        ind = bootstrap_index(joindf.loc[joindf.accept])\n",
    "        cor.append(joindf.loc[ind][column].corr(np.log(1+joindf.loc[ind]['numCitedBy'])))\n",
    "    cora = np.array(cor)\n",
    "    rho = cora.mean()\n",
    "    twosd = 2*np.sqrt(cora.var())\n",
    "    print(\"{column}\".format(column=column.replace(\"_\", \" \")))\n",
    "    print(\"Mean correlation is {rho} +/- {twosd}\".format(rho=rho, twosd=twosd))"
   ],
   "id": "e1b2d61e-fb70-4f54-a35b-47d8aab44227"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-conclusion.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_neurips/includes/neurips-experiment-conclusion.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Under the simple model we have outlined, we can be confident that there\n",
    "is inconsistency between two independent committees, but the level of\n",
    "inconsistency is much less than we would find for a random committee. If\n",
    "we accept that the bias introduced by the Area Chairs knowing when they\n",
    "were dealing with duplicates was minimal, then if we were to revisit the\n",
    "NIPS 2014 conference with an independent committee then we would expect\n",
    "between **38% and 64% of the presented papers to be the same**. If the\n",
    "conference was run at random, then we would only expect 25% of the\n",
    "papers to be the same.\n",
    "\n",
    "It’s apparent from comments and speculation about what these results\n",
    "mean, that some people might be surprised by the size of this figure.\n",
    "However, it only requires a little thought to see that this figure is\n",
    "likely to be large for any highly selective conference if there is even\n",
    "a small amount of inconsistency in the decision-making process. This is\n",
    "because once the conference has chosen to be ‘highly selective’ then\n",
    "because, by definition, only a small percentage of papers are to be\n",
    "accepted. Now if we think of a type I error as accepting a paper which\n",
    "should be rejected, such errors are easier to make because, again by\n",
    "definition, many more papers should be rejected. Type II errors\n",
    "(rejecting a paper that should be accepted) are less likely because (by\n",
    "setting the accept rate low) there are fewer papers that should be\n",
    "accepted in the first place. When there is a difference of opinion\n",
    "between reviewers, it does seem that many of the aruguments can be\n",
    "distilled down to (a subjective opinion) about whether controlling for\n",
    "type I or type II errors is more important. Further, normally when\n",
    "discussing type I and type II errors we believe that the underlying\n",
    "system of study is genuinely binary: e.g., diseased or not diseased.\n",
    "However, for conferences the accept/reject boundary is not a clear\n",
    "separation point, there is a continuum (or spectrum) of paper quality\n",
    "(as there also is for some diseases). And the decision boundary often\n",
    "falls in a region of very high density.\n",
    "\n",
    "I would prefer a world were a conference is no longer viewed as a proxy\n",
    "for research quality. The true test of quality is time. In the current\n",
    "world, papers from conferences such as NeurIPS are being used to judge\n",
    "whether a researcher is worthy of a position at a leading company, or\n",
    "whether a researcher gets tenure. This is problematic and damaging for\n",
    "the community. Reviewing is an inconsistent process, but that is not a\n",
    "bad thing. It is far worse to have a reviewing system that is\n",
    "consistently wrong than one which is inconsistently wrong.\n",
    "\n",
    "My own view of a NeurIPS paper is inspired by the Millenium Galleries in\n",
    "Sheffield. There, among the exhibitions they sometimes have work done by\n",
    "apprentices in their ‘qualification’. Sheffield is known for knives, and\n",
    "the work of the apprentices in making knives is sometimes very intricate\n",
    "indeed. But it does lead to some very impractical knives. NeurIPS seems\n",
    "to be good at judging technical skill, but not impact. And I suspect the\n",
    "same is true of many other meetings. So, a publication a NeurIPS does\n",
    "seem to indicate that the author has some of the skills required, but it\n",
    "does not necessarily imply that the paper will be impactful."
   ],
   "id": "09234e80-a8d3-4d46-b1de-46a3cce7212c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Solution\n",
    "\n",
    "I’ve worked on many community initiatives to improve the sharing of\n",
    "ideas, joining arXiv’s scientific advisory board is just the latest. In\n",
    "2005 I launched what has become PMLR, which has around 250 volumes of ML\n",
    "research from conferences varying from ICML to “I Can’t believe it’s not\n",
    "better”. It also supported “FAcT” at launch and other successful\n",
    "conferences.\n",
    "\n",
    "On seeing that some historic proceedings were disappearing from the web,\n",
    "we launched a reissue series. For expample, the [AISTATS 2001\n",
    "reissue](https://proceedings.mlr.press/r3/). Inspired by the spirit of\n",
    "these conferences next year I will organise the “Sorrento Single Author\n",
    "Paper Conference”, where the constraint is that each paper can only have\n",
    "one author, with the exception that PhD students can co-author with\n",
    "their supervisors.\n",
    "\n",
    "The event will be held in w/c 7th April, likely venue the [Grand Hotel\n",
    "Vesuvio](https://www.vesuviosorrento.com/) in Sorrento.\n",
    "\n",
    "Looking forward to seeing you there!"
   ],
   "id": "54b52031-974e-4da4-8dee-3d80bd6d28c8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Also"
   ],
   "id": "fa194f29-72e1-4e02-90f0-1eb1ea2ebbcb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Atomic Human\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-human.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-atomic-human.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://inverseprobability.com/talks/../slides/diagrams//books/the-atomic-human.jpg\" style=\"width:40%\">\n",
    "\n",
    "Figure: <i>[The Atomic Human](https://www.amazon.co.uk/dp/B0CGZHBSLL)\n",
    "(Lawrence, 2024) due for release in June 2024.</i>"
   ],
   "id": "9019f1eb-f4a1-4ce2-a64e-f0924a7ded4e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   book: [The Atomic\n",
    "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ],
   "id": "07e7ba7e-3df1-43fc-8e40-308111f93d1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ],
   "id": "e28bc725-54e4-4953-af6a-b5bb29e58e9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beygelzimer, A., Dauphin, Y.N., Liang, P., Vaughan, J.W., 2023. [Has the\n",
    "machine learning review process become more arbitrary as the field has\n",
    "grown? The NeurIPS 2021 consistency\n",
    "experiment](https://arxiv.org/abs/2306.03262), ArXiv e-prints.\n",
    "\n",
    "Lawrence, N.D., 2024. [The atomic human: Understanding ourselves in the\n",
    "age of\n",
    "AI](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248).\n",
    "Allen Lane."
   ],
   "id": "beae7d59-4dfb-4757-9d96-b05c21a43ce3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
