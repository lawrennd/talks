---
week: 5
session: 3
title: "Lab Session One"
abstract:  >
  In this notebook we review the use of the notebook, and refresh our understanding of probability.
layout: notebook
venue: Intel Lab, William Gates Building
author:
- family: Lawrence
  given: Neil D.
  gscholar: r3SJcvoAAAAJ
  institute: University of Cambridge
  twitter: lawrennd
  url: http://inverseprobability.com
time: "15:00"
date: 2021-11-09
transition: None
reveal: false
ipynb: true
---

\include{_datasets/includes/nigeria-nmis-data.md}
\include{_ml/includes/probability-intro.md}

\newslide{}

\figure{\includeyoutube{GX8VLYUYScM}{600}{450}}{MLAI Lecture 2 from 2012.}{mlai-lecture-2012}

\include{_psychology/includes/selective-attention-bias.md}
\include{_data-science/includes/data-inattention-bias.md}

\include{_data-science/includes/covid-vaccination-and-simpsons-paradox.md}

\newslide{Reading}

-   See probability review at end of slides for reminders.

\addreading{@Rogers:book11}{Section 2.2 (pg 41–53)}
\addreading{@Rogers:book11}{Section 2.4 (pg 55–58)}
\addreading{@Rogers:book11}{Section 2.5.1 (pg 58–60)}
\addreading{@Rogers:book11}{Section 2.5.3 (pg 61–62)}

- For other material in Bishop read:

\addreading{@Bishop:book06}{Probability densities: Section 1.2.1 (Pages 17–19)}
\addreading{@Bishop:book06}{Expectations and Covariances: Section 1.2.2 (Pages 19–20)}

\addreading{@Bishop:book06}{The Gaussian density: Section 1.2.4 (Pages 24–28) (don’t worry about material on bias)}
\addreading{@Bishop:book06}{For material on information theory and KL divergence try Section 1.6 & 1.6.1 (pg 48 onwards)}

- If you are unfamiliar with probabilities you should complete the
    following exercises:

\addexercise{@Bishop:book06}{Exercise 1.7}
\addexercise{@Bishop:book06}{Exercise 1.8}
\addexercise{@Bishop:book06}{Exercise 1.9}


\thanks

\reading

\exercises


\references
