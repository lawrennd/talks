{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling Things\n",
    "================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), Amazon Cambridge\n",
    "\n",
    "and University of Sheffield \\#\\#\\# 2020-08-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: Machine learning solutions, in particular those based on\n",
    "deep learning methods, form an underpinning of the current revolution in\n",
    "“artificial intelligence” that has dominated popular press headlines and\n",
    "is having a significant influence on the wider tech agenda. In this talk\n",
    "I will give an overview of where we are now with machine learning\n",
    "solutions, and what challenges we face both in the near and far future.\n",
    "These include practical application of existing algorithms in the face\n",
    "of the need to explain decision making, mechanisms for improving the\n",
    "quality and availability of data, dealing with large unstructured\n",
    "datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "%\\newcommand{\\tk}[1]{\\textbf{TK}: #1}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Machine Learning and Statistics Interface \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-and-statistics-interface.md\" target=\"_blank\" >edit</a>\\]\n",
    "=================================================================================================================================================================================\n",
    "\n",
    "Machine learning and Statistics are academic cousins, founded on the\n",
    "same mathematical princples, but often with different objectives in\n",
    "mind. But the differences can be as informative as the overlaps.\n",
    "\n",
    "In (Efron, 2020) Efron rightly alludes to the fundamental differences to\n",
    "the new wave of predictive models that have arisen in the last decades\n",
    "of machine learning. And these cultures were also beautifully described\n",
    "by Leo Breiman (Breiman, 2001).\n",
    "\n",
    "Although the prediction culture does not sit entirely in the machine\n",
    "learning domain, an excellent example of a prediction focussed approach\n",
    "would be Le Breiman’s Bagging (Breiman, 1996). Although it’s notable\n",
    "when he chose to publish the outlet was a machine learning journal.\n",
    "\n",
    "From my personal perspective, the strand of work that is most\n",
    "inspirational in prediction also comes from a statistician. Phil Dawid’s\n",
    "prequential ideas \\[Dawid:callibrated82,Dawid:prequential84\\], do\n",
    "provides some hope that a predictive approach can be reconciled with a\n",
    "scientific approach, in the sense that they allow us to falsify poorly\n",
    "calibrated models (Lawrence, 2010).\n",
    "\n",
    "The quote, apocraphally credited to Disraeli by Mark Twain “There are\n",
    "three kinds of lies: lies, damned lies and statistics” stems from the\n",
    "late 19th century. After Laplace, Gauss, Legendre and Galton and made\n",
    "their forays into regression, but well before Fisher, Pearson and others\n",
    "had begun to formulate statistics on a mathematical basis. Today, the\n",
    "academic discipline of statistics is so widely understood to be\n",
    "underpinned by mathematical rigour that we no longer bother to give the\n",
    "domain its full title of *mathematical statistics*, but the challenges\n",
    "that Efron outlines in (Efron, 2020) are part of a new phenomenon, that\n",
    "which was briefly called “big data”. In the modern world, we can see\n",
    "that there are still three types of lies: lies damned lies and big data.\n",
    "And the reason is the same as that which infected the late 19th century,\n",
    "the lack of a rigorous mathematical underpinning of this new domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happenstance Data \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/happenstance-data.md\" target=\"_blank\" >edit</a>\\]\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Following the revolution of mathematical statistics, data became a\n",
    "carefully curated commodity. It was actively connected in response to a\n",
    "scientific hypothesis. While different approaches to statistical\n",
    "hypothesis testing have been the subject of longstanding debates, there\n",
    "is no controversy around the notion that in order to remove confounders\n",
    "you must have a well designed experiment, and randomization has been a\n",
    "mainstay of statistical data collection for a century now. Randomized\n",
    "trials are used today more so than ever before, in particular due to\n",
    "their widespread use in interface design by large tech companies. Social\n",
    "experiments involving randomization across many millions of users are\n",
    "trivially implementable in real time. These A/B tests dictate our modern\n",
    "user experience.\n",
    "\n",
    "Such experiments are still carefully designed to remain valid, but the\n",
    "modern data environment is not only about larger experimental data, but\n",
    "perhaps more so about what I term “happenstance data”. Data that was not\n",
    "collected with a particular purpose in mind, but which is simply being\n",
    "recorded in the normal course of events due to increasing\n",
    "interconnection between portable digital devices and decreasing cost of\n",
    "storage.\n",
    "\n",
    "Happenstance data are the breadcrumbs of our trail through the forest of\n",
    "life. They may be being written for a particular purpose, but later we\n",
    "wish to consume them for a different purpose. For example, within the\n",
    "recent Covid-19 pandemic, the Royal Society DELVE initiative (The DELVE\n",
    "Initiative, 2020) was able to draw on transaction data to give near-real\n",
    "time assessments on the effect on GDP[1] of the the pandemic and\n",
    "governmental response (see also Carvalho et al. (2020)).\n",
    "\n",
    "Historically, data was expensive. It was carefully collected according\n",
    "to a design. Statistical surveys are still expensive, but today there is\n",
    "a strong temptation to do it on the cheap. To use happenstance data to\n",
    "achieve what had been done in the past only through rigorous\n",
    "data-fieldwork. A Professor Efron points out, early attempts to achieve\n",
    "this, such as the Google flu predictor have been somewhat naive (Jeremy\n",
    "Ginsberg, 2009, p. @Halevy:unreasonable09), but as these methodologies\n",
    "are gaining traction in the social sciences (Salganik, 2018) and the\n",
    "field of Computational Social Science (Alvarez, 2016) emerges we can\n",
    "expect more innovation and more ideas that may help us bridge the\n",
    "fundamentally different characters of qualitative and quantitative\n",
    "research. For the moment, one particularly promising approach is to use\n",
    "measures derived from happenstance data (such as searches for flu) as\n",
    "proxy indicators for statistics that are rigorously surveilled. With the\n",
    "Royal Society’s DELVE initiative, examples of this approach include work\n",
    "of Peter Diggle to visualize the progression of the Covid-19 disease.\n",
    "Across the UK the “Zoe App” has been used for self reporting of Covid\n",
    "symptons (Menni et al., 2020), and by interconnecting this data with\n",
    "Office for National Statistics surveys (Office for National Statistics,\n",
    "2020), Peter has been able to calibrate the Zoe map of Covid-19\n",
    "prevalence, allowing nowcasting of the diesease that was validated by\n",
    "the production of ONS surveys. These enriched surveys can already be\n",
    "done without innovation to our underlying mathematical\n",
    "\n",
    "So the statistical methodologies remain the gold-standard by which these\n",
    "new methodologies should be judged. The situation reminds me somewhat of\n",
    "the challenges Xerox faced with the advent of the computer revolution.\n",
    "With great prescience, Xerox realised that the advent of the computer\n",
    "meant that information was going to be shared more often on screens. As\n",
    "a company whose main revenue stream was coming from photocopying\n",
    "documents the notion of the paperless office represented something of a\n",
    "threat to Xerox. Xerox famously responded by funding their PARC research\n",
    "centre, where many of the innovations that underpin the modern computer\n",
    "were developed: the Xerox Alto (the first graphical user interface), the\n",
    "laser printer, ethernet. These inventions were commercial successes,\n",
    "although often for other companies, but as they propagated there was a\n",
    "greater need for paper. The computers produced more information, and\n",
    "much of it was still shared on paper. Per capita paper consumption\n",
    "continued to rise in the US until it peaked at around the turn of the\n",
    "millenium (Andrés et al., 2014). A similar story should now applies with\n",
    "the advent of predictive models and data science. The increasing use of\n",
    "predictive methodologies does not obviate the need for classical\n",
    "statistical approaches, it makes them more important than ever before.\n",
    "\n",
    "So, we may breathe easy that there is an ongoing role for the classical\n",
    "methodologies we have at our disposal, and historical precedent\n",
    "indicates the demand for those methodologies will likely increase before\n",
    "any fading. What about new mathematical theories? How can we come to a\n",
    "formalism for a new mathematical data science, just as early 20th\n",
    "century statisticians were able to reformulate statistics on a rigorous\n",
    "mathematical footing.\n",
    "\n",
    "Professor Efron’s paper does an excellent job a summarising the range of\n",
    "predictive models that now lie at our disposal, but of particular\n",
    "interest are deep neural networks. This is because they go beyond the\n",
    "traditional notions of what generalisation is or rather, what it has\n",
    "been, to practitioners on both the statistical and machine learning\n",
    "sides of the fence.\n",
    "\n",
    "[1] Although challenges with availability of payments data within the UK\n",
    "meant that the researchers were able to get good assessment of the\n",
    "Spanish and French economies, but struggled to assess their main target,\n",
    "the United Kingdom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Models and Generalization \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-models-and-generalization.md\" target=\"_blank\" >edit</a>\\]\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "When it comes to the mismatch between our expectations about\n",
    "generalization and the reality of deep models, perhaps the paper that\n",
    "most clearly demonstrated something was amis was (Zhang et al., 2017),\n",
    "who trained a large neural network via stochastic gradient descent to\n",
    "label an image data set. Within Professor Efron’s categorization of\n",
    "regression model, such a model is a very complex regression model with a\n",
    "particular link function and highly structured adaptive basis functions,\n",
    "which are by tradition called neurons. Despite the structuring of these\n",
    "basis functions (known as convolutional layers), their adaptive nature\n",
    "means that the model contains many millions of parameters. Traditional\n",
    "approaches to generalisation suggest that the model should over fit and\n",
    "(Zhang et al., 2017) proved that such models can do just that. The data\n",
    "they used to fit the model, the training set, was modified. They flipped\n",
    "labels randomly, removing any information in the data. After training,\n",
    "the resulting model was able to classify the training data with 100%\n",
    "accuracy. The experiment clearly demonstrates that all our gravest\n",
    "doubts about overparameterised models are true. If this model has the\n",
    "capacity to fit data which is obviously nonsense, then it is clearly not\n",
    "regularised. Our classical theories suggest that such models should not\n",
    "generalize well on previously unseen data, or test data, but yet the\n",
    "empirical experience is that they do generalize well. So, what’s going\n",
    "on?\n",
    "\n",
    "During a period of deploying machine learning models at Amazon, I was\n",
    "indoctrinated in a set of Leadership Principles, fourteen different\n",
    "ideas to help individual Amazonians structure their thinking. One of\n",
    "them was called “Dive Deep”, and a key trigger for a “Dive Deep” was\n",
    "when anecdote and data are in conflict. If there were to be a set of\n",
    "Academic leadership principles, then clearly “Dive Deep” should be\n",
    "triggered when empirical evidence and theory are in conflict. The\n",
    "purpose of the principle within Amazon was to ensure people don’t depend\n",
    "overly on anecdotes *or* data when making their decisions, but to\n",
    "develop deeper understandings of their business. In academia, we are\n",
    "equally guilty of relying too much on empirical studies or theory\n",
    "without ensuring they are reconciled. The theoreticians disbelief of\n",
    "what the experimenter tells them is encapsulated in Kahnemann’s idea of\n",
    "“theory induced blindness” (Kahneman, 2011). Fortunately, the evidence\n",
    "for good generalisation in these mammoth models is now large enough that\n",
    "the theory-blinders are falling away and a serious look is being taken\n",
    "and how and why these models can generalize well.\n",
    "\n",
    "An in depth technical understanding that applies to all these cases is\n",
    "not yet available. But some key ideas are. Firstly, if the neural\n",
    "network model is over-capacity, and can fit nonsense data in the manner\n",
    "demonstrated by (Zhang et al., 2017) then that immediately implies that\n",
    "the good generalization is arising from how the model is fitted to the\n",
    "data. When the number of parameters is so large, the parameters are very\n",
    "badly determined. In machine learning, the concept of version space\n",
    "(Mitchell, 1977) is the subset of all the hypotheses that are consistent\n",
    "with the training examples. For a neural network, the version space is\n",
    "where the neural network parameters (or weights) give predictions for\n",
    "the training data 100% accuracy. A traditional statistical perspective\n",
    "would eschew this regime, convinced that the implication is that\n",
    "overfitting must have occurred. But the empirical evidence from the deep\n",
    "learning community is that these regimes produce classification\n",
    "algorithms with excellent generalization properties. The resolution to\n",
    "this dilemma is *where* in the version space the algorithm comes to\n",
    "rest.\n",
    "\n",
    "An excellent characterisation of generalization is normally given by the\n",
    "bias-variance dilemma. The bias-variance decomposition for regression\n",
    "models separates the generalization error into two components (Geman,\n",
    "Bienenstock, and René Doursat, 1992)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Variance Decomposition \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-dilemma.md\" target=\"_blank\" >edit</a>\\]\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The bias-variance decomposition considers the expected test error for\n",
    "different variations of the *training data* sampled from,\n",
    "$\\Pr(\\dataVector, \\dataScalar)$ $$\n",
    "\\mathbb{E}\\left[ \\left(\\dataScalar - \\mappingFunction^*(\\dataVector)\\right)^2 \\right].\n",
    "$$ This can be decomposed into two parts, $$\n",
    "\\mathbb{E}\\left[ \\left(\\dataScalar - \\mappingFunction(\\dataVector)\\right)^2 \\right] = \\text{bias}\\left[\\mappingFunction^*(\\dataVector)\\right]^2 + \\text{variance}\\left[\\mappingFunction^*(\\dataVector)\\right] +\\sigma^2,\n",
    "$$ where the bias is given by $$\n",
    "  \\text{bias}\\left[\\mappingFunction^*(\\dataVector)\\right] =\n",
    "\\mathbb{E}\\left[\\mappingFunction^*(\\dataVector)\\right] * \\mappingFunction(\\dataVector)\n",
    "$$ and it represents error that arises from the models inability to\n",
    "represent the underlying complexity of the data. For example, if we were\n",
    "to model the marathon pace of the winning runner from the Olympics by\n",
    "computing the average pace across time, then that model would exhibit\n",
    "*bias* error because the reality of Olympic marathon pace is it is\n",
    "changing (typically getting faster).\n",
    "\n",
    "The variance term is given by $$\n",
    "  \\text{variance}\\left[\\mappingFunction^*(\\dataVector)\\right] = \\mathbb{E}\\left[\\left(\\mappingFunction^*(\\dataVector) - \\mathbb{E}\\left[\\mappingFunction^*(\\dataVector)\\right]\\right)^2\\right].\n",
    "  $$ The variance term is often described as arising from a model that\n",
    "is too complex, but we have to be careful with this idea. Too complex\n",
    "relative to the real world that generates the data? The real world is a\n",
    "complex place, and it is rare that we are constructing mathematical\n",
    "models that are more complex than the world around us. Rather, the ‘too\n",
    "complex’ refers to ability to estimate the parameters of the model given\n",
    "the data we have. Slight variations in the training set cause changes in\n",
    "prediction.\n",
    "\n",
    "Models that exhibit high variance are sometimes said to ‘overfit’ the\n",
    "data whereas models that exhibit high bias are sometimes described as\n",
    "‘underfitting’ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias vs Variance Error Plots \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-plots.md\" target=\"_blank\" >edit</a>\\]\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Helper function for sampling data from two different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(per_cluster=30):\n",
    "    \"\"\"Create a randomly sampled data set\n",
    "    \n",
    "    :param per_cluster: number of points in each cluster\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    scale = 3\n",
    "    prec = 1/(scale*scale)\n",
    "    pos_mean = [[-1, 0],[0,0.5],[1,0]]\n",
    "    pos_cov = [[prec, 0.], [0., prec]]\n",
    "    neg_mean = [[0, -0.5],[0,-0.5],[0,-0.5]]\n",
    "    neg_cov = [[prec, 0.], [0., prec]]\n",
    "    for mean in pos_mean:\n",
    "        X.append(np.random.multivariate_normal(mean=mean, cov=pos_cov, size=per_class))\n",
    "        y.append(np.ones((per_class, 1)))\n",
    "    for mean in neg_mean:\n",
    "        X.append(np.random.multivariate_normal(mean=mean, cov=neg_cov, size=per_class))\n",
    "        y.append(np.zeros((per_class, 1)))\n",
    "    return np.vstack(X), np.vstack(y).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for plotting the decision boundary of the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, cl, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    :param ax: matplotlib axes object\n",
    "    :param cl: a classifier\n",
    "    :param xx: meshgrid ndarray\n",
    "    :param yy: meshgrid ndarray\n",
    "    :param params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = cl.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot decision boundary and regions\n",
    "    out = ax.contour(xx, yy, Z, \n",
    "                     levels=[-1., 0., 1], \n",
    "                     colors='black', \n",
    "                     linestyles=['dashed', 'solid', 'dashed'])\n",
    "    out = ax.contourf(xx, yy, Z, \n",
    "                     levels=[Z.min(), 0, Z.max()], \n",
    "                     colors=[[0.5, 1.0, 0.5], [1.0, 0.5, 0.5]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary_plot(models, X, y, axs, filename, titles, xlim, ylim):\n",
    "    \"\"\"Plot a decision boundary on the given axes\n",
    "    \n",
    "    :param axs: the axes to plot on.\n",
    "    :param models: the SVM models to plot\n",
    "    :param titles: the titles for each axis\n",
    "    :param X: input training data\n",
    "    :param y: target training data\"\"\"\n",
    "    for ax in axs.flatten():\n",
    "        ax.clear()\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    if xlim is None:\n",
    "        xlim = [X0.min()-1, X0.max()+1]\n",
    "    if ylim is None:\n",
    "        ylim = [X1.min()-1, X1.max()+1]\n",
    "    xx, yy = np.meshgrid(np.arange(xlim[0], xlim[1], 0.02),\n",
    "                         np.arange(ylim[0], ylim[1], 0.02))\n",
    "    for cl, title, ax in zip(models, titles, axs.flatten()):\n",
    "        plot_contours(ax, cl, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        ax.plot(X0[y==1], X1[y==1], 'r.', markersize=10)\n",
    "        ax.plot(X0[y==0], X1[y==0], 'g.', markersize=10)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(title)\n",
    "        mlai.write_figure(os.path.join(filename),\n",
    "                          figure=fig,\n",
    "                          transparent=True)\n",
    "    return xlim, ylim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SVM and fit the data. \n",
    "C = 100.0  # SVM regularization parameter\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "\n",
    "per_class=30\n",
    "num_samps = 20\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10,3))\n",
    "xlim=None\n",
    "ylim=None\n",
    "for samp in range(num_samps):\n",
    "    X, y=create_data(per_class)\n",
    "    models = []\n",
    "    titles = []\n",
    "    for gamma in gammas:\n",
    "        models.append(svm.SVC(kernel='rbf', gamma=gamma, C=C))\n",
    "        titles.append('$\\gamma={}$'.format(gamma))\n",
    "    models = (cl.fit(X, y) for cl in models)\n",
    "    xlim, ylim = decision_boundary_plot(models, X, y, \n",
    "                           axs=ax, \n",
    "                           filename='../slides/diagrams/ml/bias-variance{samp:0>3}.svg'.format(samp=samp), \n",
    "                           titles=titles,\n",
    "                          xlim=xlim,\n",
    "                          ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('bias-variance{samp:0>3}.svg', \n",
    "                            directory='../slides/diagrams/ml', \n",
    "                            samp=IntSlider(0,0,10,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---->\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/bias-variance000.png\" style=\"width:80%\"><img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/bias-variance010.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>In each figure the more simple model is on the left, and the\n",
    "more complex model is on the right. Each fit is done to a different\n",
    "version of the data set. The simpler model is more consistent in its\n",
    "errors (bias error), whereas the more complex model is varying in its\n",
    "errors (variance error).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Descent \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/double-descent.md\" target=\"_blank\" >edit</a>\\]\n",
    "-------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "One of Breiman’s ideas for improving predictive performance is known as\n",
    "Bagging. The idea is to train a number of models on the data such that\n",
    "they overfit (high variance). Then average the predictions of these\n",
    "models. The models are trained on different bootstrap samples and their\n",
    "predictions are aggregated giving us the acronym, Bagging. By combining\n",
    "decision trees with Bagging we recover Random Forests\n",
    "\n",
    "Bias and variance can be estimated through the Bootstrap, and the\n",
    "traditional view has been that there’s a form of Goldilocks effect,\n",
    "where the best predictions are given by the model that is ‘just right’\n",
    "for the amount of data available. Not to simple, not too complex. The\n",
    "idea is that bias decreases with increasing model complexity and\n",
    "variance increases with increasing model complexity. Typically plots\n",
    "begin with the Mummy bear on the left (too much bias) end with the Daddy\n",
    "bear on the right (too much variance) and show a dip in the middle where\n",
    "the Baby bear (just) right finds themselves.\n",
    "\n",
    "The Daddy bear is typically positioned at the point where the model is\n",
    "able to exactly interpolate the data. For a generalized linear model\n",
    "(McCullagh and Nelder, 1989), this is the point at which the number of\n",
    "parameters is equal to the number of data[1]. But the modern empirical\n",
    "finding is that when we move beyond Daddy bear, into the dark forest of\n",
    "the massively overparameterized model we can achieve good\n",
    "generalization.\n",
    "\n",
    "As Zhang et al. (2017) starkly illustrated with their random labels\n",
    "experiment, within the dark forest there are some terrible places, big\n",
    "bad wolves of overfitting that will gobble up your model. But, as\n",
    "empirical evidence shows there is also a safe and hospitable Grandma’s\n",
    "house where these highly overparameterised models are safely consumed.\n",
    "Fundamentally, it must be about the route you take through the forest,\n",
    "and the precautions you take to ensure the wolf doesn’t see where you’re\n",
    "going and beat you to the door.\n",
    "\n",
    "There are two implications of this empirical result. Firstly, that there\n",
    "is a great deal of new theory that needs to be developed. Secondly, that\n",
    "theory is now obliged to conflate two aspects to modelling that we\n",
    "generally like to keep separate: the model and the algorithm.\n",
    "\n",
    "Classical statistical theory around predictive generalisation focusses\n",
    "specfically on the class of models that is being used for data fitting.\n",
    "Historically, whether that theory follows a Fisher-aligned estimation\n",
    "approach (see e.g. Vapnik (1998)) or model-based Bayesian approach (see\n",
    "e.g. Ghahramani (2015)), neither is fully equipped to deal with these\n",
    "new circumstances because, to continue our rather tortured analogy,\n",
    "these theories provide us with a characterisation of the *destination*\n",
    "of the algorithm, and seek to ensure that we reach that destination.\n",
    "Modern machine learning requires theories of the *journey* and what our\n",
    "route through the forest should be.\n",
    "\n",
    "Crucially, the destination is always associated with 100% accuracy on\n",
    "the training set. An objective that is always achievable for the\n",
    "overparameterized model.\n",
    "\n",
    "Intuitively, it seems that a highly over-parameterised model places\n",
    "Grandma’s house on the edge of the dark forest. Making it easily and\n",
    "quickly accessible to the algorithm. The larger the model, the more\n",
    "exposed Grandma’s house becomes. Perhaps this is due to some form of\n",
    "blessing of dimensionality brings Grandma’s house closer to the edge of\n",
    "the forest in a high dimensional stting. Really we should think of\n",
    "Grandma’s house as a low dimensional manifold of destinations that are\n",
    "safe. A path through the forest where the wolf of overfitting doesn’t\n",
    "venture. In the GLM case, we know already that when the number of\n",
    "parameters matches the number of data there is precisely one location in\n",
    "parameter space where accuracy on the training data is 100%. Our\n",
    "previous misunderstanding of generalization stemmed from the fact that\n",
    "(seemingly) it is highly unlikely that this single point is a good place\n",
    "to be from the perspective of generalization. Additionally, it is often\n",
    "difficult to find. Finding the precise polynomial coefficients in a\n",
    "least squares regression to exactly fit the basis to a small data set\n",
    "such as the Olympic marathon data requires careful consideration of the\n",
    "numerics and an orthogonalization of the design matrix (Lawson and\n",
    "Hanson, 1995).\n",
    "\n",
    "It seems that with a highly overparameterized model, these locations\n",
    "become easier to find and generalize well. In machine learning this is\n",
    "known as the “double descent phenomenon” (see e.g. Belkin et al.\n",
    "(2019)).\n",
    "\n",
    "As Professor Efron points out, modern machine learning models are often\n",
    "fitted using many millions of data points. The most extreme example of\n",
    "late is known as GPT-3. This neural network model, known as a\n",
    "Transformer, has in its largest form 175 billion parameters. The model\n",
    "was trained on a data set containing 499 billion tokens (about 2\n",
    "Terrabytes of text). Estimates suggest that the model costs around \\$4.5\n",
    "million dollars to train (see e.g. Li (2020)).\n",
    "\n",
    "[1] Assuming we are ignoring parameters in the link function and the\n",
    "distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Effectiveness of Deep Learning \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/empirical-effectiveness-of-deep-learning.md\" target=\"_blank\" >edit</a>\\]\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The OpenAI model represents just a recent example from a wave of *deep*\n",
    "neural network models has proved highly performant across a range of\n",
    "challenges that were previously seen as being beyond our statistical\n",
    "modelling capabilities.\n",
    "\n",
    "They stem from the courage of a group of researchers who saw that\n",
    "methods were improving with increasing data and chose to collect and\n",
    "label data sets of ever increasing size, in particular the ImageNet team\n",
    "led by Fei-Fei Li (Russakovsky et al., 2015) who collected a large data\n",
    "set of images for object detection (currently 14 million images)o. To\n",
    "make these neural network methods work on such large data sets new\n",
    "implementations were required. By deploying neural network training\n",
    "algorithms on graphics processing units (GPUs) breakthrough results were\n",
    "achieved on these large data sets (Krizhevsky et al., n.d.). Similar\n",
    "capabilities have then been shown in the domains of face identification\n",
    "(Taigman et al., 2014), and speech recognition (Hinton et al., 2012),\n",
    "translation (Sutskever et al., 2014) and language modelling (Devlin et\n",
    "al., 2019; Radford et al., 2019).\n",
    "\n",
    "Impressive though these performances are, they are reliant on massive\n",
    "data and enormous costs of training. Yet they can be seen through the\n",
    "lense of regression, as outlined by Professor Efron in his paper. They\n",
    "map from inputs to outputs. For language modelling, extensive use of\n",
    "auto-regression allows for sequences to be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Methods Required \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/new-methods-required.md\" target=\"_blank\" >edit</a>\\]\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The challenges of big data emerged due to companies being able to\n",
    "rapidly interconnect multiple sources of information. This leads to\n",
    "challenges in storage, distributed processing and modeling. Google’s\n",
    "search engine were at the forefront of this revolution. In particular\n",
    "they were able to demonstrate that some tasks can be easily resolved\n",
    "with fairly simple models and very large data sets (Halevy et al.,\n",
    "2009). What we are now learning is that many tasks can be solved with\n",
    "complex models and even bigger data sets.\n",
    "\n",
    "While GPT-3 does an impressive job on language generation, it can do so\n",
    "because of the vast quantities of language data we have made available\n",
    "to it. What happens if we take a more complex system, for which such\n",
    "vast data is not available. Or, at least not available in the\n",
    "homogeneous form that language data can be found. Let’s take human\n",
    "health.\n",
    "\n",
    "Consider we take a holistic view of health and the many ways in which we\n",
    "can become unhealthy, through genomic and environmental affects.\n",
    "Firstly, let’s remember that we don’t have a full understanding, even on\n",
    "all the operations in a single eukaryotic cell. Indeed we don’t even\n",
    "understand all the mechanisms by which transcription and translation\n",
    "occur in bacterial and archaeal cells. That is despite the wealth of\n",
    "gene expression and protein data about these cells. Even if we were to\n",
    "pull all this information together, would it be enough to develop that\n",
    "understanding? Health is an accumulation of\n",
    "\n",
    "There are large quantities of data, but the complexity of these systems\n",
    "iIn these domains we’d argue that even big data is small. The volume of\n",
    "the data is not large enough to determine the parameters of such complex\n",
    "models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massively Missing Data \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/massively-missing-data.md\" target=\"_blank\" >edit</a>\\]\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Our understanding of data still seems heavily influenced by the goals of\n",
    "early statistics. In particular, early mathematical statistics was\n",
    "heavily influenced by the need to overcome inductive biases in the\n",
    "human. To do this they encouraged statisticians to collect tables of\n",
    "data, with a focus on randomised control trials, to remove such\n",
    "inductive biases and ensure any conclusions drawn were valid. These\n",
    "developments were absolutely vital in ensuring rigorous evaluation of\n",
    "statistical claims. Whether it is concious or unconcious we see myriad\n",
    "examples from marketing and advertising which are designed to appeal to\n",
    "the, apparently, irrational aspects[1] of humans and trick them to doing\n",
    "something which is not to their individual benefit, but is profitable\n",
    "for the (normally corporate) entity that is doing the marketing. This\n",
    "may not matter a great deal when we are buying pet insurance or a new\n",
    "jacket, but is extremely dangerous when we are making claims for a\n",
    "particular drug to patients. Classical statistics acts as our final\n",
    "guardian against these claims, and even then it is subject to\n",
    "manipulation through failure to report on negative results.[2]\n",
    "\n",
    "> even the common man under the guidance of great minds will begin to\n",
    "> understand … \\[@\\] TODO check quote.}\n",
    "\n",
    "It is an inspiring quote, until you realise that the reality was more of\n",
    "a dystopia where large (normally commercial) organisations have become\n",
    "expert in exploiting those irrational aspects that Laplace began to\n",
    "identify and individual people have little to no understanding of the\n",
    "rational basis of uncertainty that Laplace was so convinced would become\n",
    "endemic.\n",
    "\n",
    "However, classical statistics does seem to give us a peculiar bias to\n",
    "tables of data: data where someone has carefully collected all the\n",
    "relevant features about the particular entites we are focussed on. That\n",
    "is a very different challenge to that of machine learning. In talks\n",
    "about this I like to tell the audience that my mum drives an Humvee. I\n",
    "then ask them what the audience thinks about that, what it makes them\n",
    "think about my mum. Certainly they probably think she’s unusual. Maybe\n",
    "it also affects what they think about me. Of course, she actually drives\n",
    "a VW Golf, which makes her much more of a normal mum. Importantly, the\n",
    "audience didn’t know I was going to say that before I started, but they\n",
    "were able to assimilate new information about the entity (my mum)\n",
    "through a feature they may have known existed, but they were unlikely to\n",
    "have predicted I was going to use before the talk. If we think about\n",
    "clinical data, the situation is even more extreme. If we are going to\n",
    "track someone’s health state throughout their life then we need to build\n",
    "models that might need to take into account clinical tests that don’t\n",
    "even exist yet. This is not an unusual situation, in fact it is the\n",
    "normal situation. The table of carefully collected statistical values is\n",
    "the unusual (and valuable) situation. That’s why so much attention is\n",
    "normally given to experimental design in statistics. But if we don’t\n",
    "have those controls what should we do? First we should recognize that\n",
    "missing data is the norm, not the exception: even when the table of data\n",
    "we collected is full there are probably many more things we *could have*\n",
    "collected but didn’t. Secondly we should recognise that the missing data\n",
    "normally dominates. It would be impossible to enumerate all the\n",
    "different types of data we would be missing for any complex system. The\n",
    "technique of imputation is suitable when missing data is only up to\n",
    "around half of our data set. The real world presents the challenge of\n",
    "massively missing data. Whenever you are doing analysis you are looking\n",
    "only a very tiny fraction of the things you could know. Interestingly,\n",
    "the visual and auditory systems present interesting counter examples to\n",
    "this analysis. Ignoring context (and concepts like sensor fusion) we can\n",
    "certainly think of the auditory and visual systems as presenting fixed\n",
    "dimensional signals about the entities they observe. This may explain\n",
    "why such success has been possible in these domains. But, from another\n",
    "perspective, both visual and auditory systems are *just* a very complex\n",
    "sensor. And in the type of intelligence we envisage we could have an\n",
    "arbitary number of sensors, and new sensors could be developed at all\n",
    "times. To understand the entire scene we must be able to incorporate\n",
    "such sensors as they are produced. Computational researchers who have\n",
    "worked in the biological sciences will know that over recent years\n",
    "sensorics has developed at such a rate that the most success can be\n",
    "garned by being the first to apply any method (typically PCA) to the\n",
    "sensoring domain, and that we all barely have time to catch our breath\n",
    "before the next generation of sensorics may render our work on the\n",
    "previous generation obsolete. What doesn’t change though, is the\n",
    "validity of the underlying modelling techniques that attempt to\n",
    "assimilate these data into a coherent whole.\n",
    "\n",
    "Consider the challenges of a highly multimodal domain like health data.\n",
    "Whilst we have ensured through clever engineering that speech and\n",
    "\n",
    "The wave of Developing the hypothesis that the main reason that these\n",
    "models became neglected was because there was not enough data to justify\n",
    "their implementation we advocate a return to tmotivate a return It is\n",
    "arguable that the main reason that\n",
    "\n",
    "Machine learning involves taking data and combining it with a model in\n",
    "order to make a prediction. The data consist of measurements recorded\n",
    "about the world around us. A model consists of our assumptions about how\n",
    "the data is likely to interrelate, typical assumptions include\n",
    "smoothness. Our assumptions reflect some undelying belief about the\n",
    "regularities of the universe that we expect to hold across a range of\n",
    "data sets. $$\n",
    "\\text{data} + \\text{model} \\rightarrow \\text{prediction}\n",
    "$$ From my perspective, the model is where all the innovation in machine\n",
    "learning goes. The etymology of the data indicates that it is given\n",
    "(although in some cases, such as active learning, we have a choice as to\n",
    "how it is gotten), our main control is over the model. This is the key\n",
    "to making good predictions. The model is a mathematical abstraction of\n",
    "the regularities of the universe that we believe underly the data as\n",
    "collected. If the model is chosen well we will be able to interpolate\n",
    "the data and precit likely values of future data points. If it is chosen\n",
    "badly our predictions will be overconfident and wrong.\n",
    "\n",
    "[1] Whether they are irrational or not depends on how we view them. They\n",
    "are the consequence of millions of years of evolution and it is only\n",
    "within the last 250 years that we understood the rational basis of\n",
    "probability and companies were able to exploit areas where people appear\n",
    "irrational to their own benefit. A particularly depressing read is a\n",
    "section of Laplace’s *Philosophical Essay on Probabilities* (Laplace,\n",
    "1814) where he advocates a new utopia based on rational thinking\n",
    "espousing that\n",
    "\n",
    "[2] I was once railing against the limitations of classical statistics\n",
    "to Darren Wilkinson and Joe Whittaker at a very pleasant meeting in the\n",
    "Lorenz institute, organized by Ernst Wit. It was Joe Whittaker that\n",
    "drove home this important point to me, although the connection to the\n",
    "Laplace quote is my own. That came from reading his Philosophical Essay\n",
    "on Probabilities, and for a moment I became carried away with him, when\n",
    "he glorified in the new world of rationality, until I was brought back\n",
    "to the present reality with an unpleasant jolt, in particular due to the\n",
    "stories about ‘Fixed Odds Betting Machines’ that were in the British\n",
    "media at the time. Laplace singles out games where the odds are stacked\n",
    "against the player a ‘particular evil’ TODO check. I’m not one for\n",
    "absolutes, but I think I’d agree with the idea that a larger entity,\n",
    "which has a deep understanding of rational behavior, exploiting a\n",
    "vulnerable smaller entity, who has little understanding of it, does come\n",
    "close to such evils.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models vs Algorithms \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/model-vs-algorithm.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "Much of the technical focus in machine learning is on algorithms. In\n",
    "this document I want to retain a strong separation between the *model*\n",
    "and the *algorithm*. The model is a mathematical abstraction of the\n",
    "world that encapsulates our assumptions about the data. Normally it will\n",
    "depend on one or more parameters which are adaptable. The algorithm\n",
    "provides a procedure for adapting the model to different contexts, often\n",
    "through the provision of a set of data that is used for training the\n",
    "model.}\n",
    "\n",
    "Despite the different role of model and algorithm, the two concepts are\n",
    "often conflated. This sometimes leads to a confused discussion. I was\n",
    "recently asked “Is it correct to remove the mean from the data before\n",
    "you do principal component analysis.” This question is about an\n",
    "algorithmic procedure, but the correct answer depends on what modelling\n",
    "assumption you are seeking to make when you are constructing your\n",
    "principal component analysis. Principal component analysis was\n",
    "originally proposed by a *model* for data by (Hotelling, 1933). It is a\n",
    "latent variable model that was directly inspired by work in the social\n",
    "sciences on factor analysis. However, much of our discussion of PCA\n",
    "today focusses on PCA as an algorithm. The algorithm for fitting the PCA\n",
    "model is to seek the eigenvectors of the covariance matrix, and people\n",
    "often refer to this algorithm as principal component analysis. However,\n",
    "that algorithm also finds the linear directions of maximum variance in\n",
    "the data. Seeking directions of maximum variance in the data was not the\n",
    "objective of Hotelling, but it is related to a challenge posed by\n",
    "Pearson (1901) who sought a variant of regression that predicted\n",
    "symmetrically regardless of which variable was considered to be the\n",
    "covariate and which variable the response. Coincidentally the algorithm\n",
    "for this model is also the eigenvector decomposition of the covariance\n",
    "matrix. However, the underlying model is different. The difference\n",
    "becomes clear when you begin to seek non-linear variants of principal\n",
    "component analysis. Depending on your interpretation (finding directions\n",
    "of maximum variance in the data or a latent variable model) the\n",
    "corresponding algorithm differs. For the Pearson model a valid\n",
    "non-linearization is kernel PCA (Schölkopf et al., 1998), but for the\n",
    "Hotelling model this generalization doesn’t make sense. A valid\n",
    "generalization of the Hotelling model is provided by the Gaussian\n",
    "process latent variable model (Lawrence, 2005). This confusion is often\n",
    "unhelpful, so for the moment we will leave algorithmic considerations to\n",
    "one side and focus *only* on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is my Model Useful? \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/is-my-model-useful.md\" target=\"_blank\" >edit</a>\\]\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "> All models are wrong, but some are useful\n",
    ">\n",
    "> Box (1976)\n",
    "\n",
    "This important quote has become worn by overuse (like a favourite\n",
    "sweater). Worse still it is almost being bandied around to mean that\n",
    "*because* my model is wrong it *might* be useful. It seems that people\n",
    "almost equate the statement to meaning probobability of my model being\n",
    "wrong given that its useful is = 1. Which would be an incorrect model,\n",
    "but seems to be useful in practice when trying to justify poor\n",
    "assumptions.\n",
    "\n",
    "Perhaps we should be more focussing on the quote \"... the scientist must\n",
    "be alert to what is importantly wrong. It is inappropriate to be\n",
    "concerned about mice when there are tigers abroad.\" from the same paper.\n",
    "Let’s have a think about where the tigers might be in the domain of big\n",
    "data. To consider this, let’s first see what we can write down about our\n",
    "data that isn’t implicitly wrong. If we are interested in multivariate\n",
    "data we could first write down our data in the following form. $$\n",
    "\\text{data} = \\mathbf{\\dataMatrix} \\in \\Re^{\\numData\\times \\dataDim},\n",
    "$$ where here we are assuming we have $\\numData$ data points and\n",
    "$\\dataDim$ features. However, as soon as we write down our data in this\n",
    "form it invites particular assumptions about your data that were valid,\n",
    "perhaps in the 1930s, when people were worried about tables of data.\n",
    "They collected tables of data with a specific purpose in mind and the\n",
    "data naturally sat in a matrix. Immediately we write down our data in a\n",
    "matrix form, $\\dataMatrix\\in \\Re^{\\numData\\times \\dataDim}$ it is\n",
    "somehow implicit that we are suggesting factorization assumptions across\n",
    "the $\\numData$ data points.} This assumption allows us to easily make\n",
    "predictions about new data points given a parameter vector that is\n",
    "derived from the training data. This assumptions will generally be\n",
    "wrong, and also leads to concerns about the parameters when\n",
    "$\\numData<<\\dataDim$, the so called \\`large $\\dataDim$, small\n",
    "$\\numData$’ domain. They also lead to concerns such as large $\\dataDim$,\n",
    "small $\\numData$ concerns.\n",
    "\n",
    "I think that this is a wrongheaded way of thinking about modern data,\n",
    "because in practice, $\\dataDim$, doesn’t really exist, at least not in\n",
    "the sense that the above model implies we should treat it. It doesn’t\n",
    "exist as a static view of the data: $\\dataDim$ is much more fluid than\n",
    "the model above implies. Indeed, I’ll argue below that rather than\n",
    "increasing $\\dataDim$ when we obtain a new feature about a data point,\n",
    "we should be increasing $\\numData$. That adding writing down our data in\n",
    "matrix form, $\\dataMatrix$, may even be constraining our thinking to\n",
    "these factorized models. And the fact that the factorization is strong:\n",
    "i.e. it assumes that all becomes independent given the parameters, it is\n",
    "very often wrong. That is not to say that these factorization\n",
    "assumptions are not useful, indeed we will make use of them below, but\n",
    "they should *not* be the first thing we write down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Motivating Big Data Example \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/big-data-health-motivation.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "Mathematical statisticians like Pearson, Fisher and Gosset worked with\n",
    "tables of data that they’d carefully collected, often with the specific\n",
    "purpose of answering a particular question. The decided at experiment\n",
    "*design* time what was to be measured $\\numData$. The number of samples\n",
    "was determined by statistical power calculations *CHECK THIS*, this was\n",
    "something that could be varied.\n",
    "\n",
    "One of my own interests is personalized health: what we can learn about\n",
    "patients’ state of health and when we should make an interviention. In\n",
    "the big data era, we aren’t only interested in what data we might\n",
    "collect for answering a specific question (although data of this type\n",
    "remains very important) but we are also interested in existing data that\n",
    "might be assimilated to improve our understanding of an individual’s\n",
    "health. When imagining future systems that monitor our health status, we\n",
    "should not be restricted to the type of data that might be stored in a\n",
    "doctor’s office or a hospital data base. Indeed, it might be argued that\n",
    "such data focusses on sickness rather than health, giving us an\n",
    "incomplete picture.\n",
    "\n",
    "Modern data availabilities means that we could build models that\n",
    "incorporate an individual’s exercise regime (for example through\n",
    "websites such as Strava). We could include information about an\n",
    "individual’s dietary habits (e.g. through loyalty card information like\n",
    "the Nectar card). If we were monitoring potential degradation in health\n",
    "then we may also be interested in an individual’s social network\n",
    "activity (Twitter, Facebook, Google+). Even musical tastes may feed in\n",
    "to our overall picture of the patient’s well being through music\n",
    "services like spotify. For a full perspective on a patient’s health,\n",
    "this data would need to be combined with more traditional sources\n",
    "phenotype and genotyp infomration. For example, high resolution scans of\n",
    "the genome providing a detailed characterization of genotype. Large\n",
    "scale gene expression measurements, giving detailed insights into\n",
    "phenotype at the cellular level. Images containing x-rays or biopsies.\n",
    "Doctor’s notes, but handwritten and those that encode a diangosis.\n",
    "Clinical tests, for example in cardiovascular disease cholestorol level.\n",
    "To provide a full picture of health status all this information needs to\n",
    "be assimilated. In a traditional model, we might encode each piece of\n",
    "information as another element on a feature vector: in other words, all\n",
    "the above contributes to increasing $\\dataDim$. However, for most\n",
    "patients, most of the information above is likely to be missing. The\n",
    "paradigm of missing data is often discussed, but in this domain we have\n",
    "a situation we might refer to as *massivelv missing data*. A situation\n",
    "where a missing value becomes the norm rather than an exception.\n",
    "\n",
    "Another facet of the personalized health problem will be the streaming\n",
    "nature of data. When acquiring data passively data doesn’t arrive in\n",
    "blocks, it arrives in a haphazard fashion. Our model may need to update\n",
    "because patient 2,342 has just had the results of a blood test logged,\n",
    "or because patient 28,344,219 has just been for a run or because patient\n",
    "12,012,345 just listened to a Leonard Cohen track or because patient\n",
    "12,182 just gave birth.\n",
    "\n",
    "One possible motivation for making independence assumptions across data\n",
    "points is the ease with which predictions can be made for a previously\n",
    "unseen vector $\\dataVector^*$. Given an estimate of a vector of\n",
    "parameters, $\\hat{\\paramVector}$, perhaps obtained by optimizing the\n",
    "likelihood on the training data, then due to our assumption of\n",
    "independence across data then we can easily predict for the new point\n",
    "using the conditional distribution: $$\n",
    "p(\\dataVector_*|\\hat{\\paramVector}).  $$ Perhaps, though, we should find\n",
    "this ease of prediction suspicious. Let’s momentarily examine what we\n",
    "are really saying here. We are assuming that all the information we wish\n",
    "to store about the world, and communicate to a test data set is storable\n",
    "in a parameter vector, $\\paramVector$, the nature of which (for example\n",
    "its length) is set at design time, before we’ve seen the data. That is\n",
    "precisely the meaning of statistical *independence given the\n",
    "parameters*.\n",
    "\n",
    "For applications like the personalized health monitoring system\n",
    "described above, we need a model that will give well calibrated\n",
    "predictions from the first day of it being brought on line, and\n",
    "throughout its operational life. If the model is complex enough to\n",
    "represent the full spectrum of possible human ailments, then when the\n",
    "model is first brought on stream, it is unlikely to have sufficient data\n",
    "to determine the parameters. In the standard modeling framework we are\n",
    "faced with the bias variance dilema (Geman, Bienenstock, and Rene\n",
    "Doursat, 1992), 1992)</cite>. If the model is complex enough to\n",
    "represent the underlying data structure, the parameters will be badly\n",
    "determined for small, or badly designed data sets, and the model will\n",
    "exhibit a large error due to variance. A traditional solution is to err\n",
    "towards bias, by constructing a simpler model, but one where the\n",
    "parameters can be well determined by the data, we reduce variance at the\n",
    "expense of some bias. In the context of our medical application, there\n",
    "are three major problems with this approach. Firstly, the size and scope\n",
    "of the data is continually evolving: we do not have a fixed design. This\n",
    "means that even if we were to find a good initial compromise between\n",
    "bias and variance, this compromise may be rapidly invalidated. Secondly,\n",
    "the compromise we find would have to apply equally to all patients\n",
    "despite the diversity of data we have associated with those patients.\n",
    "Finally, we should fear the confidence of predictions from a model with\n",
    "well determined parameters unless we truly believe we have sufficient\n",
    "data to capture some underlying deterministic truth. Medical outcome is\n",
    "laced with uncertainty, and this uncertainty needs to be modeled\n",
    "correctly because its structure has a significant effect on treatment.\n",
    "\n",
    "A major challenge in the domain we’ve described is to build a model that\n",
    "is complex enough to represent the diversity of human health outcomes.\n",
    "For streaming data this necessarily means that some of those parameters\n",
    "will be badly determined. I’d also argue further that if the parameters\n",
    "are well determined this is actually a warning. If all parameters are\n",
    "well determined, then our assumption of statistical independence becomes\n",
    "a strong one: the residual uncertainty is only in the noise, which by\n",
    "its independent nature, is impossible to model. However, any uncertainty\n",
    "in the parameters gives a much more structured uncertainty distribution\n",
    "for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty in Parameters \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/not-useful-model.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "If the parameters are badly determined, then small fluctuations in the\n",
    "data set lead to larger fluctuations in prediction. One approach to this\n",
    "problem is to build models in which the parameters are well determined.\n",
    "For teh independence across data points case, this involves having many\n",
    "observations (large $\\numData$) relative to the number of parameters\n",
    "(which often scales with $\\dataDim$). This motivates the issues of the\n",
    "large $\\dataDim$ small $\\numData$ domain, where the conditions are\n",
    "reversed. Of course, from a modelling perspective this issue is\n",
    "trivially solved by assuming independence across the $\\dataDim$ data\n",
    "dimensions and allowing the parameters to scale with the number of data\n",
    "$\\numData$. This is a characteristic exhibited, for example by the\n",
    "Gaussian process latent variable model (Lawrence, 2005) which in\n",
    "standard form assumes independence arcross $\\dataDim$ for high\n",
    "dimensional data and associates each data point with a latent variable\n",
    "that is treated as a parameter. In (Lawrence, 2012) I argued that the\n",
    "succesful class of *spectral* approaches to dimensionality reduction\n",
    "(e.g. LLE Roweis and Saul (2000) and maximum variance unfolding\n",
    "Weinberger et al. (n.d.), which are widely applied in the large\n",
    "$\\dataDim$ small $\\numData$ domain, also have a probabilistic\n",
    "intepretation where the underlying likelihood factorizes across data\n",
    "dimensions. Regardless of our choice of factorization though, we are\n",
    "still making the same claim: a particular vector, or matrix, of\n",
    "parameters is suffcient for us to consider that the data independent,\n",
    "either across features or data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massively Missing Data\n",
    "\n",
    "I’d like to argue that the separation of the data into features and data\n",
    "points is rather arbitrary. I believe it stems from the origin of the\n",
    "field of statistics, where the intention was to make a strong scientific\n",
    "claim based on numbers take from a *table* of data. A table naturally\n",
    "lends itself towards a matrix form. In these data a statistical design\n",
    "normally involved measuring a fixed number of *features* for a perhaps\n",
    "variable number of *items*. The objective is to find sufficient number\n",
    "of items so that you can make strong claims about which features are\n",
    "important. For example, does smoking correlate with lung cancer? This\n",
    "explains the desire to write down the data as a matrix $\\dataMatrix$. I\n",
    "think this view of data, whilst important at the time, is outdated when\n",
    "considering modern big data problems.\n",
    "\n",
    "The modern data analysis challenge is very different. We receive\n",
    "streaming data of varying provenance. If each number we receive is given\n",
    "by an observation $\\dataScalar_i$, where $\\dataScalar_i$ could be in the\n",
    "natural numbers, the real numbers or binary or in any processable form,\n",
    "then $\\dataScalar_{17}$ might be the price of a return rail fair from\n",
    "Sheffield to Oxford on 6th February 2014, whilst $\\dataScalar_{29}$\n",
    "might be the number of people on the 8:20 train that day, but\n",
    "$\\dataScalar_{72,394}$ could be the temperature of the Atlantic ocean on\n",
    "23rd August 2056 at a point on the artic circle midway between Greenland\n",
    "and Norway. When we see data in this form, we realize that most of the\n",
    "time we are missing most of the data. This leads to the idea of *massive\n",
    "missing data*. Contrast this situation with that traditionally faced in\n",
    "missing data where a table of values, $\\dataMatrix$, might have 10%-50%\n",
    "of the measurements missing, perhaps due to problems in data collection.\n",
    "I’d argue that if we are to model complex processes (such as the brain,\n",
    "or the cell, or human health) then almost all the data is missing.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/y-only-graph.svg\" class=\"\" align=\"30%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The most general graphical model. It makes no assumptions\n",
    "about conditional probability relationships between variables in the\n",
    "vector $\\dataVector$.</i>\n",
    "\n",
    "A model that’s not wrong, just not useful. I like graphical\n",
    "representations of probabilistic models and this is my favourite graph.\n",
    "It is the most simple but also the most general. It says that all the\n",
    "data in our vector $\\dataVector$ is governed by an unspecified\n",
    "probability disribution $p(\\dataVector)$. Graphical models normally\n",
    "express the conditional independence relationships in the data, with\n",
    "this graph we are not a priori considering any such relationships. This\n",
    "is the most general model (it includes all factorized models as special\n",
    "cases). It is not wrong, but since it doesn’t suggest what the next\n",
    "steps are or give us any handles on the problem it is also not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Data Consistency \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/big-data-consistency.md\" target=\"_blank\" >edit</a>\\]\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "This is the nature of modern streaming data, what has been called big\n",
    "data, although in the UK it looks like that term will gain a more\n",
    "diffuse meaning now that the government has associated a putative 189\n",
    "billion pounds of funding to it. But the characteristic of massive\n",
    "missing data is particularly obvious when we look at clinical domains.\n",
    "EMIS, a Yorkshire based provider of software to General Practitioners,\n",
    "has 39 million patient records. When we consider clinical measurements,\n",
    "we need to build models that not only take into account all current\n",
    "clinical tests, but all tests that will be invented in the future. This\n",
    "leads to the idea of massive missing data. The classical statistical\n",
    "table of data is merely the special case where someone has filled in a\n",
    "block of information.\n",
    "\n",
    "To deal with massively missing data we need to think about the\n",
    "*Kolmogorov consistency* of a process. Let me introduce Kolmogorov\n",
    "consistency by way of an example heard from Tony O’Hagan, but one that\n",
    "he credits originally to Michael Goldstein. Imagine you are on jury\n",
    "duty. You are asked to adjudicate on the guilt or innocence of Lord\n",
    "Safebury, and you are going to base your judgement on a model that is\n",
    "weighing all the evidence. You are just about to pronounce your decision\n",
    "when a maid comes running in and shouts \"He didn’t do it! He didn’t do\n",
    "it!\". The maid wasn’t on the witness list and isn’t accounted for in\n",
    "your model. How does this effect your inference? The pragmatists answer\n",
    "might be: not at all, because the maid wasn’t in the model. But in the\n",
    "interests of justice we might want to include this information in our\n",
    "inference process. If, as a result of the maid’s entry, we now think it\n",
    "is less likely that Lord Safebury committed the crime, then necessarily\n",
    "every time that the (unannounced) maid doesn’t enter the room we have to\n",
    "assume that it is more likely that Safebury commited the crime (to\n",
    "ensure that the conditional probability of guilt given the maid’s\n",
    "evidence normalizes. But we didn’t know about the maid, so how can we\n",
    "account for this? Further, how can we account for all possible other\n",
    "surprise evidence, from the announced butlers, gardners, chauffeurs and\n",
    "footmen? Kolmogorov consistency says that the net effect of\n",
    "marginalizing for all these potential bits of new information is null.\n",
    "It is a particular property of the model. Making it (only slightly) more\n",
    "formal, we can consider Kolmogorov consistency as a marginalization\n",
    "property of the model. We take the $\\numData$ dimensional vector,\n",
    "$\\dataVector$, to be an (indexed) vector of all our instantiated\n",
    "observations of the world that we have *at the current time*. Then we\n",
    "take the $\\numData^*$ dimensional vector, $\\dataVector^*$ to be the\n",
    "observations of the world that we are *yet to see*. From the sum rule of\n",
    "probability we have where the dependence of the marginal distribution\n",
    "for $\\dataVector$ aries from the fact that we are forming an\n",
    "$\\numData^*$ dimensional integral over $\\dataVector^*$. If our\n",
    "distribution is Kolmogorov consistent, then we know that the\n",
    "distribution over $\\dataVector$ is *independent* of the value of\n",
    "$\\numData^*$. So in other words\n",
    "$p(\\dataVector|\\numData*)=p(\\dataVector)$. So Kolmogorov consistency\n",
    "says that the form of $p(\\dataVector)$ remains the same *regardless* of\n",
    "the number of observations of the world that are yet to come."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric Models\n",
    "-----------------\n",
    "\n",
    "We can achieve Kolomogrov consistency almost trivially in a parametric\n",
    "model if we assume that the probability distribution is independent\n",
    "given the parameters. Then the property of being closed under\n",
    "marginalization is trivially satisfied through the independence, which\n",
    "allows us to marginalize for all future data leaving a joint\n",
    "distribution which isn’t dependent on $\\numData^*$ because each future\n",
    "data point can be marginalized independently. But, as we’ve already\n",
    "argued, this involves an assumption that is often flawed in practice. It\n",
    "is unlikely that, in a complex model, we will be able to determine the\n",
    "parameter vector well enough, given limited data, for us to truly\n",
    "believe that all the information about the training data that is\n",
    "required for predicting the test data could be passed through a fixed\n",
    "length parameter vector. This is what this independence assumption\n",
    "implies. If we consider that the model will also be acquiring new data\n",
    "at run time, then there is the question of hot to update the parameter\n",
    "vector in a consistent manner, accounting for new information, e.g. new\n",
    "clinical results in the case of personalized health.\n",
    "\n",
    "Conversely, a general assumption about independence across *features*\n",
    "would lead to models which *don’t* exhibit *Komlogorov consistency*. In\n",
    "these models the dimensionality of the test data, $\\dataVector^*$,\n",
    "denoted by $\\numData^*$ would have to be fixed and each\n",
    "$\\dataScalar^*_i$ would require marginalization. So the nature of the\n",
    "test data would need to be known at model *design* time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric Bottleneck \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/parameter-bottleneck.md\" target=\"_blank\" >edit</a>\\]\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In practice Bayesian methods suggest placing a prior over\n",
    "$\\boldsymbol{\\theta}$ and using the posterior,\n",
    "$p(\\boldsymbol{\\theta}|\\dataVector)$ for making predictions. We have a\n",
    "model that obeys Kolmogorov consistency, and is sophisticated enough to\n",
    "represent the behaviour of a very: it may well require a large number of\n",
    "parameters. One way of seeing the requirement for a large number of\n",
    "parameters is to look at how we are storing information from the\n",
    "training data to pass to the test data. The sum of all our knowledge\n",
    "about the training data is stored in the conditional distribution of the\n",
    "parameters given the training data, Uncertainty complex systA key design\n",
    "time problem is the *parametric bottleneck*. If we choose the number of\n",
    "parameters at design time, but the system turns out to be more\n",
    "complicated that we expected, we need to design a new model to deal with\n",
    "this complexity. The communication between the training data and the\n",
    "test data is like an information channel. This TT channel has a\n",
    "bandwidth that is restricted by our choice of the dimensionality of\n",
    "$\\boldsymbol{\\theta}$ at *design* time. This seems foolish. Better to\n",
    "ensure we choose a model that allows for that channel to be potentially\n",
    "infinite. This implies a non-parametric approach. Our prior over\n",
    "$\\boldsymbol{\\theta}$ should be *non parametric*. $$\n",
    "p(\\paramVector | \\dataVector),\n",
    "$$ which, as we argued above, allows us to retain the necessary sense of\n",
    "uncertainty about the parameters that is required in a very complex\n",
    "system when we have seen relatively little data. How much information\n",
    "can we store, then, about the training data? The information gain from\n",
    "the training data is given by the Kullback Leibler divergence between\n",
    "our prior distribution and our posterior distribution. $$\n",
    "\\KL{p(\\paramVector|\\dataVector)}{p(\\paramVector)}\n",
    "$$ This is the information gained, measured in ‘nats’ if we use natural\n",
    "logarithms, but it could equally be measured in bits, about our\n",
    "parameters having observed the training data. In the case that our\n",
    "likelihood is log concave[1] then this information gain provably will\n",
    "increase, with every observed data point. How much information we gain\n",
    "will depend on the likelihood associated with each data $\\dataScalar_i$.\n",
    "This Kullback Leibler divernece has an infomration theoretic\n",
    "interpretation as a communication channel passing information from the\n",
    "training data to the test data. From an information theoretic\n",
    "perspective, the channel bandwidth is controlled by the dimensionality\n",
    "of the parameter vector $\\dataVector$ and the form of the prior\n",
    "$p(\\paramVector)$.\n",
    "\n",
    "[1] This is a definite constraint on the model, there are many very\n",
    "reasonable likelihoods that are not log concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Non-parametric Challenge \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/non-parametric-challenge.md\" target=\"_blank\" >edit</a>\\]\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "We have argued that we want models that are unconstrained, at design\n",
    "time, by a fixed bandwidth for the communication between the training\n",
    "data, $\\dataVector$, and the test data, $\\dataVector^*$ and that the\n",
    "answer is to be non parameteric. By non-parametric we are proposing\n",
    "using classes of models for which the conditional distribution,\n",
    "$p(\\dataVector^*|\\dataVector)$ is not decomposable into the expectation\n",
    "of $p(\\dataVector^*|\\paramVector)$ under the posterior distribution of\n",
    "the parameters, $p(\\paramVector|\\dataVector)$ for any fixed length\n",
    "parameter vector $\\paramVector$. We don’t want to impose such a strong\n",
    "constraint on our model at *design time*. Our model may be required to\n",
    "be operational for many years and the true complexity of the system\n",
    "being modeled may not even be well understood at *design time*. We must\n",
    "turn to paradigms that allow us to be adaptable at *run time*. Non\n",
    "parametrics provides just such a paradigm, because the effect parameter\n",
    "vector increases in size as we observe more data. This seems ideal, but\n",
    "it also presents a problem.\n",
    "\n",
    "Human beings, despite are large, interconnected brains, only have finite\n",
    "storage. It is estimated that we have between 100 and 1000 trillion\n",
    "synapses in our brains. Similar for digital computers, even the GPT-3\n",
    "model is restricted to 175 billion parameters. So we need to assume that\n",
    "we can only store a finite number of things about the data\n",
    "$\\dataVector$. This seems to push us back towards non-parametric models.\n",
    "Here, though, we choose to go a different way. We choose to introduce a\n",
    "set of auxiliary variables, $\\inducingVector$, which are $\\numInducing$\n",
    "in length. Rather than representing the non parametric density directly,\n",
    "we choose to focus on storing information about $\\inducingVector$. By\n",
    "storing information about these variables, rather than storing all the\n",
    "data $\\dataVector$ we hope to get around this problem. In order for us\n",
    "to be non parametric about our predictions for $\\dataVector*$ we must\n",
    "condition on all the data, $\\dataVector$. We can’t any longer store an\n",
    "intermediate distribution to represent our sum knowlege,\n",
    "$p(\\paramVector|\\dataVector)$. Such an intermediate distribution is a\n",
    "finite dimensional object, and non-parametrics implies that we cannot\n",
    "store all the information in a finite dimensional distribution. This\n",
    "presents a problem for real systems in practice. We are now faced with a\n",
    "compromise, how can we have a distribution which is flexible enough to\n",
    "respond at *run time* to unforeseen complexity in the training data?\n",
    "Yet, simultaneously doesn’t require unbounded storage to retain all the\n",
    "information in the training data? We will now introduce a perspective on\n",
    "variational inference that will allow us to retain the advantages of\n",
    "both worlds. We will construct a parametric approximation to the true\n",
    "non-parametric conditional distribution. But, importantly, whilst this\n",
    "parametric approximation will suffer from the constraints on the\n",
    "bandwidth of the TT channel that drove us to non-parametric models in\n",
    "the first place, we will be able to change the number of parameters at\n",
    "*run time* not simply at design time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multivariate Gaussian: Closure Under Marginalization \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/multivariate-gaussian-closure.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "Being closed under marginalization is a remarkable property of our old\n",
    "friend the multivariate Gaussian distribution (old friends often have\n",
    "remarkable properties that we often take for granted, I think this is\n",
    "particularly true for the multivariate Gaussian). In particular, if we\n",
    "consider a joint distribution across $p(\\dataVector, \\dataVector^*)$,\n",
    "then the covariance matrix of the marginal distribution for the subset\n",
    "of variables, $\\dataVector$, is unaffected by the length of\n",
    "$\\dataVector^*$. Taking this to its logical conclusion, if the length of\n",
    "the data, $\\dataVector$, is $\\numData=2$. Then that implies that the\n",
    "covariance between $\\dataVector$, as defined by $\\kernelMatrix$, is only\n",
    "a $2\\times 2$ matrix, and it can only depend on the indices of the two\n",
    "data points in $\\dataVector$. Since this covariance matrix must remain\n",
    "the same for any two values *regardless* of the length of $\\dataVector$\n",
    "and $\\dataVector^*$ then the value of the elements of this covariance\n",
    "must depend only on the two indices associated with $\\dataVector$.\n",
    "\n",
    "Since the covariance matrix is specified pairwise, this implies that the\n",
    "covariance matrix must be dependent only on the index of the two\n",
    "observations $\\dataScalar_i$ and $\\dataScalar_j$ for which the\n",
    "covariance is being computed. In general we can also think of this index\n",
    "as being infinite: it could be a spatial or temporal location. where\n",
    "each $\\dataScalar_i$ is now defined across the real line, and the\n",
    "dimensionality of $\\dataVector*$ is irrelevant. Prediction consists of\n",
    "conditioning the joint density on $\\dataVector^*$. So for any new value\n",
    "of $\\dataVector^*$, given its index we compute\n",
    "$p(\\dataVector^* | \\dataVector)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Parameters non-Parametric \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/making-parameters-non-parametric.md\" target=\"_blank\" >edit</a>\\]\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "We will start by introducing a set of variables, $\\inducingVector$, that\n",
    "are finite dimensional. These variables will eventually be used to\n",
    "communicate information between the training and test data, i.e. across\n",
    "the TT channel. $$\n",
    "p(\\dataVector^*|\\dataVector) = \\int p(\\dataVector^*|\\inducingVector) q(\\inducingVector|\\dataVector) \\text{d}\\inducingVector\n",
    "$$ where we have introduced a distribution over $\\inducingVector$,\n",
    "$q(\\inducingVector|\\dataVector)$ which is not necessarily the true\n",
    "posterior distribution.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/u-to-y.svg\" class=\"\" align=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Augmenting the variable space with a set of latent *inducing\n",
    "vectors*</i>\n",
    "\n",
    "Our simple graphical model augmented with $\\inducingVector$ which we\n",
    "refer to as inducing variables. Note that the model is still totally\n",
    "general because $p(\\dataVector, \\inducingVector)$ is an augmented\n",
    "variable model and the original $p(\\dataVector)$ is easily recovered by\n",
    "simple marginalization of $\\inducingVector$. We haven’t yet made any\n",
    "assumptions about our data.\n",
    "\n",
    "The model we’ve introduced now seems remarkably like the parametric\n",
    "model we argued against in the previous section. So what’s going on\n",
    "here, is there going to be some kind of parametric/non parametric 3 card\n",
    "trick where with sleight of hand we are trying to introduce a parametric\n",
    "model? Well clearly not, because I’ve just given the game away. But I\n",
    "believe there are some important differences to the traditional approach\n",
    "for parameterizing a model. Philosophically, our variables\n",
    "$\\inducingVector$ are variables that augment the the model. We have not\n",
    "yet made any assumptions by introducing them. Normally the\n",
    "parameterization of the model instantiates assumptions, but this is not\n",
    "happening here. In particular note that we have *not* assumed that the\n",
    "training data factorize given the inducing variables. Secondly, we are\n",
    "not going to specify the dimensionality of $\\inducingVector$ (i.e. the\n",
    "size of the TT channel) at *design* time. We are going to allow it to\n",
    "change at *run* time. We will do this by ensuring that the inducing\n",
    "variables also obey Kolmogorov consistency. In particular we require\n",
    "that If we build a joint density as follows: where $\\inducingVector$ are\n",
    "the inducing variables we choose might choose to instantiate at any\n",
    "given time (of dimensionality $\\numInducing$) and $\\inducingVector^*$ is\n",
    "the $\\numInducing^*$ dimensional pool of future inducing variables we\n",
    "have *not yet* chosen to instantiate (where $\\numInducing^*$ could be\n",
    "infinite). Our new Kolmogorov consistency condition requires that $$\n",
    "p(\\dataVector, \\inducingVector|\\numInducing^*,\\numData^*) = p(\\dataVector, \\inducingVector).\n",
    "$$ It doesn’t need to be predetermined at *design time* because we allow\n",
    "for the presence of a (potentially infinite) number of inducing\n",
    "variables $\\inducingVector^*$ that we may wish to *later* instantiate to\n",
    "improve the quality of our model. In other words, it is very similar to\n",
    "the parametric approach, but now we have access to a future pool of\n",
    "additional parameters, $\\inducingVector^*$ that we can call upon to\n",
    "increase the bandwidth of the TT channel as appropriate. In parametric\n",
    "modelling, calling upon such parameters has a significant effect on the\n",
    "likelihood of the model, but here these variables are auxiliary\n",
    "variables that will *not* effect the likelihood of the model. They\n",
    "merely effect our ability to approximate the true bandwidth of the TT\n",
    "channel. The quality of this approximation can be varied at run time. It\n",
    "is not necessary to specify it at design time. This gives us the\n",
    "flexibility we need in terms of modeling, whilst keeping computational\n",
    "complexity and memory demands manageable and appropriate to the task at\n",
    "hand.\n",
    "\n",
    "Figure: <i></i>\n",
    "\n",
    "Adding in the test data and the inducing variables we have not yet\n",
    "chosen to instantiate. Here we see that we still haven’t defined any\n",
    "structure in the graph, and therefore we have not yet made any\n",
    "assumptions about our data. Not shown in the graph is the additional\n",
    "assumption that whilst $\\dataVector$ has $\\numData$ dimensions and\n",
    "$\\inducingVector$ has $\\numInducing$ dimensions, $\\dataVector^*$ and\n",
    "$\\inducingVector^*$ are potentially infinite dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Variables\n",
    "\n",
    "To focus our model further, we assume that we observe observations,\n",
    "$\\dataVector$ that are derived from some underlying fundamental,\n",
    "$\\mappingFunctionVector$, through simple factorized likelihoods. The\n",
    "idea of the fundamental variables is that they are sufficient to\n",
    "describe the world around us, but we might not be able to observe them\n",
    "directly. In particular we might observe relatively simple corruptions\n",
    "of the fundamental variables such as independent addition of noise, or\n",
    "thresholding. We might observe something relative about two fundamental\n",
    "veriables. For example if we took $\\mappingFunction_{12,345}$ to be the\n",
    "height of Tom Cruise and $\\mappingFunction_{23,789}$ to be the height of\n",
    "Penelope Cruz then we might take for an observation a binary value\n",
    "indicating the relative heights, so\n",
    "$\\datascalar_{72,394} = \\mappingFunction_{12,345} < \\mappingFunction_{23,789}$.\n",
    "The fundamental variable is an artificial construct, but it can prove to\n",
    "be a useful one. In particular we’d like to assume that the relationship\n",
    "between our observations, $\\dataVector$ and the fundamental variables,\n",
    "$\\mappingFunctionVector$ might factorize in some way. In the framework\n",
    "we think of this relationship, given by $p(\\dataVector|\\inducingVector)$\n",
    "as the *likelihood*. We can ensure that assuming the likelihood\n",
    "factorizes does not at all reduce the generality of our model, by\n",
    "forcing the distribution over the fundamentals,\n",
    "$p(\\mappingFunctionVector)$ to also be Kolmogorov consistent. This\n",
    "ensures that in the case where the the likelihood is fully factorized\n",
    "over $\\numData$ the model is still general if we allow the factors of\n",
    "the likelihood to be Dirac delta functions suggesing that\n",
    "$\\dataScalar_i = \\mappingFunction_i$. Since we haven’t yet specified any\n",
    "forms for the probability distributions this *is* allowed and therefore\n",
    "the formulation is still totally general. $$\n",
    "p(\\dataVector|\\numData^*) = \\int p(\\dataVector|\\mappingFunctionVector) p(\\mappingFunctionVector, \\mappingFunctionVector^*)\\text{d}\\mappingFunctionVector \\text{d}\\mappingFunctionVector^*\n",
    "$$ and since we enforce Kolmogorov consistency we have $$\n",
    "p(\\dataVector|\\numData*) = p(\\dataVector).\n",
    "$$\n",
    "\n",
    "Figure: <i></i>\n",
    "\n",
    "Now we assume some form of factorization for our data observations,\n",
    "$\\dataVector$, given the fundamental variables,\n",
    "$\\mappingFunctionVector$, so that we have $$\n",
    "p(\\dataVector|\\mappingFunctionVector) = \\prod_{i} p(\\dataVector^i| \\mappingFunctionVector^i)\n",
    "$$ so that we have subsets of the data $\\dataVector^i$ which are\n",
    "dependent on sub sets of the fundamental variables, $\\mappingFunction$.\n",
    "For simplicity of notation we will assume a factorization across the\n",
    "entire data set, so each observation, $\\dataScalar_i$, has a single\n",
    "underlying fundamental variable, $\\mappingFunction_i$, although more\n",
    "complex factorizations are also possible and can be considered within\n",
    "the analysis. $$\n",
    "p(\\dataVector|\\mappingFunctionVector) = \\prod_{i=1}^\\numData p(\\dataScalar_i|\\mappingFunction_i)\n",
    "$$\n",
    "\n",
    "We now decompose, without loss of generality, our joint distribution\n",
    "over inducing variables and fundamentals into the following parts $$\n",
    "p(\\inducingVector, \\mappingFunctionVector) = p(\\mappingFunctionVector|\\inducingVector)p(\\inducingVector),\n",
    "$$ where we assume that we have marginalised $\\mappingFunctionVector^*$\n",
    "and $\\inducingVector^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the Model\n",
    "-----------------------\n",
    "\n",
    "So far we haven’t made any assumptions about the data in our model,\n",
    "other than a factorization assumption between the fundamental variables\n",
    "and the observations, $\\dataVector$. Even this assumption does not\n",
    "affect the generality of the model decomposition, because in the worst\n",
    "case the likelihood $p(\\dataVector|\\mappingFunctionVector)$ could be a\n",
    "Dirac $\\delta$ function, implying $\\dataVector=\\mappingFunctionVector$\n",
    "and allowing us to include complex interelations between $\\dataVector$\n",
    "directly in $p(\\mappingFunctionVector)$. We have specified that\n",
    "$p(\\mappingFunctionVector, \\inducingVector)$ should be Kolmogorov\n",
    "consistent with $\\mappingFunctionVector^*$ and $\\inducingVector^*$ being\n",
    "marginalised and we have argued that non-parametric models are important\n",
    "in practice to ensure that all the information in our training data can\n",
    "be passed to the test data.\n",
    "\n",
    "For a model to be useful, we need to specify relationships between our\n",
    "data variables. Of course, this is the point at which a model also\n",
    "typically becomes wrong. The following considerations should arise:\n",
    "\n",
    "If our model is not correct, is it a useful abstraction given what we\n",
    "expect to observe about the data? For example, Brownian motion is\n",
    "modelled as a stochastic differential equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Processes\n",
    "\n",
    "A flexible class of models that fulfils the constraints of being\n",
    "non-parametric and Kolmogorov consistent is Gaussian processes. Gaussian\n",
    "processes assume that the data is jointly Gaussian distributed. Each\n",
    "data point, $\\dataScalar_i$, is is jointly distributed with each other\n",
    "data point $\\dataScalar_j$ as a multivariate Gaussian. The covariance of\n",
    "this Gaussian is a function of the indices of the two data, in this case\n",
    "$i$ and $j$. But these indices are not just restricted to discrete\n",
    "values. The index can be a continuous value such as time, $t$, or\n",
    "spatial location, $\\inputVector$. The words index and indicate have a\n",
    "common etymology. This is appropriate because the index indicates the\n",
    "provenance of the data. In effect we have multivariate indices to\n",
    "account for the full provenance, so that our observations of the world\n",
    "are given as a function of, for example, the when, the where and the\n",
    "what. When is given by time, where is given by spatial location and what\n",
    "is given by a (potentially discrete) index indicating the further\n",
    "provenance of the data. To define a joint Gaussian density, we need to\n",
    "define the mean of the density and the covariance. Both this mean and\n",
    "the covariance also need to be indexed by the when, the where and the\n",
    "what."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting with Inducing Variables in Gaussian Processes\n",
    "\n",
    "To define our model we need to describe the relationship between the\n",
    "fundamental variables, $\\dataMappingVector$, and the inducing variables,\n",
    "$\\inducingVector$. This needs to be done in such a way that the inducing\n",
    "variables are also Kolmogorov consistent. A straightforward way of\n",
    "achieving this is through a joint Gaussian process model over the\n",
    "inducing variables and the data mapping variables, so in other words we\n",
    "define a Gaussian process prior over $$\n",
    "\\begin{bmatrix}\\mappingFunctionVector \\inducingVector\\end{bmatrix} \\sim \\gaussianDist{\\mathbf{m}}{\\kernelMatrix}\n",
    "$$ where the covariance matrix has a block form, $$\n",
    "\\kernelMatrix = \\begin{bmatrix} \\kernelMatrix_{\\mappingFunctionVector\\mappingFunctionVector} & \\kernelMatrix_{\\mappingFunctionVector\\inducingVector} \\ \\kernelMatrix_{\\inducingVector\\mappingFunctionVector} & \\kernelMatrix_{\\inducingVector\\inducingVector}\\end{bmatrix}\n",
    "$$ and $\\kernelMatrix_{\\mappingFunctionVector\\mappingFunctionVector}$\n",
    "gives the covariance between the fundamentals vector,\n",
    "$\\kernelMatrix_{\\inducingVector\\inducingVector}$ gives the covariance\n",
    "matrix between the inducing variables and\n",
    "$\\kernelMatrix_{\\inducingVector\\mappingFunctionVector} = \\kernelMatrix_{\\mappingFunctionVector\\inducingVector}^\\top$\n",
    "gives the cross covariance between the inducing variables,\n",
    "$\\inducingVector$ and the mapping function variables,\n",
    "$\\mappingFunctionVector$.\n",
    "\n",
    "The elements of\n",
    "$\\kernelMatrix_{\\mappingFunctionVector\\mappingFunctionVector}$ will be\n",
    "computed through a covariance function (or kernel) given by\n",
    "$\\kernelScalar_\\mappingFunction(\\inputVector, \\inputVector^\\prime)$\n",
    "where $\\inputVector$ is a vector representing the *provenance* of the\n",
    "data, which as we discussed earlier could involve a spatial location, a\n",
    "time, or something about the nature of the data. In a Gaussian process\n",
    "most of the modelling decisions take place in the construction of\n",
    "$\\kernelScalar_\\mappingFunction(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mean Function\n",
    "\n",
    "The mean of the process is given by a vector $\\mathbf{m}$ which is\n",
    "derived from a mean function $m(\\inputVector)$. There are many occasions\n",
    "when it is useful to include a mean function, but normally the mean\n",
    "function will have a parametric form, $m(\\inputVector;\\paramVector)$,\n",
    "and be subject (in itself) to the same constraints that a standard\n",
    "parametric model has. Indeed, if we choose to model a function as a\n",
    "parametric form plus Gaussian noise, we can recast such a model as a\n",
    "simple Gaussian process with a covariance function\n",
    "$k_\\mappingFunction(\\inputVector_i,\\inputVector_j) = \\dataStd^2 \\delta_{i, j}$,\n",
    "where $\\delta_{i, j}$ is the *Kronecker* delta-function and a mean\n",
    "function that is given by the standard parametric form. In this case we\n",
    "see that the covariance function is mopping up the *residuals* that are\n",
    "not captured by the mean function. If we genuinely were interested in\n",
    "the form of a parametric mean function, as we often are in statistics,\n",
    "where the mean function may include a set of covariates and potential\n",
    "effects, often denoted by $$\n",
    "m(\\inputVector) = \\boldsymbol{\\beta}^\\top \\inputVector,\n",
    "$$ where here the provenance of the data is known as the covariates, and\n",
    "the variable associated with $\\dataVector$ is typically known as a\n",
    "*response* variable. In this case the particular influence of each of\n",
    "the covariates is being encoded in a vector $\\boldsymbol{\\beta}$. To a\n",
    "statistician, the relative values of the elements of this vector are\n",
    "often important in making a judgement about the influence of the\n",
    "covariates. For example, in disease modelling the mean function might be\n",
    "used in a *generalised* linear model through a link function to\n",
    "represent a rate or risk of disease (<span class=\"citeproc-not-found\"\n",
    "data-reference-id=\"Diggle:somewhere\">**???**</span>). The covariates\n",
    "should *co-vary* (or move together) with the response variable.\n",
    "Appropriate covariates for malaria incidence rate might include known\n",
    "influencers of the disease. For example if we are dealing with *malaria*\n",
    "then we might expect disease rates to be influenced by altitude, average\n",
    "temperature, average rainfall, local distribution of prophylactic\n",
    "measures (such as nets) etc. The covariance of the Gaussian process then\n",
    "has the role of taking care of the *residual* variance in the data: the\n",
    "data that is not explained by the mean function, i.e. the variance that\n",
    "cannot be explained by the parametric model. In a disease mapping model\n",
    "it makes sense to assume that these residuals may not be independent. An\n",
    "underestimate of disease at one spatial location, may imply an\n",
    "underestimate of disease rates at a nearby location. The mismatch\n",
    "between the observed disease rate and that predicted by modeling the\n",
    "relationship with the covariates through the mean function is then given\n",
    "by the covariance function.\n",
    "\n",
    "The modeling philosophy in machine learning is somewhat different from\n",
    "that followed in traditional statistics. In machine learning the aim is\n",
    "often to be predictive, rather than explanatory. There is typically less\n",
    "need for an interpretable model, and so the mean function is much less\n",
    "rarely used. The objective is to predict the data entirely through the\n",
    "covariance function. From the arguments we developed earlier about the\n",
    "need for nonparametrics this makes a lot of sense. In particular if we\n",
    "rely on the mean function to make our predictions and assume that the\n",
    "covariance function is dealing with the residuals, then as we obtain\n",
    "more data the parameters of the mean function will become better\n",
    "determined. If the mean function does capture the majority of the\n",
    "variance of our observations, then the role of the covariance function\n",
    "will be reduced to capture only the variance of the residuals. But at\n",
    "this point we are left with a model that is dominated by is parametric\n",
    "part at the expense of its non parametric part. If the parameters have\n",
    "become well determined then the uncertainty about future predictions\n",
    "will be reduced. However, if we enter a novel domain (one where the\n",
    "provenance of the data differs significantly from the data we observed\n",
    "at training time) then we will still make very confident extrapolations\n",
    "when predicting for the new data. For this reason in machine learning we\n",
    "often prefer to leave out the mean function to ensure that the signal\n",
    "variance is explained through non parametric part of the model rather\n",
    "than the parametric mean function. In what follows we will drop the mean\n",
    "function and focus only on the covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.util.datasets.mauna_loa()\n",
    "kern = GPy.kern.Linear(1) + GPy.kern.RBF(1) + GPy.kern.Bias(1)\n",
    "model = GPy.models.GPRegression(data['X'], data['Y'], kern)\n",
    "#model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we *could* interpret Gaussian process models as approaches to dealing\n",
    "with residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling $\\mappingFunctionVector$ \\[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/making-parameters-non-parametric-illustration.md\" target=\"_blank\" >edit</a>\\]\n",
    "\n",
    "In conclusion, for a non parametric framework, our model for\n",
    "$\\mappingFunctionVector$ is predominantly in the covariance function\n",
    "$\\kernelMatrix_{\\mappingFunctionVector\\mappingFunctionVector}$. This is\n",
    "our data model. We are assuming the inducing variables are drawn from a\n",
    "joint Gaussian process with $\\mappingFunctionVector$. The cross\n",
    "covariance between $\\inducingVector$ and $\\mappingFunctionVector$ is\n",
    "given by $\\kernelMatrix_{\\mappingFunctionVector\\inducingVector}$. This\n",
    "gives the relationship between the function and the inducing variables.\n",
    "There are a range of ways in which the inducing variables can interelate\n",
    "with the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative Example\n",
    "\n",
    "For this illustrative example, we’ll consider a simple regression\n",
    "problem. The example is based on one that James Hensman showed at the\n",
    "January 2014 Gaussian process winter school in his talk is on low rank\n",
    "Gaussian process approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to a Simple Regression Problem\n",
    "-----------------------------------\n",
    "\n",
    "Here we set up a simple one dimensional regression problem. The input\n",
    "locations, $\\inputMatrix$, are in two separate clusters. The response\n",
    "variable, $\\dataVector$, is sampled from a Gaussian process with an\n",
    "exponentiated quadratic covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy\n",
    "from scipy import optimize\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise_var = 0.01\n",
    "X = np.zeros((50, 1))\n",
    "X[:25, :] = np.linspace(0,3,25)[:,None] # First cluster of inputs/covariates\n",
    "X[25:, :] = np.linspace(7,10,25)[:,None] # Second cluster of inputs/covariates\n",
    "\n",
    "xlim = (-2,12)\n",
    "ylim = (-4, 0)\n",
    "\n",
    "# Sample response variables from a Gaussian process with exponentiated quadratic covariance.\n",
    "k = GPy.kern.RBF(1)\n",
    "y = np.random.multivariate_normal(np.zeros(N),k.K(X)+np.eye(N)*np.sqrt(noise_var)).reshape(-1,1)\n",
    "scale = np.sqrt(np.var(y))\n",
    "offset = np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we perform a full Gaussian process regression on the data. We\n",
    "create a GP model, `m_full`, and fit it to the data, plotting the\n",
    "resulting fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from gp_tutorial import ax_default, meanplot, gpplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_output(model, output_dim=0, scale=1.0, offset=0.0, ax=None, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "    ax.plot(model.X.flatten(), model.Y[:, output_dim]*scale + offset, 'r.',markersize=10)\n",
    "    ax.set_xlabel(xlabel, fontsize=fontsize)\n",
    "    ax.set_ylabel(ylabel, fontsize=fontsize)\n",
    "    xt = plot.pred_range(model.X, portion=portion)\n",
    "    yt_mean, yt_var = model.predict(xt)\n",
    "    yt_mean = yt_mean*scale + offset\n",
    "    yt_var *= scale*scale\n",
    "    yt_sd=np.sqrt(yt_var)\n",
    "    if yt_sd.shape[1]>1:\n",
    "        yt_sd = yt_sd[:, output_dim]\n",
    "\n",
    "    _ = gpplot(xt.flatten(),\n",
    "               yt_mean[:, output_dim],\n",
    "               yt_mean[:, output_dim]-2*yt_sd.flatten(),\n",
    "               yt_mean[:, output_dim]+2*yt_sd.flatten(), \n",
    "               ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(X,y)\n",
    "m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-full-gp.svg\" class=\"\" align=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A full Gaussian process fit to the simulated data set.</i>\n",
    "\n",
    "Now we set up the inducing variables, $\\inducingVector$. Each inducing\n",
    "variable has its own associated input index, $\\mathbf{Z}$, which lives\n",
    "in the same space as $\\inputMatrix$. Here we are using the true\n",
    "covariance function parameters to generate the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(1)\n",
    "Z = np.hstack(\n",
    "        (np.linspace(2.5,4.,3),\n",
    "        np.linspace(7,8.5,3)))[:,None]\n",
    "m = GPy.models.SparseGPRegression(X,y,kernel=kern,Z=Z)\n",
    "m.noise_var = noise_var\n",
    "m.inducing_inputs.constrain_fixed()\n",
    "#m.tie_params('.*variance')\n",
    "#m.ensure_default_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m) # why is it not printing noise variance correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg\" class=\"\" align=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sparse Gaussian process with six constrained inducing\n",
    "variables and parameters learned.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-constrained-inducing-6-learned-gp.svg\" class=\"\" align=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sparse Gaussian process with six constrained inducing\n",
    "variables and parameters learned.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.randomize()\n",
    "m.inducing_inputs.unconstrain()\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg\" class=\"\" align=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sparse Gaussian process with six unconstrained inducing\n",
    "variables, initialized randomly and then optimized.</i>\n",
    "\n",
    "Now we will vary the number of inducing points used to form the\n",
    "approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.Z.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.num_inducing=8\n",
    "m.randomize()\n",
    "M = 8\n",
    "\n",
    "m.set_Z(np.random.rand(M,1)*12)\n",
    "\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg\" class=\"\" align=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Sparse Gaussian process with eight inducing variables,\n",
    "initialized randomly and then optimized.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.log_likelihood(), m_full.log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty about the Provenance of the Data\n",
    "\n",
    "Provenance could include the time that the data was acquired, the\n",
    "location that the data was acquired, even the ‘type’ of data that is\n",
    "acquired. For example, in computer vision pixels are arriving from\n",
    "different objects. We are uncertain about the provenance of the pixels\n",
    "in terms of which *object* they are arriving from. The spatial location\n",
    "of the object in the image. This uncertainty relates to uncertainty\n",
    "about the covariance function. Unfortunately, it is not directly on the\n",
    "covariance function itself, but relates to values through which the\n",
    "covariance is nonlinearly related. These variables become *latent* or\n",
    "*confounders*.\n",
    "\n",
    "**Not sure about this**: Provenance of data is often finite. Consider a\n",
    "diseased person. That person consists of a finite (if very large) state\n",
    "vector. Of course the number of measurements we can make about that\n",
    "person is infinite. But there are a set of fundamental limitations to\n",
    "what can go wrong with the individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethics\n",
    "------\n",
    "\n",
    "Ownership of data, returning it to the individual. In healthcare the\n",
    "danger of confusing it with marketing, Laplace, and the utopian view of\n",
    "data. Invalidity of insurance. How the results are presented to the\n",
    "patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alvarez, R.M. (Ed.), 2016. Computational social science. Cambridge\n",
    "University Press.\n",
    "\n",
    "Andrés, L., Zentner, A., Zentner, J., 2014. Measuring the effect of\n",
    "internet adoption on paper consumption. The World Bank.\n",
    "\n",
    "Belkin, M., Hsu, D., Ma, S., Soumik Mandal, 2019. Reconciling modern\n",
    "machine-learning practice and the classical bias-variance trade-off.\n",
    "Proc. Natl. Acad. Sci. USA 116, 15849–15854.\n",
    "\n",
    "Box, G.E.P., 1976. Science and statistics. Journal of the American\n",
    "Statistical Association 71.\n",
    "\n",
    "Breiman, L., 2001. Statistical modeling: The two cultures. Statistical\n",
    "Science 16, 199–231.\n",
    "\n",
    "Breiman, L., 1996. Bagging predictors. Machine Learning 24, 123–140.\n",
    "<https://doi.org/10.1007/BF00058655>\n",
    "\n",
    "Carvalho, V.M., Hansen, S., Ortiz, Á., García, J.R., Rodrigo, T., Mora,\n",
    "S.R., Ruiz, J., 2020. Tracking the covid-19 crisis with high-resolution\n",
    "transaction data (No. DP14642). Center for Economic Policy Research.\n",
    "\n",
    "Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., 2019. BERT:\n",
    "Pre-training of deep bidirectional transformers for language\n",
    "understanding, in: Proceedings of the 2019 Conference of the North\n",
    "American Chapter of the Association for Computational Linguistics: Human\n",
    "Language Technologies, Volume 1 (Long and Short Papers). Association for\n",
    "Computational Linguistics, Minneapolis, Minnesota, pp. 4171–4186.\n",
    "<https://doi.org/10.18653/v1/N19-1423>\n",
    "\n",
    "Efron, B., 2020. Prediction, estimation, and attribution. Journal of the\n",
    "American Statistical Association 115, 636–655.\n",
    "<https://doi.org/10.1080/01621459.2020.1762613>\n",
    "\n",
    "Geman, S., Bienenstock, E., Doursat, R., 1992. Neural networks and the\n",
    "bias/variance dilemma. Neural Computation 4, 1–58.\n",
    "<https://doi.org/10.1162/neco.1992.4.1.1>\n",
    "\n",
    "Geman, S., Bienenstock, E., Doursat, R., 1992. Neural networks and the\n",
    "bias/variance dilema. Neural Computation 4, 1–58.\n",
    "\n",
    "Ghahramani, Z., 2015. Probabilistic machine learning and artificial\n",
    "intelligence. Nature 452–459.\n",
    "\n",
    "Halevy, A.Y., Norvig, P., Pereira, F., 2009. The unreasonable\n",
    "effectiveness of data. IEEE Intelligent Systems 24, 8–12.\n",
    "<https://doi.org/10.1109/MIS.2009.36>\n",
    "\n",
    "Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-r., Jaitly, N.,\n",
    "Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N., Kingsbury, B.,\n",
    "2012. Deep neural networks for acoustic modeling in speech recognition:\n",
    "The shared views of four research groups. IEEE Signal Processing\n",
    "Magazine 29, 82–97. <https://doi.org/10.1109/MSP.2012.2205597>\n",
    "\n",
    "Hotelling, H., 1933. Analysis of a complex of statistical variables into\n",
    "principal components. Journal of Educational Psychology 24, 417–441.\n",
    "\n",
    "Jeremy Ginsberg, R.S.P., Matthew H. Mohebbi, 2009. Detecting influenza\n",
    "epdiemics using search engine query data. Nature 1012–1014.\n",
    "\n",
    "Kahneman, D., 2011. Thinking fast and slow.\n",
    "\n",
    "Krizhevsky, A., Sutskever, I., Hinton, G.E., n.d. ImageNet\n",
    "classification with deep convolutional neural networks, in:. pp.\n",
    "1097–1105.\n",
    "\n",
    "Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.\n",
    "Courcier, Paris.\n",
    "\n",
    "Lawrence, N.D., 2012. A unifying probabilistic perspective for spectral\n",
    "dimensionality reduction: Insights and new models. Journal of Machine\n",
    "Learning Research 13.\n",
    "\n",
    "Lawrence, N.D., 2010. Introduction to learning and inference in\n",
    "computational systems biology, in:.\n",
    "\n",
    "Lawrence, N.D., 2005. Probabilistic non-linear principal component\n",
    "analysis with Gaussian process latent variable models. Journal of\n",
    "Machine Learning Research 6, 1783–1816.\n",
    "\n",
    "Lawson, C.L., Hanson, R.J., 1995. Solving least squares problems. SIAM.\n",
    "<https://doi.org/10.1137/1.9781611971217>\n",
    "\n",
    "Li, C., 2020. OpenAI’s gpt-3 language model: A technical overview.\n",
    "\n",
    "McCullagh, P., Nelder, J.A., 1989. Generalized linear models, 2nd ed.\n",
    "Chapman; Hall.\n",
    "\n",
    "Menni, C., Valdes, A.M., Freidin, M.B., Sudre, C.H., Nguyen, L.H., Drew,\n",
    "D.A., Ganesh, S., Varsavsky, T., Cardoso, M.J., Moustafa, J.S.E.-S.,\n",
    "Visconti, A., Hysi, P., Bowyer, R.C.E., Mangino, M., Falchi, M., Wolf,\n",
    "J., Ourselin, S., Chan, A.T., Steves, C.J., Spector, T.D., 2020.\n",
    "Real-time tracking of self-reported symptoms to predict potential\n",
    "covid-19. Nature Medicine 1037–1040.\n",
    "\n",
    "Mitchell, T.M., 1977. Version spaces: A candidate elimination approach\n",
    "to rule-learning (pp. 305–310), in: Proceedings of the Fifth\n",
    "International Joint Conference on Artificial Intelligence.\n",
    "\n",
    "Office for National Statistics, 2020. Coronavirus (covid-19) infection\n",
    "survey pilot: England and wales, 14 august 2020.\n",
    "\n",
    "Pearson, K., 1901. On lines and planes of closest fit to systems of\n",
    "points in space. The London, Edinburgh and Dublin Philosophical Magazine\n",
    "and Journal of Science, Sixth Series 2, 559–572.\n",
    "\n",
    "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.,\n",
    "2019. Language models are unsupervised multitask learners, in:.\n",
    "\n",
    "Roweis, S.T., Saul, L.K., 2000. Nonlinear dimensionality reduction by\n",
    "locally linear embedding. Science 290, 2323–2326.\n",
    "<https://doi.org/10.1126/science.290.5500.2323>\n",
    "\n",
    "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S.,\n",
    "Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei,\n",
    "L., 2015. ImageNet Large Scale Visual Recognition Challenge.\n",
    "International Journal of Computer Vision (IJCV) 115, 211–252.\n",
    "<https://doi.org/10.1007/s11263-015-0816-y>\n",
    "\n",
    "Salganik, M.J., 2018. Bit by bit: Social research in the digital age.\n",
    "Princeton University Press.\n",
    "\n",
    "Schölkopf, B., Smola, A., Müller, K.-R., 1998. Nonlinear component\n",
    "analysis as a kernel eigenvalue problem. Neural Computation 10,\n",
    "1299–1319. <https://doi.org/10.1162/089976698300017467>\n",
    "\n",
    "Sutskever, I., Vinyals, O., Le, Q.V., 2014. Sequence to sequence\n",
    "learning with neural networks, in: Ghahramani, Z., Welling, M., Cortes,\n",
    "C., Lawrence, N.D., Weinberger, K.Q. (Eds.), Advances in Neural\n",
    "Information Processing Systems 27. Curran Associates, Inc., pp.\n",
    "3104–3112.\n",
    "\n",
    "Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. DeepFace: Closing\n",
    "the gap to human-level performance in face verification, in: Proceedings\n",
    "of the IEEE Computer Society Conference on Computer Vision and Pattern\n",
    "Recognition. <https://doi.org/10.1109/CVPR.2014.220>\n",
    "\n",
    "The DELVE Initiative, 2020. Economic aspects of the covid-19 crisis in\n",
    "the uk (No. 5). DELVE.\n",
    "\n",
    "Vapnik, V.N., 1998. Statistical learning theory. wiley, New York.\n",
    "\n",
    "Weinberger, K.Q., Sha, F., Saul, L.K., n.d. Learning a kernel matrix for\n",
    "nonlinear dimensionality reduction, in:. pp. 839–846.\n",
    "\n",
    "Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O., 2017.\n",
    "Understanding deep learning requires rethinking generalization, in:\n",
    "https://openreview.net/forum?id=Sy8gdB9xx (Ed.), International\n",
    "Conference on Learning Representations."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
