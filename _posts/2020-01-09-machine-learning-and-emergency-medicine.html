---
title: "Machine Learning and Emergency Medicine"
venue: "TERM"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: Amazon Cambridge and University of Sheffield
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 0000-0001-9258-1030
date: 2020-01-09
published: 2020-01-09
reveal: 2020-01-09-machine-learning-and-emergency-medicine.slides.html
layout: talk
categories:
- notes
---


<div style="display:none">
  $${% include talk-notation.tex %}$$
</div>

<script src="/talks/figure-magnify.js"></script>
<script src="/talks/figure-animate.js"></script>
    
<div id="modal-frame" class="modal">
  <span class="close" onclick="closeMagnify()">&times;</span>
  <div class="modal-figure">
    <div class="figure-frame">
      <div class="modal-content" id="modal01"></div>
      <!--<img class="modal-content" id="object01">-->
    </div>
    <div class="caption-frame" id="modal-caption"></div>
  </div>
</div>	  

<!-- Front matter -->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!--Back matter-->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<h1 id="my-background">My Background</h1>
<p>I’d like to start this talk by emphasizing that I don’t know anything about emergency medicine, except what I picked up by cycling with a friend who works in A&amp;E in Rotherham.</p>
<p>So as far as I’m aware, all emergency medicine doctors are extremely good cyclists.</p>
<p>What I do know about is deployments of machine learning, how they succeed, how they fail. Both from a manager’s perspective, a domain expert’s perspective and a user’s perspective. For our context, let’s map these people onto the senior administrator, the doctors and nurses and the patients.</p>
<h1 id="artificial-intelligence">Artificial Intelligence</h1>
<p>Let’s start with the term artificial intelligence. It’s an extremely emotive term because intelligence is something precious to us. It implies that the machine is encroaching on territory which we formerly thought of as our own.</p>
<h1 id="anthrox">Anthrox</h1>
<p>The emotive nature of the term is not the only problem, the second problem with the term is anthrox. In the word anthrox the X stands for pomorphisation. So the full word is normally anthropomorphisation.</p>
<p>We are bandwidth limited, so when interacting with other intelligent agents, we make assumptions that they think and operate like us. That is the tendency to anthrox. The problem is pernicious because it’s difficult to say, so we tend not to call it out in case our tongue trips us up. That’s why we’ll call it anthtrox. When communicating with humans, anthrox is a short cut to gaining a form of information coherence of what your objectives are. You can use contextual analogies to align activities. You don’t need to send detailed and explicit instructions. You can rely on your team of professionals to work towards one goal, making the patient safe and comfortable. All of this is implicit in the humans around you. It cannot be taken forgranted in the computer.</p>
<h3 id="bandwidth-constrained-conversations">Bandwidth Constrained Conversations</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> pods</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">pods.notebook.display_plots(<span class="st">&#39;anne-bob-conversation</span><span class="sc">{sample:0&gt;3}</span><span class="st">.svg&#39;</span>, </a>
<a class="sourceLine" id="cb2-2" data-line-number="2">                            <span class="st">&#39;../slides/diagrams&#39;</span>,  sample<span class="op">=</span>IntSlider(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure">
<div id="anne-bob-conversation-civil-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/anne-bob-conversation006.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-bob-conversation-civil-magnify" class="magnify" onclick="magnifyFigure(&#39;anne-bob-conversation-civil&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="anne-bob-conversation-civil-caption" class="caption-frame">
<p>Figure: Conversation relies on internal models of other individuals.</p>
</div>
</div>
<div class="figure">
<div id="anne-bob-conversation-argument-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/anne-bob-conversation007.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-bob-conversation-argument-magnify" class="magnify" onclick="magnifyFigure(&#39;anne-bob-conversation-argument&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="anne-bob-conversation-argument-caption" class="caption-frame">
<p>Figure: Misunderstanding of context and who we are talking to leads to arguments.</p>
</div>
</div>
<p>Embodiment factors imply that, in our communication between humans, what is <em>not</em> said is, perhaps, more important than what is said. To communicate with each other we need to have a model of who each of us are.</p>
<p>To aid this, in society, we are required to perform roles. Whether as a parent, a teacher, an employee or a boss. Each of these roles requires that we conform to certain standards of behaviour to facilitate communication between ourselves.</p>
<p>Control of self is vitally important to these communications.</p>
<p>The high availability of data available to humans undermines human-to-human communication channels by providing new routes to undermining our control of self.</p>
<p>If you must anthrox AI, then please think of it as an extremely literal and willful friend, who ignores social cues. Is the last to remain at the party, will blurt out the most inappropriate conclusions regardless of the social context. Has an encyclopedic knowledge about things that sound irrelevant to you.</p>
<p>The truth is that the current generation of artificial intelligence solutions are almost entirely based on machine learning, and machine learning is simply a combination of compute and statistics. So it will help you a great deal if whenever you see the term AI, you mentally replace it with computers and statistics. That will give you a more realistic understanding of its capabilities.</p>
<h3 id="bandwidth-constrained-conversations-1">Bandwidth Constrained Conversations</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">import</span> pods</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</a></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1">pods.notebook.display_plots(<span class="st">&#39;anne-computer-conversation</span><span class="sc">{sample:0&gt;3}</span><span class="st">.svg&#39;</span>, </a>
<a class="sourceLine" id="cb4-2" data-line-number="2">                            <span class="st">&#39;../slides/diagrams&#39;</span>,  sample<span class="op">=</span>IntSlider(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure">
<div id="anne-computer-conversation-civil-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/anne-computer-conversation006.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-computer-conversation-civil-magnify" class="magnify" onclick="magnifyFigure(&#39;anne-computer-conversation-civil&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="anne-computer-conversation-civil-caption" class="caption-frame">
<p>Figure: Conversation relies on internal models of other individuals.</p>
</div>
</div>
<div class="figure">
<div id="anne-computer-conversation-argument-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/anne-computer-conversation007.svg" width="70%" style=" ">
</object>
</div>
<div id="anne-computer-conversation-argument-magnify" class="magnify" onclick="magnifyFigure(&#39;anne-computer-conversation-argument&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="anne-computer-conversation-argument-caption" class="caption-frame">
<p>Figure: Misunderstanding of context and who we are talking to leads to arguments.</p>
</div>
</div>
<p>Embodiment factors imply that, in our communication between humans, what is <em>not</em> said is, perhaps, more important than what is said. To communicate with each other we need to have a model of who each of us are.</p>
<p>To aid this, in society, we are required to perform roles. Whether as a parent, a teacher, an employee or a boss. Each of these roles requires that we conform to certain standards of behaviour to facilitate communication between ourselves.</p>
<p>Control of self is vitally important to these communications.</p>
<p>The high availability of data available to humans undermines human-to-human communication channels by providing new routes to undermining our control of self.</p>
<p>{Two particular biases that are important are <a href="https://en.wikipedia.org/wiki/Automation_bias">automation bias</a> and</p>
<p>I was skimming a recent nature paper about these techniques in medicine, a paper that used machine learning to classify potentially cancerous images. Throuhout the paper they refer to an ‘AI system’. Whether it is conscious or not, the paper plays on our tendency to anthrox. It makes the work feel like it’s doing something magical, that it relates to our intelligence. This is a particularly dangerous trap for a medical professional to fall into.</p>
<p>Managers see AI as a way of improving processes and giving them more control over decision making in their organisation. People are hard to control, computers are easy to control.</p>
<p>Domain experts have an understanding of context of decisions, and an intuition about what the right decision might be. But they can be blind to unusual situations and overly convinced by their own expertise.</p>
<p>Users are placing trust in the decision making process. They want to believe in the outcome, and like to think that it considers there personal circumstances and that it is fair in some way.</p>
<p>The promise of artificial intelligence is that it is the first generation of automation that will adapt to us rather than us adapting to it. What do I mean by that? The challenges of automation are not new. In the 18th century, automation of the physical labour of pumping mines clear of water was a priority, and the steam engine was invented and evolved for that task. The challenge is that the engine is not flexible, it is designed to do one job repetitively. It is like a small baby. It needs feeding, it needs cleaning, and it has regular emissions. It has no concept of time or convenience of those that maintain it.</p>
<p>Although the computer is more advanced than the steam engine in many respects, it is still utterly dependent on its human operators. For the information it gets in the form of data (the input coal) to the actions it takes in response (the operation of the mine pump). It doesn’t have a contextual awareness of side effects of those decisions because they don’t fall within its cognitive landscape. Just like the steam engine, it doesn’t wear nappies, so any unaccounted for side effects of feeding it with data are felt by the environment.</p>
<div class="figure">
<div id="young-baby-image-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ai/person-cute-portrait-young-small-child-684477-pxhere.com.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="young-baby-image-magnify" class="magnify" onclick="magnifyFigure(&#39;young-baby-image&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="young-baby-image-caption" class="caption-frame">
<p>Figure: Machines are as helpless and unaware as small children, and less cute. Just like a small child they require us to maintain and service them.</p>
</div>
</div>
<div class="figure">
<div id="giant-baby-image-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ai/Giant_Baby_One.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="giant-baby-image-magnify" class="magnify" onclick="magnifyFigure(&#39;giant-baby-image&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="giant-baby-image-caption" class="caption-frame">
<p>Figure: When we deploy at scale the damage done by deploying a naive machine can feel exactly like the damage done by deploying a maliscious machine.</p>
</div>
</div>
<p>The machine is also relentless, as long as it is fed, it can complete actions tirelessly, day or night.</p>
<p>Having said that, it’s remarkable what we’re doing today with computers and statistics. And the drive for this is really the quantity of data available. In particular we have significant advances in what we might think of as tasks that traditionally needed human perception to solve. For example, object detection in images. We can now train computers to identify and highlight objects that a decade ago were only identifiable by humans.</p>
<p>How have we achieved this?</p>
<h1 id="deep-learning-edit">Deep Learning <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h1>
<!-- No slide titles in this context -->
<h3 id="deepface-edit">DeepFace <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-face.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-face.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h3>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deep-face-magnify" class="magnify" onclick="magnifyFigure(&#39;deep-face&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="deep-face-caption" class="caption-frame">
<p>Figure: The DeepFace architecture <span class="citation" data-cites="Taigman:deepface14">(Taigman et al. 2014)</span>, visualized through colors to represent the functional mappings at each layer. There are 120 million parameters in the model.</p>
</div>
</div>
<p>The DeepFace architecture <span class="citation" data-cites="Taigman:deepface14">(Taigman et al. 2014)</span> consists of layers that deal with <em>translation</em> and <em>rotational</em> invariances. These layers are followed by three locally-connected layers and two fully-connected layers. Color illustrates feature maps produced at each layer. The neural network includes more than 120 million parameters, where more than 95% come from the local and fully connected layers.</p>
<h3 id="deep-learning-as-pinball-edit">Deep Learning as Pinball <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-as-pinball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-as-pinball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h3>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-pinball-magnify" class="magnify" onclick="magnifyFigure(&#39;early-pinball&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="early-pinball-caption" class="caption-frame">
<p>Figure: Deep learning models are composition of simple functions. We can think of a pinball machine as an analogy. Each layer of pins corresponds to one of the layers of functions in the model. Input data is represented by the location of the ball from left to right when it is dropped in from the top. Output class comes from the position of the ball as it leaves the pins at the bottom.</p>
</div>
</div>
<p>Sometimes deep learning models are described as being like the brain, or too complex to understand, but one analogy I find useful to help the gist of these models is to think of them as being similar to early pin ball machines.</p>
<p>In a deep neural network, we input a number (or numbers), whereas in pinball, we input a ball.</p>
<p>Think of the location of the ball on the left-right axis as a single number. Our simple pinball machine can only take one number at a time. As the ball falls through the machine, each layer of pins can be thought of as a different layer of ‘neurons’. Each layer acts to move the ball from left to right.</p>
<p>In a pinball machine, when the ball gets to the bottom it might fall into a hole defining a score, in a neural network, that is equivalent to the decision: a classification of the input object.</p>
<p>An image has more than one number associated with it, so it is like playing pinball in a <em>hyper-space</em>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="im">import</span> pods</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</a></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">pods.notebook.display_plots(<span class="st">&#39;pinball</span><span class="sc">{sample:0&gt;3}</span><span class="st">.svg&#39;</span>, </a>
<a class="sourceLine" id="cb6-2" data-line-number="2">                            <span class="st">&#39;../slides/diagrams&#39;</span>,</a>
<a class="sourceLine" id="cb6-3" data-line-number="3">                            sample<span class="op">=</span>IntSlider(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/pinball001.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-initialization-magnify" class="magnify" onclick="magnifyFigure(&#39;pinball-initialization&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="pinball-initialization-caption" class="caption-frame">
<p>Figure: At initialization, the pins, which represent the parameters of the function, aren’t in the right place to bring the balls to the correct decisions.</p>
</div>
</div>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/pinball002.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-trained-magnify" class="magnify" onclick="magnifyFigure(&#39;pinball-trained&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="pinball-trained-caption" class="caption-frame">
<p>Figure: After learning the pins are now in the right place to bring the balls to the correct decisions.</p>
</div>
</div>
<p>Learning involves moving all the pins to be in the correct position, so that the ball ends up in the right place when it’s fallen through the machine. But moving all these pins in hyperspace can be difficult.</p>
<p>In a hyper-space you have to put a lot of data through the machine for to explore the positions of all the pins. Even when you feed many millions of data points through the machine, there are likely to be regions in the hyper-space where no ball has passed. When future test data passes through the machine in a new route unusual things can happen.</p>
<p><em>Adversarial examples</em> exploit this high dimensional space. If you have access to the pinball machine, you can use gradient methods to find a position for the ball in the hyper space where the image looks like one thing, but will be classified as another.</p>
<p>Probabilistic methods explore more of the space by considering a range of possible paths for the ball through the machine. This helps to make them more data efficient and gives some robustness to adversarial examples.</p>
<p>Similar comments could be made for machine learning systems that are recognising speech (e.g. in voice assistances such as Siri, Alexa and Google Assistant). And for automated language translation. However, these systems are critically dependent on our data, on data that has been labelled by humans. They are emulating our perceptual set up through having observed lare amounts of data.</p>
<p>On average we may be able to show that these machines outperform us, but this also can bring problems. It turns out to be important not just whether we are wrong, but how we are wrong.</p>
<h2 id="bias-vs-variance-error-plots-edit">Bias vs Variance Error Plots <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-plots.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-plots.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>Helper function for sampling data from two different classes.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">def</span> create_data(per_cluster<span class="op">=</span><span class="dv">30</span>):</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;Create a randomly sampled data set</span></a>
<a class="sourceLine" id="cb8-3" data-line-number="3"><span class="co">    </span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="co">    :param per_cluster: number of points in each cluster</span></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-6" data-line-number="6">    X <span class="op">=</span> []</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">    y <span class="op">=</span> []</a>
<a class="sourceLine" id="cb8-8" data-line-number="8">    scale <span class="op">=</span> <span class="dv">3</span></a>
<a class="sourceLine" id="cb8-9" data-line-number="9">    prec <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(scale<span class="op">*</span>scale)</a>
<a class="sourceLine" id="cb8-10" data-line-number="10">    pos_mean <span class="op">=</span> [[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>],[<span class="dv">0</span>,<span class="fl">0.5</span>],[<span class="dv">1</span>,<span class="dv">0</span>]]</a>
<a class="sourceLine" id="cb8-11" data-line-number="11">    pos_cov <span class="op">=</span> [[prec, <span class="fl">0.</span>], [<span class="fl">0.</span>, prec]]</a>
<a class="sourceLine" id="cb8-12" data-line-number="12">    neg_mean <span class="op">=</span> [[<span class="dv">0</span>, <span class="fl">-0.5</span>],[<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.5</span>],[<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.5</span>]]</a>
<a class="sourceLine" id="cb8-13" data-line-number="13">    neg_cov <span class="op">=</span> [[prec, <span class="fl">0.</span>], [<span class="fl">0.</span>, prec]]</a>
<a class="sourceLine" id="cb8-14" data-line-number="14">    <span class="cf">for</span> mean <span class="kw">in</span> pos_mean:</a>
<a class="sourceLine" id="cb8-15" data-line-number="15">        X.append(np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>pos_cov, size<span class="op">=</span>per_class))</a>
<a class="sourceLine" id="cb8-16" data-line-number="16">        y.append(np.ones((per_class, <span class="dv">1</span>)))</a>
<a class="sourceLine" id="cb8-17" data-line-number="17">    <span class="cf">for</span> mean <span class="kw">in</span> neg_mean:</a>
<a class="sourceLine" id="cb8-18" data-line-number="18">        X.append(np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>neg_cov, size<span class="op">=</span>per_class))</a>
<a class="sourceLine" id="cb8-19" data-line-number="19">        y.append(np.zeros((per_class, <span class="dv">1</span>)))</a>
<a class="sourceLine" id="cb8-20" data-line-number="20">    <span class="cf">return</span> np.vstack(X), np.vstack(y).flatten()</a></code></pre></div>
<p>Helper function for plotting the decision boundary of the SVM.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">def</span> plot_contours(ax, cl, xx, yy, <span class="op">**</span>params):</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;Plot the decision boundaries for a classifier.</span></a>
<a class="sourceLine" id="cb9-3" data-line-number="3"></a>
<a class="sourceLine" id="cb9-4" data-line-number="4"><span class="co">    :param ax: matplotlib axes object</span></a>
<a class="sourceLine" id="cb9-5" data-line-number="5"><span class="co">    :param cl: a classifier</span></a>
<a class="sourceLine" id="cb9-6" data-line-number="6"><span class="co">    :param xx: meshgrid ndarray</span></a>
<a class="sourceLine" id="cb9-7" data-line-number="7"><span class="co">    :param yy: meshgrid ndarray</span></a>
<a class="sourceLine" id="cb9-8" data-line-number="8"><span class="co">    :param params: dictionary of params to pass to contourf, optional</span></a>
<a class="sourceLine" id="cb9-9" data-line-number="9"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb9-10" data-line-number="10">    Z <span class="op">=</span> cl.decision_function(np.c_[xx.ravel(), yy.ravel()])</a>
<a class="sourceLine" id="cb9-11" data-line-number="11">    Z <span class="op">=</span> Z.reshape(xx.shape)</a>
<a class="sourceLine" id="cb9-12" data-line-number="12">    <span class="co"># Plot decision boundary and regions</span></a>
<a class="sourceLine" id="cb9-13" data-line-number="13">    out <span class="op">=</span> ax.contour(xx, yy, Z, </a>
<a class="sourceLine" id="cb9-14" data-line-number="14">                     levels<span class="op">=</span>[<span class="op">-</span><span class="fl">1.</span>, <span class="fl">0.</span>, <span class="dv">1</span>], </a>
<a class="sourceLine" id="cb9-15" data-line-number="15">                     colors<span class="op">=</span><span class="st">&#39;black&#39;</span>, </a>
<a class="sourceLine" id="cb9-16" data-line-number="16">                     linestyles<span class="op">=</span>[<span class="st">&#39;dashed&#39;</span>, <span class="st">&#39;solid&#39;</span>, <span class="st">&#39;dashed&#39;</span>])</a>
<a class="sourceLine" id="cb9-17" data-line-number="17">    out <span class="op">=</span> ax.contourf(xx, yy, Z, </a>
<a class="sourceLine" id="cb9-18" data-line-number="18">                     levels<span class="op">=</span>[Z.<span class="bu">min</span>(), <span class="dv">0</span>, Z.<span class="bu">max</span>()], </a>
<a class="sourceLine" id="cb9-19" data-line-number="19">                     colors<span class="op">=</span>[[<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>], [<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>]])</a>
<a class="sourceLine" id="cb9-20" data-line-number="20">    <span class="cf">return</span> out</a></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="im">import</span> mlai</a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="im">import</span> os</a></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">def</span> decision_boundary_plot(models, X, y, axs, filename, titles, xlim, ylim):</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;Plot a decision boundary on the given axes</span></a>
<a class="sourceLine" id="cb11-3" data-line-number="3"><span class="co">    </span></a>
<a class="sourceLine" id="cb11-4" data-line-number="4"><span class="co">    :param axs: the axes to plot on.</span></a>
<a class="sourceLine" id="cb11-5" data-line-number="5"><span class="co">    :param models: the SVM models to plot</span></a>
<a class="sourceLine" id="cb11-6" data-line-number="6"><span class="co">    :param titles: the titles for each axis</span></a>
<a class="sourceLine" id="cb11-7" data-line-number="7"><span class="co">    :param X: input training data</span></a>
<a class="sourceLine" id="cb11-8" data-line-number="8"><span class="co">    :param y: target training data&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb11-9" data-line-number="9">    <span class="cf">for</span> ax <span class="kw">in</span> axs.flatten():</a>
<a class="sourceLine" id="cb11-10" data-line-number="10">        ax.clear()</a>
<a class="sourceLine" id="cb11-11" data-line-number="11">    X0, X1 <span class="op">=</span> X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb11-12" data-line-number="12">    <span class="cf">if</span> xlim <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb11-13" data-line-number="13">        xlim <span class="op">=</span> [X0.<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X0.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb11-14" data-line-number="14">    <span class="cf">if</span> ylim <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb11-15" data-line-number="15">        ylim <span class="op">=</span> [X1.<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X1.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb11-16" data-line-number="16">    xx, yy <span class="op">=</span> np.meshgrid(np.arange(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="fl">0.02</span>),</a>
<a class="sourceLine" id="cb11-17" data-line-number="17">                         np.arange(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="fl">0.02</span>))</a>
<a class="sourceLine" id="cb11-18" data-line-number="18">    <span class="cf">for</span> cl, title, ax <span class="kw">in</span> <span class="bu">zip</span>(models, titles, axs.flatten()):</a>
<a class="sourceLine" id="cb11-19" data-line-number="19">        plot_contours(ax, cl, xx, yy,</a>
<a class="sourceLine" id="cb11-20" data-line-number="20">                      cmap<span class="op">=</span>plt.cm.coolwarm, alpha<span class="op">=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb11-21" data-line-number="21">        ax.plot(X0[y<span class="op">==</span><span class="dv">1</span>], X1[y<span class="op">==</span><span class="dv">1</span>], <span class="st">&#39;r.&#39;</span>, markersize<span class="op">=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb11-22" data-line-number="22">        ax.plot(X0[y<span class="op">==</span><span class="dv">0</span>], X1[y<span class="op">==</span><span class="dv">0</span>], <span class="st">&#39;g.&#39;</span>, markersize<span class="op">=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb11-23" data-line-number="23">        ax.set_xlim(xlim)</a>
<a class="sourceLine" id="cb11-24" data-line-number="24">        ax.set_ylim(ylim)</a>
<a class="sourceLine" id="cb11-25" data-line-number="25">        ax.set_xticks(())</a>
<a class="sourceLine" id="cb11-26" data-line-number="26">        ax.set_yticks(())</a>
<a class="sourceLine" id="cb11-27" data-line-number="27">        ax.set_title(title)</a>
<a class="sourceLine" id="cb11-28" data-line-number="28">        mlai.write_figure(os.path.join(filename),</a>
<a class="sourceLine" id="cb11-29" data-line-number="29">                          figure<span class="op">=</span>fig,</a>
<a class="sourceLine" id="cb11-30" data-line-number="30">                          transparent<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb11-31" data-line-number="31">    <span class="cf">return</span> xlim, ylim</a></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="im">import</span> matplotlib</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">font <span class="op">=</span> {<span class="st">&#39;family&#39;</span> : <span class="st">&#39;sans&#39;</span>,</a>
<a class="sourceLine" id="cb12-3" data-line-number="3">        <span class="st">&#39;weight&#39;</span> : <span class="st">&#39;bold&#39;</span>,</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">        <span class="st">&#39;size&#39;</span>   : <span class="dv">22</span>}</a>
<a class="sourceLine" id="cb12-5" data-line-number="5"></a>
<a class="sourceLine" id="cb12-6" data-line-number="6">matplotlib.rc(<span class="st">&#39;font&#39;</span>, <span class="op">**</span>font)</a>
<a class="sourceLine" id="cb12-7" data-line-number="7"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># Create an instance of SVM and fit the data. </span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2">C <span class="op">=</span> <span class="fl">100.0</span>  <span class="co"># SVM regularization parameter</span></a>
<a class="sourceLine" id="cb13-3" data-line-number="3">gammas <span class="op">=</span> [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb13-4" data-line-number="4"></a>
<a class="sourceLine" id="cb13-5" data-line-number="5"></a>
<a class="sourceLine" id="cb13-6" data-line-number="6">per_class<span class="op">=</span><span class="dv">30</span></a>
<a class="sourceLine" id="cb13-7" data-line-number="7">num_samps <span class="op">=</span> <span class="dv">20</span></a>
<a class="sourceLine" id="cb13-8" data-line-number="8"><span class="co"># Set-up 2x2 grid for plotting.</span></a>
<a class="sourceLine" id="cb13-9" data-line-number="9">fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb13-10" data-line-number="10">xlim<span class="op">=</span><span class="va">None</span></a>
<a class="sourceLine" id="cb13-11" data-line-number="11">ylim<span class="op">=</span><span class="va">None</span></a>
<a class="sourceLine" id="cb13-12" data-line-number="12"><span class="cf">for</span> samp <span class="kw">in</span> <span class="bu">range</span>(num_samps):</a>
<a class="sourceLine" id="cb13-13" data-line-number="13">    X, y<span class="op">=</span>create_data(per_class)</a>
<a class="sourceLine" id="cb13-14" data-line-number="14">    models <span class="op">=</span> []</a>
<a class="sourceLine" id="cb13-15" data-line-number="15">    titles <span class="op">=</span> []</a>
<a class="sourceLine" id="cb13-16" data-line-number="16">    <span class="cf">for</span> gamma <span class="kw">in</span> gammas:</a>
<a class="sourceLine" id="cb13-17" data-line-number="17">        models.append(svm.SVC(kernel<span class="op">=</span><span class="st">&#39;rbf&#39;</span>, gamma<span class="op">=</span>gamma, C<span class="op">=</span>C))</a>
<a class="sourceLine" id="cb13-18" data-line-number="18">        titles.append(<span class="st">&#39;$\gamma=</span><span class="sc">{}</span><span class="st">$&#39;</span>.<span class="bu">format</span>(gamma))</a>
<a class="sourceLine" id="cb13-19" data-line-number="19">    models <span class="op">=</span> (cl.fit(X, y) <span class="cf">for</span> cl <span class="kw">in</span> models)</a>
<a class="sourceLine" id="cb13-20" data-line-number="20">    xlim, ylim <span class="op">=</span> decision_boundary_plot(models, X, y, </a>
<a class="sourceLine" id="cb13-21" data-line-number="21">                           axs<span class="op">=</span>ax, </a>
<a class="sourceLine" id="cb13-22" data-line-number="22">                           filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml/bias-variance</span><span class="sc">{samp:0&gt;3}</span><span class="st">.svg&#39;</span>.<span class="bu">format</span>(samp<span class="op">=</span>samp), </a>
<a class="sourceLine" id="cb13-23" data-line-number="23">                           titles<span class="op">=</span>titles,</a>
<a class="sourceLine" id="cb13-24" data-line-number="24">                          xlim<span class="op">=</span>xlim,</a>
<a class="sourceLine" id="cb13-25" data-line-number="25">                          ylim<span class="op">=</span>ylim)</a></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="im">import</span> pods</a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</a></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1">pods.notebook.display_plots(<span class="st">&#39;bias-variance</span><span class="sc">{samp:0&gt;3}</span><span class="st">.svg&#39;</span>, </a>
<a class="sourceLine" id="cb15-2" data-line-number="2">                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml&#39;</span>, </a>
<a class="sourceLine" id="cb15-3" data-line-number="3">                            samp<span class="op">=</span>IntSlider(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>))</a></code></pre></div>
<!---->
<div class="figure">
<div id="bias-variance-errors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bias-variance000.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bias-variance010.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="bias-variance-errors-magnify" class="magnify" onclick="magnifyFigure(&#39;bias-variance-errors&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="bias-variance-errors-caption" class="caption-frame">
<p>Figure: In each figure the more simple model is on the left, and the more complex model is on the right. Each fit is done to a different version of the data set. The simpler model is more consistent in its errors (bias error), whereas the more complex model is varying in its errors (variance error).</p>
</div>
</div>
<p>One of the very real problems we face at the moment is we have an ecosystem of people and companies that are delivering solutions that are not driven by the ‘pain points’ of our health service, not driven by getting the fundamentals right. Not driven by solutions that would make a difference for patients, doctors, nurses and administrators but driven by headlines. These are companies that are not making money from these solutions, but are engaged in a wider battle for mindshare. To give the impression that AI systems are delivering magical solutions.</p>
<p>It’s an intrinsic part of their agenda because in the near term this is the fastest route to large scale investment. Capturing the imagination of the public, investors, vice-presidents and CEOs is a far surer route to a billion dollar check than actually trying to build and deploy a product in such a complex ecosystem as the healthcare market.</p>
<p>But we shouldn’t bemoan this too much. This stage is likely inevitable, but it is unsustainable. The key question is, how do we take advantage of the excitement to deliver sustainable change that benefits all doctors?</p>
<p>Many of the particular answers to this are personal, but here’s a couple of ideas I think might help. Firstly, it’s a learning opportunity. First, you should assimilate the skepticism, but once you’ve understood the very real reasons why there won’t be overnight success, then you should indulge your curiousity, and allow the child in you to emerge. There are fascinating technologies emerging, and the current excitement means there will be more opportunities to learn about these technologies and inspire you to think about how they might make your life easier. But then you need to take off the rose-tinted spectacles and examine how such technologies might be making a difference in your particular A&amp;E. How does this neural network change the life of the metal worker from Rotherham with a steel splint in her eye? Because if that question isn’t being asked, then the applicability of the technology is not being considered. The benefit may of course be indirect, but if it is not eventually felt by patients in some way then the utility is questionable.</p>
<p>Secondly, to deploy any of these systems you will find that the main barriers are in your existing processes. A classic mistake is to believe that you can bring in an advanced machine learning technique and deploy it within your hospital environment, when the reality is that the machine will face all the challenges you face and more. In particular, given that the machine is making decisions on the basis of data that you provide, how does it get access to that data? How do you ensure that the data is of sufficient quality and consistency for the machine to make a high quality decision? How do you ensure that patient privacy is respected in the processing of that data, and that the decision isn’t biased to favour a particular group? As you face the reality of the deployment you’re considering you’ll find that the real challenges are the very poor quality of existing processes. Improving those processes will improve outcome even if you do not integrate and deploy an AI system. Making your department ‘AI ready’ is far more work than building the machine learning model.</p>
<p>An excellent way of considering these two challenges can be done today, without major AI expertise. If you have an idea for how AI can help you in your job, here’s my suggestion. Go back to your department and mock-it-up. Whatever decision the AI is going to take can be first taken by a human. Decide what information that human will have available. Choose something where the human is going to make a decision quickly on the basis of only the information they have in front of them. The decision should be one that can take less than a second. Now install that human in your decision making pipeline. How is that decision going to be presented to patients, nurses and doctors? What does the interface look like? Remember, you don’t get to ask the decision maker to contextualise it, you need to be able to take the decision at face value and integrate it into your existing processes.</p>
<p>I suspect there are very few areas where this can be usefully done in an exising A&amp;E department. Why aren’t there more? Because currently most of the processes rely on human flexibility and ability to contextualise. They are poorly documented, and in many cases deliberately obscured in an effort to protect individual domains. The very excercise of understanding why these decisions are difficult to make can be helpful in improving processes in any organisation, and I suspect A&amp;E departments in hospitals are no different.</p>
<p>It’s equivalent to turning up in a 16th century farm with an internal combustion engine. At that time in the UK<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> 58% of the labour force worked in agriculture, today 1.2% of the labour force are in agriculture. But the only people who are empowered to make this change are the domain experts. The people in this room.</p>
<h2 id="propagation-of-best-practice">Propagation of Best Practice</h2>
<p>We must also be careful to maintain openness in this new genaration of digital solutions for patient care. Matthew Syed’s book, “Black Box Thinking” <span class="citation" data-cites="Syed:blackbox15">(Syed 2015)</span>, emphasizes the importance of surfacing errors as a route to learning and improved process. Taking aviation as an example, and contrasting it with the culture in medicine, Matthew relates the story of <a href="https://chfg.org/trustees/martin-bromiley/">Martin Bromiley</a>, an airline pilot whose wife died during a routine hospital procedure and his efforts to improve the culture of safety in medicine. The motivation for the book is the difference in culture between aviation and medicine in how errors are acknowledged and dealt with. We must ensure that these high standards of oversight apply to the era of data-driven automated decision making.</p>
<p>In particular, while there is much to be gained by involving comemrcial companies, if the process by which they are drawing inference about patient condition is hidden (for example, due to commercial confidentiality), this may prevent us from understanding errors in diagnosis or treatment. This would be a retrograde step. It may be that health device certification needs modification or reform for data-driven automated decision making, but we need a spirit of transparency around how these systems are deriving their inferences to ensure best practice.</p>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Syed:blackbox15">
<p>Syed, Matthew. 2015. <em>Black Box Thinking: The Surprising Truth About Success</em>. John Murray.</p>
</div>
<div id="ref-Taigman:deepface14">
<p>Taigman, Yaniv, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014. “DeepFace: Closing the Gap to Human-Level Performance in Face Verification.” In <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>. <a href="https://doi.org/10.1109/CVPR.2014.220" class="uri">https://doi.org/10.1109/CVPR.2014.220</a>.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>farm<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>


