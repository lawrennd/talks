{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)} % Kullback Leibler divergence\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{\\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{{\\bf Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{{\\bf Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{{\\bf Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{{\\bf Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\activationFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{\\activationFunction}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{\\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{\\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{\\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{\\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{\\MakeUppercase{\\coregionalizationScalar}}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{\\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{\\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceTwoScalar}{s}\n",
    "\\newcommand{\\covarianceTwoVector}{\\mathbf{\\covarianceTwoScalar}}\n",
    "\\newcommand{\\covarianceTwoMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{\\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{\\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{\\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{\\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{\\degreeScalar}}\n",
    "\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{\\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{\\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{\\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{\\MakeUppercase{\\eigenvectorScalar}}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{\\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{\\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{\\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{\\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{\\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{\\MakeUppercase{\\inducingInputScalar}}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{\\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{\\MakeUppercase{\\inducingScalar}}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{{\\bf X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVector}{{\\bf \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}} % Kernel matrix\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{\\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{\\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{\\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{\\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{\\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{\\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{x}\n",
    "\\newcommand{\\latentVector}{\\mathbf{\\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{\\MakeUppercase{\\latentScalar}}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{\\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{\\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{\\mappingFunction}}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{\\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{\\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{\\MakeUppercase{\\meanTwoScalar}}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{\\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{\\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{\\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n} % number of data points in the data set\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m} % number of inducing variables in a GP model\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{\\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{\\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{\\MakeUppercase{\\precisionScalar}}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{\\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{\\MakeUppercase{\\pseudotargetScalar}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{\\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{\\MakeUppercase{\\rotationScalar}}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{\\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{\\MakeUppercase{\\sampleCovScalar}}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{\\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{{\\bf \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{\\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{\\MakeUppercase{\\sufficientStatsScalar}}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{\\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{\\MakeUppercase{\\switchScalar}}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)} % matrix trace\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vec}[1]{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{\\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{\\MakeUppercase{\\weightScalar}}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{\\MakeUppercase{\\weightedAdjacencyScalar}}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{\\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}} % vector of ones\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}} % vector of zeros$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Gaussian Process?\n",
    "\n",
    "In this notebook we're going to introduce Gaussian processes, describe what they are and how they can be used in machine learning. \n",
    "\n",
    "First of all, we'll consider the question, what is machine learning? By my definition Machine Learning is a combination of\n",
    "\n",
    "$$\\text{data} + \\text{model} \\rightarrow \\text{prediction}$$\n",
    "\n",
    "and the reason that machine learning has become a mainstay of artificial intelligence is the importance of predictions in artificial intelligence. \n",
    "\n",
    "So how do Gaussian processes come in? Well in practice we normally perform machine learning using two functions. To combine data with a model we typically make use of:\n",
    "\n",
    "**a prediction function** a function which is used to make the predictions. It includes our assumptions about how the world works, e.g. smoothness, spatial similarities, temporal similarities.\n",
    "\n",
    "**an objective function** a function which defines the cost of misprediction. Typically it includes knowledge about the world's generating processes (probabilistic objectives) or the costs we pay for mispredictions (empiricial risk minimization).\n",
    "\n",
    "In practice, we normally also have uncertainty associated with these functions. Uncertainty in the prediction function arises from \n",
    "\n",
    "1. scarcity of training data and \n",
    "2. mismatch between the set of prediction functions we choose and all possible prediction functions.\n",
    "\n",
    "There are also challenges around specification of the objective function, but for we will save those for another day. For the moment, let us focus on the prediction function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and Prediction Functions\n",
    "\n",
    "Neural networks are adaptive non-linear function models. Originally, they were studied (by McCulloch and Pitts [@McCulloc:neuron43]) as simple models for neurons, but over the last decade they have become popular because of their ability to model data. A particular characteristic of neural network models is that they can be composed to form highly complex functions which encode many of our expectations of the real world. They allow us to encode our assumptions about how the world works.\n",
    "\n",
    "We will return to composition later, but for the moment, let's focus on a one hidden layer neural network. We are interested in the prediction function, so we'll ignore the objective function (which is often called an error function) for the moment, and just describe the mathematical object of interest\n",
    "\n",
    "$$\n",
    "\\mappingFunction(\\inputVector) = \\mappingMatrix^\\top \\activationVector(\\mappingMatrixTwo, \\inputVector)\n",
    "$$\n",
    "\n",
    "Where in this case $\\mappingFunction(\\cdot)$ is a scalar function with vector inputs, and $\\activationVector(\\cdot)$ is a vector function with vector inputs. The dimensionality of the vector function is known as the number of hidden units, or the number of neurons. The elements of this vector function are known as the *activation* function of the neural network and $\\mappingMatrixTwo$ are the parameters of the activation functions.\n",
    "\n",
    "In statistics activation functions are traditionally known as *basis functions*. And we would think of this as a *linear model*. It's doesn't make linear predictions, but it's linear because in statistics estimation focuses on the parameters, $\\mappingMatrix$, not the parameters, $\\mappingMatrixTwo$. The linear model terminology refers to the fact that the model is *linear in the parameters*, but it is *not* linear in the data unless the activation functions are chosen to be linear.\n",
    "\n",
    "The first difference in the (early) neural network literature to the classical statistical literature is the decision to optimize these parameters, $\\mappingMatrixTwo$, as well as the  parameters, $\\mappingMatrix$ (which would normally be denoted in statistics by $\\boldsymbol{\\beta}$).\n",
    "\n",
    "In this tutorial, we're going to go revisit that decision, and follow the path of Radford Neal who, inspired by work of David MacKay and others did his PhD thesis on Bayesian Neural Networks. If we take a Bayesian approach to parameter inference (note I am using inference here in the classical sense, not in the sense of prediction of test data, which seems to be a newer usage), then we don't wish to fit parameters at all, rather we wish to integrate them away and understand the family of functions that the model describes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Modelling\n",
    "\n",
    "This Bayesian approach is designed to deal with uncertainty arising from fitting our prediction function to the data we have, a reduced data set.\n",
    "\n",
    "The Bayesian approach can be derived from a broader understanding of what our objective is. If we accept that we can jointly represent all things that happen in the world with a probability distribution, then we can interogate that probability to make predictions. So, if we are interested in predictions, $\\dataScalar_*$ at future points input locations of interest, $\\inputVector_*$ given previously observed test observations, $\\dataVector$ and their corresponding inputs, $\\inputMatrix$, then we are really interogating the following probability density,\n",
    "$$\n",
    "p(\\dataScalar_*|\\dataVector, \\inputMatrix, \\inputVector_*),\n",
    "$$\n",
    "there is nothing controversial here, as long as you accept that you have a good joint model of the world around you that relates test data to training data, $p(\\dataScalar_*, \\dataVector, \\inputMatrix, \\inputVector_*)$ then this conditional distribution can be recovered through standard rules of probability ($\\text{data} + \\text{model} \\rightarrow \\text{prediction}$). \n",
    "\n",
    "We can construct this joint density through the use of the following decomposition:\n",
    "$$\n",
    "p(\\dataScalar_*|\\dataVector, \\inputMatrix, \\inputVector_*) = \\int p(\\dataScalar_*|\\inputVector_*, \\parameterVector) p(\\parameterVector | \\dataVector, \\inputMatrix) \\text{d} \\parameterVector\n",
    "$$\n",
    "where, for convenience, we are assuming *all* the parameters of the model are now represented by $\\parameterVector$ (which contains $\\mappingMatrix$ and $\\mappingMatrixTwo$) and $p(\\parameterVector | \\dataVector, \\inputMatrix)$ is recognised as the posterior density of the parameters given data and $p(\\dataScalar_*|\\inputVector_*, \\parameterVector)$ is the *likelihood* of an individual test data point given the parameters. he likelihood of the data is normally assumed to be independent across the parameters,\n",
    "$$\n",
    "p(\\dataVector|\\inputMatrix, \\parameterVector) \\prod_{i=1}^\\numData p(\\dataScalar_i|\\inputVector_i, \\parameterVector),$$\n",
    "and if that is so, it is easy to extend our predictions across all future, potential, locations,\n",
    "$$\n",
    "p(\\dataVector_*|\\dataVector, \\inputMatrix, \\inputMatrix_*) = \\int p(\\dataVector_*|\\inputMatrix_*, \\parameterVector) p(\\parameterVector | \\dataVector, \\inputMatrix) \\text{d} \\parameterVector.\n",
    "$$\n",
    "The likelihood is also where the *prediction function* is incorporated. For example in the regression case, we consider an objective based around the Gaussian density,\n",
    "$$\n",
    "p(\\dataScalar_i | \\mappingFunction(\\inputVector_i)) = \\frac{1}{\\sqrt{2\\pi \\dataStd^2}} \\exp\\left(-\\frac{\\left(\\dataScalar_i - \\mappingFunction(\\inputVector_i)\\right)^2}{2\\dataStd^2}\\right)\n",
    "$$\n",
    "\n",
    "In short, that is the classical approach to probabilistic inference, and all approaches to Bayesian neural networks fall within this path. For a deep probabilistic model, we can simply take this one stage further and place a probability distribution over the input locations,\n",
    "$$\n",
    "p(\\dataVector_*|\\dataVector) = \\int p(\\dataVector_*|\\inputMatrix_*, \\parameterVector) p(\\parameterVector | \\dataVector, \\inputMatrix) p(\\inputMatrix) p(\\inputMatrix_*) \\text{d} \\parameterVector \\text{d} \\inputMatrix \\text{d}\\inputMatrix_*\n",
    "$$\n",
    "and we have *unsupervised learning*, and families of deep probabilistic models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Of course, the devil is in the detail, and while everything is easy to write in terms of probability densities, as we move from $\\text{data}$ and $\\text{model}$ to $\\text{prediction}$ there is that simple $\\rightarrow$ sign, which is now burying a wealth of difficulties. Each integral sign above is a high dimensional integral which will typically need approximation. Approximations also come with computational demands. As we consider more complex classes of functions, the challenges around the integrals become harder and prediction of future test data given our model and the data becomes so involved as to be impractical or impossible. \n",
    "\n",
    "Statisticians realized these challenges early on, indeed, so early that they were actually physicists, both Laplace and Gauss worked on models such as this, in Gauss's case he made his career on prediction of the location of the lost planet (later reclassified as a asteroid, then dwarf planet), Ceres. Gauss and Laplace made use of maximum a posteriori estimates for simplifying their computations and Laplace developed Laplace's method (and invented the Gaussian density) to expand around that mode. But classical statistics needs better guarantees around model performance and interpretation, and as a result has focussed more on the *linear* model implied by \n",
    "\n",
    "$$\n",
    "\\mappingFunction(\\inputVector) = \\mappingMatrix^\\top \\activationVector(\\parameterVector, \\inputVector)\n",
    "$$\n",
    "\n",
    "by holding $\\parameterVector$ fixed for any given analysis. In this case, a Gaussian prior is formulated over the parameters $\\mappingMatrix$,\n",
    "$$\n",
    "\\mappingMatrix \\sim \\gaussianSamp{\\zerosVector}{\\covarianceMatrix}.\n",
    "$$\n",
    "\n",
    "The Gaussian likelihood given above implies that the data observation is related to the function by noise corruption so we have,\n",
    "$$\n",
    "\\dataScalar_i = \\mappingFunction(\\inputVector_i) + \\noiseScalar_i,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\noiseScalar_i \\sim \\gaussianSamp{0}{\\dataStd^2}\n",
    "$$\n",
    "and while normally integrating over high dimensional parameter vectors is highly complex, here it is *trivial*. That is because of a property of the multivariate Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Properties\n",
    "\n",
    "Gaussian processes are initially of interest because\n",
    "\n",
    "1. linear Gaussian models are easier to deal with \n",
    "2. Even the parameters *within* the process can be handled, by considering a particular limit.\n",
    "\n",
    "Let's first of all review the properties of the Gaussian process that make linear Gaussian models easier to deal with. We'll return to the, perhaps surprising, result on the parameters within the nonlinearity, $\\parameterVector$, shortly.\n",
    "\n",
    "To work with linear Gaussian models, to find the marginal likelihood all you need to know is the following rules. If\n",
    "$$\n",
    "\\dataVector = \\mappingMatrix \\inputVector + \\noiseVector,\n",
    "$$\n",
    "where $\\dataVector$, $\\inputVector$ and $\\noiseVector$ and we assume that $\\inputVector$ and $\\noiseVector$ are drawn from multivariate Gaussians,\n",
    "\\begin{align}\n",
    "\\inputVector & \\sim \\gaussianSamp{\\meanVector}{\\covarianceMatrix}\\\\\n",
    "\\noiseVector & \\sim \\gaussianSamp{\\zerosVector}{\\covarianceTwoMatrix}\n",
    "\\end{align}\n",
    "then we know that $\\dataVector$ is also drawn from a multivariate Gaussian with,\n",
    "$$\n",
    "\\dataVector \\sim \\gaussianSamp{\\mappingMatrix\\meanVector}{\\mappingMatrix\\covarianceMatrix\\mappingMatrix^\\top + \\covarianceTwoMatrix}.\n",
    "$$\n",
    "which in the case given above we can write all training data as \n",
    "$$\n",
    "\\dataVector = \\activationMatrix\\mappingVector + \\noiseVector.\n",
    "$$\n",
    "\n",
    "With apprioriately defined covariance, $\\covarianceTwoMatrix$, this is actually the marginal likelihood for Factor Analysis, or Probabilistic Principal Component Analysis [@], because we integrated out the inputs (or *latent* variables they would be called in that case). \n",
    "\n",
    "However, we are focussing on what happens in models which are non-linear in the inputs, whereas the above would be *linear* in the inputs. To consider these, we introduce a matrix, called the design matrix. We set each activation function computed at each data point to be\n",
    "$$\n",
    "\\activationScalar_{i,j} = \\activationScalar(\\mappingVectorTwo_{j}, \\inputVector_{i})\n",
    "$$\n",
    "and define the matrix of activations (known as the *design matrix* in statistics) to be,\n",
    "$$\n",
    "\\activationMatrix = \n",
    "\\begin{bmatrix}\n",
    "\\activationScalar_{1, 1} & \\activationScalar_{1, 2} & \\dots & \\activationScalar_{1, \\numHidden} \\\\\n",
    "\\activationScalar_{1, 2} & \\activationScalar_{1, 2} & \\dots & \\activationScalar_{1, \\numData} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\activationScalar_{\\numData, 1} & \\activationScalar_{\\numData, 2} & \\dots & \\activationScalar_{\\numData, \\numHidden}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "By convention this matrix always has $\\numData$ rows and $\\numHidden$ columns, now if we define the vector of all noise corruptions, $\\noiseVector = \\left[\\noiseScalar_1, \\dots \\noiseScalar_\\numData\\right]^\\top$. If we define the prior distribution over the vector $\\mappingVector$ to be Gaussian,\n",
    "$$\n",
    "\\mappingVector \\sim \\gaussianSamp{\\zerosVector}{\\alpha\\eye},\n",
    "$$\n",
    "then we can use rules of multivariate Gaussians to see that,\n",
    "$$\n",
    "\\dataVector \\sim \\gaussianSamp{\\zerosVector}{\\alpha \\activationMatrix \\activationMatrix^\\top + \\dataStd^2 \\eye}.\n",
    "$$\n",
    "\n",
    "In other words, our training data is distributed as a multivariate Gaussian, with zero mean and a covariance given by \n",
    "$$\n",
    "\\kernelMatrix = \\alpha \\activationMatrix \\activationMatrix^\\top + \\dataStd^2 \\eye.\n",
    "$$\n",
    "This is an $\\numData \\times \\numData$ size matrix. Its elements are in the form of a function. The maths shows that any element, index by $i$ and $j$, is a function *only* of inputs associated with data points $i$ and $j$, $\\dataVector_i$, $\\dataVector_j$. $\\kernel_{i,j} = \\kernel\\left(\\inputVector_i, \\inputVector_j\\right)$\n",
    "\n",
    "If we look at the portion of this function associated only with $\\mappingFunction(\\cdot)$, i.e. we remove the noise, then we can write down the covariance associated with our neural network,\n",
    "$$\n",
    "\\kernel_\\mappingFunction\\left(\\inputVector_i, \\inputVector_j\\right) = \\alpha \\activationVector\\left(\\mappingMatrixTwo, \\inputVector_i\\right)^\\top \\activationVector\\left(\\mappingMatrixTwo, \\inputVector_j\\right)\n",
    "$$\n",
    "so the elements of the covariance or *kernel* matrix are formed by inner products of the rows of the *design matrix*.  \n",
    "\n",
    "This is the essence of a Gaussian process. Instead of making assumptions about our density over each data point, $\\dataScalar_i$ as i.i.d. we make a joint Gaussian assumption over our data. The covariance matrix is now a function of both the parameters of the activation function, $\\mappingMatrixTwo$, and the input variables, $\\inputMatrix$. This comes about through integrating out the parameters of the model, $\\mappingVector$. \n",
    "\n",
    "We can basically put anything inside the basis functions, and many people do. These can be deep kernels [@Cho:deep09] or we can learn the parameters of a convolutional neural network inside there.\n",
    "\n",
    "Viewing a neural network in this way is also what allows us to beform sensible *batch* normalizations [@Ioffe:batch15]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-degenerate Gaussian Processes\n",
    "\n",
    "The process described above is degenerate. The covariance function is of rank at most $\\numHidden$ and since the theoretical amount of data could always increase $\\numData \\rightarrow \\infty$, the covariance function is not full rank. \n",
    "This means as we increase the amount of data to infinity, there will come a point where we can't normalize the process because the multivariate Gaussian has the form,\n",
    "$$\n",
    "\\gaussianDist{\\mappingFunctionVector}{\\zerosVector}{\\kernelMatrix} = \\frac{1}{\\left(2\\pi\\right)^{\\frac{\\numData}{2}}\\det{\\kernelMatrix}^\\frac{1}{2}} \\exp\\left(-\\frac{\\mappingFunctionVector^\\top\\kernelMatrix \\mappingFunctionVector}{2}\\right) \n",
    "$$\n",
    "and a non-degenerate kernel matrix leads to $\\det{\\kernelMatrix} = 0$ defeating the normalization (it's equivalent to finding a projection in the high dimensional Gaussian where the variance of the the resulting univariate Gaussian is zero, i.e. there is a null space on the covariance, or alternatively you can imagine there are one or more directions where the Gaussian has become the delta function). \n",
    "\n",
    "In the machine learning field, it was Radford Neal [Neal:bayesian94] that realized the potential of the next step. In his 1994 thesis, he was considering Bayesian neural networks, of the type we described above, and in considered what would happen if you took the number of hidden nodes, or neurons, to infinity, i.e. $\\numHidden \\rightarrow \\infty$. \n",
    "\n",
    "![http://www.cs.toronto.edu/~radford/ftp/thesis.pdf](../diagrams/neal-infinite-priors.png)\n",
    "*Page 37 of Radford Neal's 1994 thesis*\n",
    "\n",
    "In loose terms, what Radford considers is what happens to the elements of the covariance function, \n",
    "\\begin{align*}\n",
    "\\kernel_\\mappingFunction\\left(\\inputVector_i, \\inputVector_j\\right) & = \\alpha \\activationVector\\left(\\mappingMatrixTwo, \\inputVector_i\\right)^\\top \\activationVector\\left(\\mappingMatrixTwo, \\inputVector_j\\right)\\\\\n",
    "& = \\sum_k \\activationScalar\\left(\\mappingVectorTwo_k, \\inputVector_i\\right) \\activationScalar\\left(\\mappingVectorTwo_k, \\inputVector_j\\right)\n",
    "\\end{align*}\n",
    "if instead of considering a finite number you sample infinitely many of these activation functions, sampling parameters from a prior density, $p(\\mappingVectorTwo)$, for each one,\n",
    "$$\n",
    "\\kernel_\\mappingFunction\\left(\\inputVector_i, \\inputVector_j\\right) = \\int \\activationScalar\\left(\\mappingVectorTwo, \\inputVector_i\\right) \\activationScalar\\left(\\mappingVectorTwo, \\inputVector_j\\right) p(\\mappingVectorTwo) \\text{d}\\mappingVectorTwo\n",
    "$$\n",
    "And that's not *only* for Gaussian $p(\\mappingVectorTwo)$. In fact this result holds for a range of activations, and a range of prior densities because of the *central limit theorem*. \n",
    "\n",
    "To write it in the form of a probabilistic program, as long as the distribution for $\\phi_i$ implied by this short probabilistic program,\n",
    "\\begin{align*}\n",
    "\\mappingVectorTwo & \\sim p(\\cdot)\\\\\n",
    "\\phi_i & = \\activationScalar\\left(\\mappingVectorTwo, \\inputVector_i\\right), \n",
    "\\end{align*}\n",
    "has finite variance, then the result of taking the number of hidden units to infinity, with appropriate scaling, is also a Gaussian process. \n",
    "\n",
    "To understand this argument in more detail, I highly recommend reading chapter 2 of Neal's thesis, which remains easy to read and clear today. Indeed, for readers interested in Bayesian neural networks, both Raford Neal's and David MacKay's PhD thesis [@MacKay:bayesian92] remain essential reading. Both theses embody a clarity of thought, and an ability to weave together threads from different fields that was the business of machine learning in the 1990s. Radford and David were also pioneers in making their software widely available and publishing material on the web.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and Inference in Gaussian Processes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading and Viewing\n",
    "\n",
    "* Mackay Tutorial\n",
    "\n",
    "[@MacKay:gpintroduction98]\n",
    "\n",
    "* Rasmussen and Williams Book \n",
    "\n",
    "[@Rasmussen:book06]\n",
    "\n",
    "* MacKay Video http://videolectures.net/gpip06_mackay_gpb/\n",
    "\n",
    "* Gaussian Process Summer School Introductions\n",
    "\n",
    "https://www.youtube.com/watch?v=nICydGQCIrs&index=2&list=PLpTp0l_CVmgwyAthrUmmdIFiunV1VvicM&ab_channel=MichaelSmith\n",
    "\n",
    "https://www.youtube.com/watch?v=bH40tEngu7I&index=1&list=PLpTp0l_CVmgwyAthrUmmdIFiunV1VvicM&ab_channel=MichaelSmith\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "@BOOK{Rasmussen:book06,\n",
    "  title = {Gaussian Processes for Machine Learning},\n",
    "  publisher = mit,\n",
    "  year = {2006},\n",
    "  author = {Carl Edward Rasmussen and Christopher K. I. Williams},\n",
    "  address = {Cambridge, MA},\n",
    "  abstract = {Gaussian Processes (GPs) provide a principled, practical, probabilistic\n",
    "\tapproach to learning in kernel machines. GPs have received increased\n",
    "\tattention in the machine-learning community over the past decase,\n",
    "\tand this book provides a long-needed systematic and unified treatment\n",
    "\tof theoretical and practical aspects of GPs in machine learning.\n",
    "\tThe treatment is comprehensive and self-contained, targetd at researchers\n",
    "\tand students in machine learning and applied statistics.\\\\\\\\\n",
    "\t\n",
    "\t The book deals with the supervised-learning problem for both regression\n",
    "\tand classification, and includes detailed algorithms. A wide variety\n",
    "\tof covariance (kernel) functions are presented and their properties\n",
    "\tdiscussed. Model selection is discussed both from a Bayesian and\n",
    "\ta classical perspective. Many connections to other well known techniques\n",
    "\tfrom machine learning and statistics are discussed, including support-vector\n",
    "\tmachines, neural networks, splines, regularization networks, relevance\n",
    "\tvector machines, and others. Theoretical issues including learning\n",
    "\tcurves and the PAC-Bayesian framework are treated, and several approximation\n",
    "\tmethods for learning with large datasets are discussed. The book\n",
    "\tcontains illustrative examples and exercises, and code and datasets\n",
    "\tare available on the Web. Appendixes provide mathematical background\n",
    "\tand a discussion of Gaussian Markov processes.},\n",
    "  group = {gp},\n",
    "  isbn = {0-262-18253-X},\n",
    "  label1 = {Website},\n",
    "  link1 = {http://www.GaussianProcess.org/gpml}\n",
    "}\n",
    "\n",
    "@InProceedings{Cho:deep09,\n",
    "title = {Kernel Methods for Deep Learning},\n",
    "author = {Youngmin Cho and Lawrence K. Saul},\n",
    "booktitle = {Advances in Neural Information Processing Systems 22},\n",
    "editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},\n",
    "pages = {342--350},\n",
    "year = {2009},\n",
    "publisher = {Curran Associates, Inc.},\n",
    "url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}\n",
    "}\n",
    "\n",
    "@InProceedings{Ioffe:batch15,\n",
    "  title = \t {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},\n",
    "  author = \t {Sergey Ioffe and Christian Szegedy},\n",
    "  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n",
    "  pages = \t {448--456},\n",
    "  year = \t {2015},\n",
    "  editor = \t {Francis Bach and David Blei},\n",
    "  volume = \t {37},\n",
    "  series = \t {Proceedings of Machine Learning Research},\n",
    "  address = \t {Lille, France},\n",
    "  month = \t {07--09 Jul},\n",
    "  publisher = \t {PMLR},\n",
    "  pdf = \t {http://proceedings.mlr.press/v37/ioffe15.pdf},\n",
    "  url = \t {http://proceedings.mlr.press/v37/ioffe15.html},\n",
    "  abstract = \t {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\\% top-5 test error, exceeding the accuracy of human raters.}\n",
    "}\n",
    "\n",
    "@INCOLLECTION{MacKay:gpintroduction98,\n",
    "  author = {David J. C. {MacKay}},\n",
    "  title = {Introduction to {G}aussian {P}rocesses},\n",
    "  pages = {133--166},\n",
    "  abstract = {Feedforward neural networks such as multilayer perceptrons are popular\n",
    "\ttools for nonlinear regression and classification problems. From\n",
    "\ta Bayesian perspective, a choice of a neural network model can be\n",
    "\tviewed as defining a prior probability distribution over non-linear\n",
    "\tfunctions, and the neural network's learning process can be interpreted\n",
    "\tin terms of the posterior probability distribution over the unknown\n",
    "\tfunction. (Some learning algorithms search for the function with\n",
    "\tmaximum posterior probability and other Monte Carlo methods draw\n",
    "\tsamples from this posterior probability). In the limit of large but\n",
    "\totherwise standard networks, \\cite{Neal:book96} has shown that the\n",
    "\tprior distribution over non-linear functions implied by the Bayesian\n",
    "\tneural network falls in a class of probability distributions known\n",
    "\tas Gaussian processes. The hyperparameters of the neural network\n",
    "\tmodel determine the characteristic lengthscales of the Gaussian process.\n",
    "\tNeal's observation motivates the idea of discarding parameterized\n",
    "\tnetworks and working directly with Gaussian processes. Computations\n",
    "\tin which the parameters of the network are optimized are then replaced\n",
    "\tby simple matrix operations using the covariance matrix of the Gaussian\n",
    "\tprocess. In this chapter I will review work on this idea by \\cite{Williams:Gaussian96},\n",
    "\t\\cite{Neal:montecarlogp97}, \\cite{Barber:Gaussian97} and \\cite{Gibbs:variational00},\n",
    "\tand will assess whether, for supervised regression and classification\n",
    "\ttasks, the feedforward network has been superceded.},\n",
    "  crossref = {Bishop:neural98},\n",
    "  group = {gp},\n",
    "  linkpsgz = {http://www.cs.toronto.edu/~mackay/gpB.ps.gz}\n",
    "}\n",
    "@PhDThesis{Neal:bayesian94,\n",
    "author = {Radford M. Neal},\n",
    "year = {1994},\n",
    "title = {Bayesian Learning for Neural Networks}, \n",
    "school = {Dept. of Computer Science, University of Toronto},\n",
    "pdf = {http://www.cs.toronto.edu/~radford/ftp/thesis.pdf},\n",
    "softwarelink = {http://www.cs.toronto.edu/~radford/fbm.software.html},\n",
    "abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present difficulties - the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.\n",
    "\n",
    "I address the first problem by defining classes of prior distributions for network parameters that reach sensible limits as the size of the network goes to infinity. In this limit, the properties of these priors can be elucidated. Some priors converge to Gaussian processes, in which functions computed by the network may be smooth, Brownian, or fractionally Brownian. Other priors converge to non-Gaussian stable processes. Interesting effects are obtained by combining priors of both sorts in networks with more than one hidden layer.\n",
    "\n",
    "The problem of integrating over the posterior can be solved using Markov chain Monte Carlo methods. I demonstrate that the hybrid Monte Carlo algorithm, which is based on dynamical simulation, is superior to methods based on simple random walks.\n",
    "\n",
    "I use a hybrid Monte Carlo implementation to test the performance of Bayesian neural network models on several synthetic and real data sets. Good results are obtained on small data sets when large networks are used in conjunction with priors designed to reach limits as network size increases, confirming that with Bayesian learning one need not restrict the complexity of the network based on the size of the data set. A Bayesian approach is also found to be effective in automatically determining the relevance of inputs.\n",
    "\n",
    "Ph.D. Thesis, Dept. of Computer Science, University of Toronto, 195 pages}\n",
    "}\n",
    "\n",
    "@PhDThesis{MacKay:bayesian92,\n",
    "author = {David J. C. MacKay},\n",
    "year = {1992},\n",
    "title = {Bayesian Methods for Adaptive Models}, \n",
    "school = {California Institute of Technology},\n",
    "pdf = {http://www.inference.org.uk/mackay/thesis.pdf},\n",
    "abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying\n",
    "interpolation and classification problems modelled with both linear and non–linear models.\n",
    "This framework quantitatively embodies `Occam's razor'. Over–complex and under–\n",
    "regularised models are automatically inferred to be less probable, even though their flexibility\n",
    "allows them to fit the data better.\n",
    "When applied to `neural networks', the Bayesian framework makes possible (1) objective\n",
    "comparison of solutions using alternative network architectures; (2) objective stopping rules\n",
    "for network pruning or growing procedures; (3) objective choice of type of weight decay\n",
    "terms (or regularisers); (4) on–line techniques for optimising weight decay (or regularisation\n",
    "constant) magnitude; (5) a measure of the effective number of well–determined parameters\n",
    "in a model; (6) quantified estimates of the error bars on network parameters and on network\n",
    "output. In the case of classification models, it is shown that the careful incorporation of\n",
    "error bar information into a classifier’s predictions yields improved performance.\n",
    "Comparisons of the inferences of the Bayesian framework with more traditional cross–\n",
    "validation methods help detect poor underlying assumptions in learning models.\n",
    "The relationship of the Bayesian learning framework to `active learning' is examined.\n",
    "Objective functions are discussed which measure the expected informativeness of candidate\n",
    "data measurements, in the context of both interpolation and classification problems.\n",
    "The concepts and methods described in this thesis are quite general and will be applicable\n",
    "to other data modelling problems whether they involve regression, classification or\n",
    "density estimation.}\n",
    "}\n",
    "@ARTICLE{McCulloch:neuron43,\n",
    "  author = {Warren S. McCulloch and Walter Pitts},\n",
    "  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},\n",
    "  journal = {Bulletin of Mathematical Biophysics},\n",
    "  year = {1943},\n",
    "  volume = {5},\n",
    "  pages = {115--133},\n",
    "  pdf = {https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf},\n",
    "  abstract = {Because of the \"all-or-none\" character of nervous activity, neural\n",
    "events and the relations among them can be treated by means of propositional\n",
    "logic. It is found that the behavior of every net can be described\n",
    "in these terms, with the addition of more complicated logical means for\n",
    "nets containing circles; and that for any logical expression satisfying\n",
    "certain conditions, one can find a net behaving in the fashion it describes.\n",
    "It is shown that many particular choices among possible neurophysiological\n",
    "assumptions are equivalent, in the sense that for every net behaving\n",
    "under one assumption, there exists another net which behaves under\n",
    "the other and gives the same results, although perhaps not in the\n",
    "same time. Various applications of the calculus are discussed. }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
