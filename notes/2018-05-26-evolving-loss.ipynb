{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{\\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{{\\bf Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{{\\bf Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{{\\bf Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{{\\bf Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{\\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{\\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{\\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{\\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{\\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{\\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{\\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{\\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{\\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{\\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{\\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{\\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{\\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{\\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{\\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{\\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{\\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{\\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{\\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{\\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{\\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{{\\bf X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{{\\bf \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{\\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{\\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{\\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{\\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{\\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{\\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{\\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{\\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{\\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{\\mappingFunction}}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{\\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{\\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{\\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{\\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{\\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{\\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{\\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{\\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{\\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{\\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{\\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{\\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{{\\bf \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{\\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{\\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\scaleVector}{\\mathbf{\\scaleScalar}}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{\\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{\\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving Loss\n",
    "\n",
    "Uncertainty in models is handled by Bayesian inference, here we consider uncertainty arising in loss functions.\n",
    "\n",
    "Consider a loss function which decomposes across individual observations, $\\dataScalar_{k,j}$, each of which is dependent on some set of features, $\\inputVector_k$. \n",
    "$$\n",
    "\\sum_{k}\\sum_{j} L(\\dataScalar_{k,j}, \\inputVector_k)\n",
    "$$\n",
    "\n",
    "Assume that the loss function depends on the features through some mapping function, $\\mappingFunction_j(\\cdot)$ which we call the *prediction function*. \n",
    "$$\n",
    "\\sum_{k}\\sum_{j} L(\\dataScalar_{k,j}, \\mappingFunction_j(\\inputVector_k))\n",
    "$$\n",
    "without loss of generality, we can move the index to the inputs, so we have $\\inputVector_i =\\left[\\inputVector \\quad j\\right]$, and we set $\\dataScalar_i = \\dataScalar_{k, j}$. So we have\n",
    "$$\n",
    "\\sum_{i} L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))\n",
    "$$\n",
    "Bayesian inference considers uncertainty in $\\mappingFunction$, often through parameterizing it, $\\mappingFunction(\\inputVector; \\parameterVector)$, and considering a *prior* distribution for the parameters, $p(\\parameterVector)$, this in turn implies a distribution over functions, $p(\\mappingFunction)$. Process models, such as Gaussian processes specify this distribution, known as a process, directly. \n",
    "\n",
    "Bayesian inference proceeds by specifying a *likelihood* which relates the data, $\\dataScalar$, to the parameters. Here we choose not to do this, but instead we only consider the *loss* function for our objective. The loss is the cost we pay for a misclassification. \n",
    "\n",
    "The *risk function* is the expectation of the loss under the distribution of the data. Here we are using the framework of *empirical risk* minimization, because we have a sample based approximation. The new expectation we are considering is around the loss function itself, not the uncertainty in the data.\n",
    "\n",
    "The loss function and the log likelihood may take a mathematically similar form but they are philosophically very different. The log likelihood assumes something about the *generating* function of the data, whereas the loss function assumes something about the cost we pay. Importantly the loss function in Bayesian inference only normally enters at the point of decision.\n",
    "\n",
    "The key idea in Bayesian inference is that the probabilistic inference can be performed *without* knowing the loss becasue if the model is correct, then the form of the loss function is irrelevant when performing inference. In practice, however, the model is *never* correct.\n",
    "\n",
    "Some of the maths below looks similar to the maths we can find in Bayesian methods, in particular variational Bayes, but that is merely a consequence of the availability of analytical mathematics. There are only particular ways of developing tractable algorithms, one route involves linear algebra. However, the similarity of the mathematics belies a difference in interpretation. It is similar to travelling a road (e.g. Ermine Street) in a wild landscape. We travel together because that is where efficient progress is to be made, but in practice a our destinations (Lincoln, York), may be different. \n",
    "\n",
    "To introduce uncertainty we consider a weighted version of the loss function, we introduce positive weights, $\\left\\{\\scaleScalar_i\\right\\}_{i=1}^\\numData$.\n",
    "$$\n",
    "\\sum_{i} \\scaleScalar_i L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))\n",
    "$$\n",
    "We now assume that tmake the assumption that these weights are drawn from a distribution, $q(\\scaleScalar)$. Instead of looking to minimize the loss direction, we look at the expected loss under this distribution.\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\sum_{i}\\expectationDist{\\scaleScalar_i L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))}{q(\\scaleScalar)}\\\\\n",
    "& \\sum_{i}\\expectationDist{\\scaleScalar_i }{q(\\scaleScalar)}L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that our process, $q(\\scaleScalar)$ can depend on a variety of inputs such as $\\dataVector$, $\\inputMatrix$ and time, $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize uncertainty in $q(w)$ we maximize its entropy. Following Jaynes formalism of maximum entropy, in the continuous space we do this with respect to an invariant measure,\n",
    "$$\n",
    "H(\\scaleScalar)= - \\int q(\\scaleScalar) \\log \\frac{q(\\scaleScalar)}{m(\\scaleScalar)} \\text{d}\\scaleScalar\n",
    "$$\n",
    "and since we minimize the loss, we balance this by adding in this term to form\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\beta\\sum_{i}\\expectationDist{\\scaleScalar_i }{q(\\scaleScalar)}L(\\dataScalar_i, \\mappingFunction(\\inputVector_i)) -  H(\\scaleScalar)\\\\\n",
    "&\\propto \\beta\\sum_{i}\\expectationDist{\\scaleScalar_i }{q(\\scaleScalar)}L(\\dataScalar_i, \\mappingFunction(\\inputVector_i)) +  \\int q(\\scaleScalar) \\log \\frac{q(\\scaleScalar)}{m(\\scaleScalar)} \\text{d}\\scaleScalar\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\beta$ serves to weight the relative contribution of the entropy term and the loss term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now minimize this modified loss with respect to the density $q(\\scaleScalar)$, the freeform optimization over this term leads to \n",
    "$$\n",
    "\\begin{align*}\n",
    "q(\\scaleScalar) \\propto & \\exp\\left(- \\beta \\sum_{i=1}^\\numData \\scaleScalar_i L(\\dataScalar_i, \\mappingFunction(\\inputVector_i)) \\right) m(\\scaleScalar)\\\\\n",
    " \\propto & \\prod_{i=1}^\\numData \\exp\\left(- \\beta \\scaleScalar_i L(\\dataScalar_i, \\mappingFunction(\\inputVector_i)) \\right) m(\\scaleScalar)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Assume \n",
    "$$\n",
    "m(\\scaleScalar) = \\prod_i \\lambda\\exp\\left(-\\lambda\\scaleScalar_i\\right)\n",
    "$$\n",
    "which is the distribution with the maximum entropy for a given mean, $\\scaleScalar$. Then we have\n",
    "$$\n",
    "q(\\scaleScalar) = \\prod_i q(\\scaleScalar_i)\n",
    "$$\n",
    "$$\n",
    "q(\\scaleScalar_i) \\propto \\frac{1}{\\lambda+\\beta L_i} \\exp\\left(-(\\lambda+\\beta L_i) w_i\\right)\n",
    "$$\n",
    "and we can compute \n",
    "$$\n",
    "\\expectationDist{\\scaleScalar_i}{q(\\scaleScalar)} = \\frac{1}{\\lambda + \\beta L_i}\n",
    "$$\n",
    "\n",
    "## Algorithmic Optimization\n",
    "\n",
    "We can optimize the expected loss by iterating between an E-step, setting the expectation correctly,\n",
    "$$\n",
    "q(\\scaleScalar_i) = \\frac{1}{\\lambda+\\beta L_i} \\exp\\left(-(\\lambda+\\beta L_i) w_i\\right)\n",
    "$$\n",
    "which implies \n",
    "$$\n",
    "\\expectationDist{\\scaleScalar_i}{q(\\scaleScalar_i)} = \\frac{1}{\\lambda+\\beta L_i}\n",
    "$$\n",
    "and an M-step, optimizing with respect to $\\mappingFunction(\\cdot)$ the expected loss under the expectation, \n",
    "$$\n",
    "\\beta \\sum_{i=1}^\\numData \\expectationDist{\\scaleScalar_i}{q(\\scaleScalar_i)} L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))\n",
    "$$\n",
    "This is recognized as a *reweighted least squares algorithm*.\n",
    "\n",
    "In addition to the above, in our example below, we updated $\\beta$ to normalize the expected loss to be $\\numData$ at each iteration, so we have\n",
    "$$\n",
    "\\beta = \\frac{\\numData}{\\sum_{i=1}^\\numData \\expectationDist{\\scaleScalar_i}{q(\\scaleScalar_i)} L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a weighted linear regression class, inheriting from the ```mlai.LM``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LML(mlai.LM):\n",
    "    \"\"\"Linear model with evolving loss\n",
    "    :param X: input values\n",
    "    :type X: numpy.ndarray\n",
    "    :param y: target values\n",
    "    :type y: numpy.ndarray\n",
    "    :param basis: basis function \n",
    "    :param type: function\n",
    "    :param beta: weight of the loss function\n",
    "    :param type: float\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, basis=None, beta=1.0, lambd=1.0):\n",
    "        \"Initialise\"\n",
    "        if basis is None:\n",
    "            basis = mlai.basis(mlai.polynomial, number=2)\n",
    "        mlai.LM.__init__(self, X, y, basis)\n",
    "        self.s = np.ones((self.num_data, 1))#np.random.rand(self.num_data, 1)>0.5       \n",
    "        self.update_w()\n",
    "        self.sigma2 = 1/beta\n",
    "        self.beta = beta\n",
    "        self.name = 'LML_'+basis.function.__name__\n",
    "        self.objective_name = 'Weighted Sum of Square Training Error'\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def update_QR(self):\n",
    "        \"Perform the QR decomposition on the basis matrix.\"\n",
    "        self.Q, self.R = np.linalg.qr(self.Phi*np.sqrt(self.s))\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Minimize the objective function with respect to the parameters\"\"\"\n",
    "        for i in range(30):\n",
    "            self.update_w() # In the linear regression clas\n",
    "            self.update_s()\n",
    "        \n",
    "    def update_w(self):\n",
    "        self.update_QR()\n",
    "        self.w_star = sp.linalg.solve_triangular(self.R, np.dot(self.Q.T, self.y*np.sqrt(self.s)))\n",
    "        self.update_losses()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the result of the prediction function.\"\"\"\n",
    "        return np.dot(self.basis.Phi(X), self.w_star), None\n",
    "        \n",
    "    def update_s(self):\n",
    "        \"\"\"Update the weights\"\"\"\n",
    "        self.s = 1/(self.lambd + self.beta*self.losses)\n",
    "                                                 \n",
    "    def update_losses(self):\n",
    "        \"\"\"Compute the loss functions for each data point.\"\"\"\n",
    "        self.update_f()\n",
    "        self.losses = ((self.y-self.f)**2)\n",
    "        self.beta = 1/(self.losses*self.s).mean()\n",
    "        \n",
    "    def objective(self):\n",
    "        \"\"\"Compute the objective function.\"\"\"\n",
    "        self.update_losses()\n",
    "        return (self.losses*self.s).sum()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the olympic marathon data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a linear model (polynomial with two basis functions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis=2 \n",
    "data_limits=[1890, 2020]\n",
    "basis = mlai.basis(mlai.polynomial, num_basis, data_limits=data_limits)\n",
    "model = LML(x, y, basis=basis, lambd=1, beta=1)\n",
    "model2 = mlai.LM(x, y, basis=basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()\n",
    "model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(data_limits[0], data_limits[1], 130)[:, None]\n",
    "f_test, f_var = model.predict(x_test)\n",
    "f2_test, f2_var = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "from matplotlib import rc, rcParams\n",
    "rcParams.update({'font.size': 22})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x_test, f2_test, linewidth=3, color='r')\n",
    "ax.plot(x, y, 'g.', markersize=10)\n",
    "ax.set_xlim(data_limits[0], data_limits[1])\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('pace min/km')\n",
    "_ = ax.set_ylim(2, 6)\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-loss-linear-regression000.svg', transparent=True)\n",
    "ax.plot(x_test, f_test, linewidth=3, color='b')\n",
    "ax.plot(x, y, 'g.', markersize=10)\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x.flatten(), model.s.flatten(), width=2, color='b')\n",
    "ax2.set_ylim(0, 4)\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "ax2.set_ylabel('$\\langle s_i \\\\rangle$')\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-loss-linear-regression001.svg', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_plots('olympic-loss-linear-regression{number:0>3}.svg', \n",
    "                            directory='../slides/diagrams/ml', number=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Uncertainty\n",
    "\n",
    "Classical Bayesian inference is concerned with parameter uncertainty, which equates to uncertainty in the *prediction function*, $\\mappingFunction(\\inputVector)$. The prediction function is normally an estimate of the value of $\\dataScalar$ or constructs a probability density for $\\dataScalar$. \n",
    "\n",
    "Uncertainty in the prediction function can arise through uncertainty in our loss function, but also through uncertainty in parameters in the classical Bayesian sense. The full maximum entropy formalism would now be\n",
    "$$\n",
    "\\expectationDist{\\beta \\scaleScalar_i L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))}{q(\\scaleScalar, \\mappingFunction)} + \\int q(\\scaleScalar, \\mappingFunction) \\log \\frac{q(\\scaleScalar, \\mappingFunction)}{m(\\scaleScalar)m(\\mappingFunction)}\\text{d}\\scaleScalar \\text{d}\\mappingFunction\n",
    "$$\n",
    "$$\n",
    "q(\\mappingFunction, \\scaleScalar) \\propto  \\prod_{i=1}^\\numData \\exp\\left(- \\beta \\scaleScalar_i L(\\dataScalar_i, \\mappingFunction(\\inputVector_i)) \\right) m(\\scaleScalar)m(\\mappingFunction)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLML(mlai.BLM):\n",
    "    \"\"\"Bayesian Linear model with evolving loss\n",
    "    :param X: input values\n",
    "    :type X: numpy.ndarray\n",
    "    :param y: target values\n",
    "    :type y: numpy.ndarray\n",
    "    :param basis: basis function \n",
    "    :param type: function\n",
    "    :param beta: weight of the loss function\n",
    "    :param type: float\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, basis=None, alpha=1.0, beta=1.0, lambd=1.0):\n",
    "        \"Initialise\"\n",
    "        if basis is None:\n",
    "            basis = mlai.basis(mlai.polynomial, number=2)\n",
    "        mlai.BLM.__init__(self, X, y, basis=basis, alpha=alpha, sigma2=1/beta)\n",
    "        self.s = np.ones((self.num_data, 1))#np.random.rand(self.num_data, 1)>0.5       \n",
    "        self.update_w()\n",
    "        self.beta = beta\n",
    "        self.name = 'BLML_'+basis.function.__name__\n",
    "        self.objective_name = 'Weighted Sum of Square Training Error'\n",
    "        self.lambd = lambd     \n",
    "\n",
    "    def update_QR(self):\n",
    "        \"Perform the QR decomposition on the basis matrix.\"\n",
    "        self.Q, self.R = np.linalg.qr(np.vstack([self.Phi*np.sqrt(self.s), np.sqrt(self.sigma2/self.alpha)*np.eye(self.basis.number)]))\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Minimize the objective function with respect to the parameters\"\"\"\n",
    "        for i in range(30):\n",
    "            self.update_w()\n",
    "            self.update_s()\n",
    "        \n",
    "    def update_w(self):\n",
    "        self.update_QR()\n",
    "        self.QTy = np.dot(self.Q[:self.y.shape[0], :].T, self.y*np.sqrt(self.s))\n",
    "        self.mu_w = sp.linalg.solve_triangular(self.R, self.QTy)\n",
    "        self.RTinv = sp.linalg.solve_triangular(self.R, np.eye(self.R.shape[0]), trans='T')\n",
    "        self.C_w = np.dot(self.RTinv, self.RTinv.T)\n",
    "        self.update_losses()\n",
    "\n",
    "    def update_s(self):\n",
    "        \"\"\"Update the weights\"\"\"\n",
    "        self.s = 1/(self.lambd + self.beta*self.losses)\n",
    "                                                 \n",
    "    def update_losses(self):\n",
    "        \"\"\"Compute the loss functions for each data point.\"\"\"\n",
    "        self.update_f()\n",
    "        self.losses = ((self.y-self.f_bar)**2) + self.f_cov[:, np.newaxis]\n",
    "        self.beta = 1/(self.losses*self.s).mean()\n",
    "        self.sigma2=1/self.beta\n",
    "        \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BLML(x, y, basis=basis, alpha=1000, lambd=1, beta=1)\n",
    "model2 = mlai.BLM(x, y, basis=basis, alpha=1000, sigma2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()\n",
    "model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(data_limits[0], data_limits[1], 130)[:, None]\n",
    "f_test, f_var = model.predict(x_test)\n",
    "f2_test, f2_var = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gp_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "from matplotlib import rc, rcParams\n",
    "rcParams.update({'font.size': 22})\n",
    "rc('text', usetex=True)\n",
    "gp_tutorial.gpplot(x_test, f2_test, f2_test - 2*np.sqrt(f2_var), f2_test + 2*np.sqrt(f2_var), ax=ax, edgecol='r', fillcol='#CC3300')\n",
    "ax.plot(x, y, 'g.', markersize=10)\n",
    "ax.set_xlim(data_limits[0], data_limits[1])\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('pace min/km')\n",
    "_ = ax.set_ylim(2, 6)\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-loss-bayes-linear-regression000.svg', transparent=True)\n",
    "gp_tutorial.gpplot(x_test, f_test, f_test - 2*np.sqrt(f_var), f_test + 2*np.sqrt(f_var), ax=ax, edgecol='b', fillcol='#0033CC')\n",
    "#ax.plot(x_test, f_test, linewidth=3, color='b')\n",
    "ax.plot(x, y, 'g.', markersize=10)\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x.flatten(), model.s.flatten(), width=2, color='b')\n",
    "ax2.set_ylim(0, 0.2)\n",
    "ax2.set_yticks([0, 0.1, 0.2])\n",
    "ax2.set_ylabel('$\\langle s_i \\\\rangle$')\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-loss-bayes-linear-regression001.svg', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_plots('olympic-loss-bayes-linear-regression{number:0>3}.svg', \n",
    "                            directory='../slides/diagrams/ml', number=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Scales\n",
    "\n",
    "Going beyond independence between weights, we now consider $m(\\vScalar)$ to be a Gaussian process, and scale by the *square* of $\\vScalar$, $\\scaleScalar=\\vScalar^2$ that gives us\n",
    "$$\n",
    "q(\\vScalar) \\propto \\prod_{i=1}^\\numData \\exp\\left(- \\beta \\vScalar_i^2 L(\\dataScalar_i, \\mappingFunction(\\inputVector_i)) \\right) \\exp\\left(-\\frac{1}{2}(\\vVector-\\meanVector)^\\top \\kernelMatrix^{-1} (\\vVector-\\meanVector)\\right)\n",
    "$$\n",
    "where $\\kernelMatrix$ is the covariance of the process made up of elements taken from the covariance function, $\\kernelScalar(\\inputVector, t, \\dataVector; \\inputVector^\\prime, t^\\prime, \\dataVector^\\prime)$ so $q(\\vScalar)$ itself is Gaussian with covariance \n",
    "$$\n",
    "\\covarianceMatrix = \\left(\\beta\\mathbf{L} + \\kernelMatrix^{-1}\\right)^{-1}\n",
    "$$\n",
    "and mean\n",
    "$$\n",
    "\\meanTwoVector = \\beta\\covarianceMatrix\\mathbf{L}\\meanVector\n",
    "$$\n",
    "where $\\mathbf{L}$ is a matrix containing the loss functions, $L(\\dataScalar_i, \\mappingFunction(\\inputVector_i))$ along its diagonal elements with zeros elsewhere.\n",
    "\n",
    "The update is given by \n",
    "$$\n",
    "\\expectationDist{\\vScalar_i^2}{q(\\vScalar)} = \\meanTwoScalar_i^2 + \\covarianceScalar_{i, i}.\n",
    "$$\n",
    "\n",
    "To compare with before, if the mean of the measure $m(\\vScalar)$  was zero and the prior covariance was spherical, $\\kernelMatrix=\\lambda^{-1}\\eye$. Then this would equate to an update,\n",
    "$$\n",
    "\\expectationDist{\\vScalar_i^2}{q(\\vScalar)} = \\frac{1}{\\lambda + \\beta L_i}\n",
    "$$\n",
    "which is the same as we had before for the exponential prior over $\\scaleScalar$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in a an observation\n",
    "\n",
    "Now that we have defined a process over $\\vScalar$, we could define a region in which we're certain that we would like the weights to be high. For example, if we were looking to have a test point at location $\\inputVector_*$, we could update our measure to be a Gaussian process that is conditioned on the observation of $\\vScalar_*$ set appropriately at $\\inputScalar_*$. In this case we have,\n",
    "$$\n",
    "\\kernelMatrix^\\prime = \\kernelMatrix - \\kernelVector_*\\kernelScalar_{*,*}^{-1}\\kernelVector^\\top_* \n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\meanVector^\\prime = \\meanVector + \\kernelVector_*\\kernelScalar_{*,*}^{-1} (\\vScalar_*-\\meanScalar)\n",
    "$$\n",
    "where $\\kernelScalar_*$ is the vector computed through the covariance function between the training data $\\inputMatrix$ and the proposed point that we are conditioning the scale upon, $\\inputVector_*$ and $\\kernelScalar_{*,*}$ is the covariance function computed for $\\inputVector_*$. \n",
    "\n",
    "Now the updated mean and covariance can be used in the maximum entropy formulation as before.\n",
    "$$\n",
    "q(\\vScalar) \\propto \\prod_{i=1}^\\numData \\exp\\left(- \\beta \\vScalar_i^2 L(\\dataScalar_i, \\mappingFunction(\\inputVector_i)) \\right) \\exp\\left(-\\frac{1}{2}(\\vVector-\\meanVector^\\prime)^\\top \\left.\\kernelMatrix^\\prime\\right.^{-1} (\\vVector-\\meanVector^\\prime)\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We will consider the same data set as above. We first create a Gaussian process model for the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPL(mlai.GP):\n",
    "    def __init__(self, X, losses, kernel, beta=1.0, mu=0.0, X_star=None, v_star=None):\n",
    "        # Bring together locations\n",
    "        self.kernel = kernel\n",
    "        self.K = self.kernel.K(X)\n",
    "        self.mu = np.ones((X.shape[0],1))*mu\n",
    "        self.beta = beta\n",
    "        if X_star is not None:\n",
    "            kstar = kernel.K(X, X_star)\n",
    "            kstarstar = kernel.K(X_star, X_star)\n",
    "            kstarstarInv = np.linalg.inv(kstarstar)\n",
    "            kskssInv = np.dot(kstar, kstarstarInv)\n",
    "            self.K -= np.dot(kskssInv,kstar.T)\n",
    "            if v_star is not None:\n",
    "                self.mu = kskssInv*(v_star-self.mu)+self.mu\n",
    "                Xaug = np.vstack((X, X_star))\n",
    "            else:\n",
    "                raise ValueError(\"v_star should not be None when X_star is None\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLMLGP(BLML):\n",
    "    def __init__(self, X, y, basis=None, kernel=None, beta=1.0, mu=0.0, alpha=1.0, X_star=None, v_star=None):\n",
    "        BLML.__init__(self, X, y, basis=basis, alpha=alpha, beta=beta, lambd=None)\n",
    "        self.gp_model=GPL(self.X, self.losses, kernel=kernel, beta=beta, mu=mu, X_star=X_star, v_star=v_star)\n",
    "    def update_s(self):\n",
    "        \"\"\"Update the weights\"\"\"\n",
    "        self.gp_model.C = sp.linalg.inv(sp.linalg.inv(self.gp_model.K+np.eye(self.X.shape[0])*1e-6) + self.beta*np.diag(self.losses.flatten()))\n",
    "        self.gp_model.diagC = np.diag(self.gp_model.C)[:, np.newaxis]\n",
    "        self.gp_model.f = self.gp_model.beta*np.dot(np.dot(self.gp_model.C,np.diag(self.losses.flatten())),self.gp_model.mu) +self.gp_model.mu\n",
    "        \n",
    "        #f, v = self.gp_model.K self.gp_model.predict(self.X)\n",
    "        self.s = self.gp_model.f*self.gp_model.f + self.gp_model.diagC # + 1.0/(self.losses*self.gp_model.beta)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BLMLGP(x, y, \n",
    "           basis=basis, \n",
    "           kernel=mlai.kernel(mlai.eq_cov, lengthscale=20, variance=1.0),\n",
    "           mu=0.0,\n",
    "           beta=1.0, \n",
    "           alpha=1000,\n",
    "           X_star=np.asarray([[2020]]), \n",
    "           v_star=np.asarray([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test, f_var = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.cla()\n",
    "from matplotlib import rc, rcParams\n",
    "rcParams.update({'font.size': 22})\n",
    "rc('text', usetex=True)\n",
    "gp_tutorial.gpplot(x_test, f2_test, f2_test - 2*np.sqrt(f2_var), f2_test + 2*np.sqrt(f2_var), ax=ax, edgecol='r', fillcol='#CC3300')\n",
    "ax.plot(x, y, 'g.', markersize=10)\n",
    "ax.set_xlim(data_limits[0], data_limits[1])\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('pace min/km')\n",
    "_ = ax.set_ylim(2, 6)\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-gp-loss-bayes-linear-regression000.svg', transparent=True)\n",
    "gp_tutorial.gpplot(x_test, f_test, f_test - 2*np.sqrt(f_var), f_test + 2*np.sqrt(f_var), ax=ax, edgecol='b', fillcol='#0033CC')\n",
    "#ax.plot(x_test, f_test, linewidth=3, color='b')\n",
    "ax.plot(x, y, 'g.', markersize=10)\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x.flatten(), model.s.flatten(), width=2, color='b')\n",
    "ax2.set_ylim(0, 3)\n",
    "ax2.set_yticks([0, 0.5, 1])\n",
    "ax2.set_ylabel('$\\langle s_i \\\\rangle$')\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-gp-loss-bayes-linear-regression001.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_plots('olympic-gp-loss-bayes-linear-regression{number:0>3}.svg', \n",
    "                            directory='../slides/diagrams/ml', number=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Uncertainty\n",
    "\n",
    "Finally we make an attempt to show the joint uncertainty by first of all sampling from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "num_samps=10\n",
    "samps=np.random.multivariate_normal(model.gp_model.f.flatten(), model.gp_model.C, size=100).T**2\n",
    "ax.plot(x, samps, '-x', markersize=10, linewidth=2)\n",
    "ax.set_xlim(data_limits[0], data_limits[1])\n",
    "ax.set_xlabel('year')\n",
    "_ = ax.set_ylabel('$s_i$')\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-gp-loss-samples.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x, y, 'r.', markersize=10)\n",
    "ax.set_xlim(data_limits[0], data_limits[1])\n",
    "ax.set_ylim(2, 6)\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('pace min/km')\n",
    "gp_tutorial.gpplot(x_test, f_test, f_test - 2*np.sqrt(f_var), f_test + 2*np.sqrt(f_var), ax=ax, edgecol='b', fillcol='#0033CC')\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-gp-loss-bayes-linear-regression-and-samples000.svg', transparent=True)\n",
    "allsamps = []\n",
    "for i in range(samps.shape[1]):\n",
    "    model.s = samps[:, i:i+1]\n",
    "    model.update_w()\n",
    "    f_bar, f_cov =model.predict(x_test, full_cov=True)\n",
    "    f_samp = np.random.multivariate_normal(f_bar.flatten(), f_cov, size=10).T\n",
    "    ax.plot(x_test, f_samp, linewidth=0.5, color='k')\n",
    "    allsamps+=list(f_samp[-1, :])\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-gp-loss-bayes-linear-regression-and-samples001.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_plots('olympic-gp-loss-bayes-linear-regression-and-samples{number:0>3}.svg', \n",
    "                            directory='../slides/diagrams/ml', number=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.hist(np.asarray(allsamps), bins=30, density=True)\n",
    "ax.set_xlabel='pace min/kim'\n",
    "mlai.write_figure('../slides/diagrams/ml/olympic-gp-loss-histogram-2020.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
