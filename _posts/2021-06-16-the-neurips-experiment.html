---
title: "A Retrospective on the 2014 NeurIPS Experiment"
venue: "Computer Lab Seminar Series"
abstract: "<p>In 2014, along with Corinna Cortes, I was Program Chair of the Neural Information Processing Systems conference. At the time, when wondering about innovations for the coference, Corinna and I decided it would be interesting to test the consistency of reviewing. With this in mind, we randomly selected 10% of submissions and had them reviewed by two independent committees. In this talk I will review the construction of the experiment, explain how the NeurIPS review process worked and talk about what I felt the implications for reviewing were, vs what the community reaction was.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_neurips/the-neurips-experiment.md
date: 2021-06-16
published: 2021-06-16
week: 0
reveal: 2021-06-16-the-neurips-experiment.slides.html
ipynb: 2021-06-16-the-neurips-experiment.ipynb
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_neurips/the-neurips-experiment.md
layout: talk
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="introduction">Introduction</h1>
<p>The NIPS experiment was an experiment to determine the consistency of the review process. After receiving papers we selected 10% that would be independently rereviewed. The idea was to determine how consistent the decisions between the two sets of independent papers would be. In 2014 NIPS received 1678 submissions and we selected 170 for the experiment. These papers are referred to below as ‘duplicated papers.’</p>
<p>To run the experiment we created two separate committees within the NIPS program committee. The idea was that the two separate committees would review each duplicated paper independently and results compared.</p>
<h2 id="neurips-in-numbers">NeurIPS in Numbers</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-in-numbers.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-in-numbers.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In 2014 the NeurIPS conference had 1474 active reviewers (up from 1133 in 2013), 92 area chairs (up from 67) and two program chairs, myself and Corinna Cortes.</p>
<p>The conference received 1678 submissions and presented 414 accepted papers, of which 20 were presented as talks in the single track session, 62 were presented as spotlights and 331 papers were presented as posters. Of the 1678 submissions, 19 papers were rejected without review.</p>
<h2 id="the-neurips-experiment">The NeurIPS Experiment</h2>
<p>The objective of the NeurIPS experiment was to determine how consistent was the process of peer review? What would happen to the papers in the conference if the process was independently rerun?</p>
<p>To explore this question, we selected <span class="math inline">\(\approx 10\%\)</span> of submitted papers to be reviewed twice, by independent committees.</p>
<p>We randomly selected 170 papers from the total submitted for this treatment. For these papers the program committee was divided into two. Reviewers were placed randomly on one side of the committee or the other. For Program Chairs we also engaged in some manual selection to ensure we had expert coverage in all the conference areas on both side of the committee.</p>
<h2 id="notes-on-the-timeline-for-neurips">Notes on the Timeline for NeurIPS</h2>
<p><strong>AC recruitment (3 waves):</strong> * 17/02/2014 * 08/03/2014 * 09/04/2014</p>
<p><strong>We requested names of reviewers from ACs in two waves:</strong> * 25/03/2014 * 11/04/2014</p>
<p><strong>Reviewer recruitment (4 waves):</strong> * 14/04/2014 * 28/04/2014 * 09/05/2014 * 10/06/2014 (note this is after deadline … lots of area chairs asked for reviewers after the deadline!). We invited them en-masse.</p>
<ul>
<li>06/06/2014 Submission Deadline</li>
<li>12/06/2014 Bidding Open For Area Chairs (this was <em>delayed</em> by CMT issues)</li>
<li>17/06/2014 Bidding Open For Reviewers</li>
<li>01/07/2014 Start Reviewing</li>
<li>21/07/2014 Reviewing deadline</li>
<li>04/08/2014 Reviews to Authors</li>
<li>11/08/2014 Author Rebuttal Due</li>
<li>25/08/2014 Teleconferences Begin</li>
<li>30/08/2014 Teleconferences End</li>
<li>1/09/2014 Preliminary Decisions Made</li>
<li>9/09/2014 Decisions Sent to Authors</li>
</ul>
<h2 id="decision-making-timeline">Decision Making Timeline</h2>
<ul>
<li>Submission deadline 6th June</li>
</ul>
<ol type="1">
<li>three weeks for paper <em>bidding</em> and allocation</li>
<li>three weeks for <em>review</em></li>
<li>two weeks for discussion and <em>adding/augmenting</em> reviews/reviewers</li>
<li>one week for <em>author rebuttal</em></li>
<li>two weeks for <em>discussion</em></li>
<li>one week for <em>teleconferences</em> and <em>final decisons</em></li>
<li>one week cooling off</li>
</ol>
<ul>
<li>Decisions sent 9th September</li>
</ul>
<h2 id="paper-scoring-and-reviewer-instructions">Paper Scoring and Reviewer Instructions</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/paper-scoring.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/paper-scoring.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The instructions to reviewers for the 2014 conference are still available <a href="https://nips.cc/Conferences/2014/PaperInformation/ReviewerInstructions">online here</a>.</p>
<p>To keep quality of reviews high, we tried to keep load low. We didn’t assign any reviewer more than 5 papers, most reviewers received 4 papers.</p>
<h2 id="quantitative-evaluation">Quantitative Evaluation</h2>
<p>Reviewers give a score of between 1 and 10 for each paper. The program committee will interpret the numerical score in the following way:</p>
<ul>
<li><p>10: Top 5% of accepted NIPS papers, a seminal paper for the ages.</p>
<p>I will consider not reviewing for NIPS again if this is rejected.</p></li>
<li><p>9: Top 15% of accepted NIPS papers, an excellent paper, a strong accept.</p>
<p>I will fight for acceptance.</p></li>
<li><p>8: Top 50% of accepted NIPS papers, a very good paper, a clear accept.</p>
<p>I vote and argue for acceptance.</p></li>
<li><p>7: Good paper, accept.</p>
<p>I vote for acceptance, although would not be upset if it were rejected.</p></li>
<li><p>6: Marginally above the acceptance threshold.</p>
<p>I tend to vote for accepting it, but leaving it out of the program would be no great loss.</p></li>
<li><p>5: Marginally below the acceptance threshold.</p>
<p>I tend to vote for rejecting it, but having it in the program would not be that bad.</p></li>
<li><p>4: An OK paper, but not good enough. A rejection.</p>
<p>I vote for rejecting it, although would not be upset if it were accepted.</p></li>
<li><p>3: A clear rejection.</p>
<p>I vote and argue for rejection.</p></li>
<li><p>2: A strong rejection. I’m surprised it was submitted to this conference.</p>
<p>I will fight for rejection.</p></li>
<li><p>1: Trivial or wrong or known. I’m surprised anybody wrote such a paper.</p>
<p>I will consider not reviewing for NIPS again if this is accepted.</p></li>
</ul>
<p>Reviewers should NOT assume that they have received an unbiased sample of papers, nor should they adjust their scores to achieve an artificial balance of high and low scores. Scores should reflect absolute judgments of the contributions made by each paper.</p>
<h2 id="impact-score">Impact Score</h2>
<p>Independently of the Quality Score above, this is your opportunity to identify papers that are very different, original, or otherwise potentially impactful for the NIPS community.</p>
<p>There are two choices:</p>
<p>2: This work is different enough from typical submissions to potentially have a major impact on a subset of the NIPS community.</p>
<p>1: This work is incremental and unlikely to have much impact even though it may be technically correct and well executed.</p>
<p>Examples of situations where the impact and quality scores may point in opposite directions include papers which are technically strong but unlikely to generate much follow-up research, or papers that have some flaw (e.g. not enough evaluation, not citing the right literature) but could lead to new directions of research.</p>
<h2 id="confidence-score">Confidence Score</h2>
<p>Reviewers also give a confidence score between 1 and 5 for each paper. The program committee will interpret the numerical score in the following way:</p>
<p>5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature.</p>
<p>4: The reviewer is confident but not absolutely certain that the evaluation is correct. It is unlikely but conceivable that the reviewer did not understand certain parts of the paper, or that the reviewer was unfamiliar with a piece of relevant literature.</p>
<p>3: The reviewer is fairly confident that the evaluation is correct. It is possible that the reviewer did not understand certain parts of the paper, or that the reviewer was unfamiliar with a piece of relevant literature. Mathematics and other details were not carefully checked.</p>
<p>2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper.</p>
<p>1: The reviewer’s evaluation is an educated guess. Either the paper is not in the reviewer’s area, or it was extremely difficult to understand.</p>
<h2 id="qualitative-evaluation">Qualitative Evaluation</h2>
<p>All NIPS papers should be good scientific papers, regardless of their specific area. We judge whether a paper is good using four criteria; a reviewer should comment on all of these, if possible:</p>
<ul>
<li><p>Quality</p>
<p>Is the paper technically sound? Are claims well-supported by theoretical analysis or experimental results? Is this a complete piece of work, or merely a position paper? Are the authors careful (and honest) about evaluating both the strengths and weaknesses of the work?</p></li>
<li><p>Clarity</p>
<p>Is the paper clearly written? Is it well-organized? (If not, feel free to make suggestions to improve the manuscript.) Does it adequately inform the reader? (A superbly written paper provides enough information for the expert reader to reproduce its results.)</p></li>
<li><p>Originality</p>
<p>Are the problems or approaches new? Is this a novel combination of familiar techniques? Is it clear how this work differs from previous contributions? Is related work adequately referenced? We recommend that you check the proceedings of recent NIPS conferences to make sure that each paper is significantly different from papers in previous proceedings. Abstracts and links to many of the previous NIPS papers are available from http://books.nips.cc</p></li>
<li><p>Significance</p></li>
</ul>
<p>Are the results important? Are other people (practitioners or researchers) likely to use these ideas or build on them? Does the paper address a difficult problem in a better way than previous research? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions on existing data, or a unique theoretical or pragmatic approach?</p>
<h2 id="speculation">Speculation</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-speculation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-speculation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>With the help of <a href="http://nicolofusi.com/">Nicolo Fusi</a>, <a href="http://blog.scicast.org/tag/charles-twardy/">Charles Twardy</a> and the entire Scicast team we launched <a href="https://scicast.org/#!/questions/1083/trades/create/power">a Scicast question</a> a week before the results were revealed. The comment thread for that question already had <a href="https://scicast.org/#!/questions/1083/comments/power">an amount of interesting comment</a> before the conference. Just for informational purposes before we began reviewing Corinna forecast this figure would be 25% and I forecast it would be 20%. The box plot summary of predictions from Scicast is below.</p>
<div class="figure">
<div id="scicast-forecast-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/slides/diagrams//neurips/scicast-forecast.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="scicast-forecast-magnify" class="magnify" onclick="magnifyFigure(&#39;scicast-forecast&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="scicast-forecast-caption" class="caption-frame">
<p>Figure: Summary forecast from those that responded to the scicast question about how consistent the decision making was.</p>
</div>
</div>
<h2 id="neurips-experiment-results">NeurIPS Experiment Results</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-results.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-results.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The final results of the experiment were as follows. From 170 papers 4 had to be withdrawn or were rejected without completing the review process, for the remainder, the ‘confusion matrix’ for the two committee’s decisions is below.</p>
<table>
<tr>
<td colspan="2">
</td>
<td colspan="2">
Committee 1
</td>
</tr>
<tr>
<td colspan="2">
</td>
<td>
Accept
</td>
<td>
Reject
</td>
</tr>
<tr>
<td rowspan="2">
Committee 2
</td>
<td>
Accept
</td>
<td>
22
</td>
<td>
22
</td>
</tr>
<tr>
<td>
Reject
</td>
<td>
21
</td>
<td>
101
</td>
</tr>
</table>
<p>4 papers rejected or withdrawn without review.</p>
<h2 id="summarizing-the-table">Summarizing the Table</h2>
<p>There are a few ways of summarizing the numbers in this table as percent or probabilities. First of all, the inconsistency, the proportion of decisions that were not the same across the two committees. The decisions were inconsistent for 43 out of 166 papers or 0.259 as a proportion. This number is perhaps a natural way of summarizing the figures if you are submitting your paper and wish to know an estimate of what the probability is that your paper would have different decisons according to the different committes. Secondly, the accept precision: if you are attending the conference and looking at any given paper, then you might want to know the probability that the paper would have been rejected in an independent rerunning of the conference. We can estimate this for Committee 1’s conference as 22/(22 + 22) = 0.5 (50%) and for Committee 2’s conference as 21/(22+21) = 0.49 (49%). Averaging the two estimates gives us 49.5%. Finally, the reject precision: if your paper was rejected from the conference, you might like an estimate of the probability that the same paper would be rejected again if the review process had been independently rerun. That estimate is 101/(22+101) = 0.82 (82%) for Committee 1 and 101/(21+101)=0.83 (83%) for Committee 2, or on average 82.5%. A final quality estimate might be the ratio of consistent accepts to consistent rejects, or the agreed accept rate, 22/123 = 0.18 (18%).</p>
<ul>
<li><em>inconsistency</em>: 43/166 = <strong>0.259</strong>
<ul>
<li>proportion of decisions that were not the same</li>
</ul></li>
<li><em>accept precision</em> <span class="math inline">\(0.5 \times 22/44\)</span> + <span class="math inline">\(0.5 \times 21/43\)</span> = <strong>0.495</strong>
<ul>
<li>probability any accepted paper would be rejected in a rerunning</li>
</ul></li>
<li><em>reject precision</em> = <span class="math inline">\(0.5\times 101/(22+101)\)</span> + <span class="math inline">\(0.5\times 101/(21 + 101)\)</span> = <strong>0.175</strong>
<ul>
<li>probability any rejected paper would be rejected in a rerunning</li>
</ul></li>
<li><em>agreed accept rate</em> = 22/101 = <strong>0.218</strong></li>
<li>ratio between aggreed accepted papers and agreed rejected papers.</li>
</ul>
<h2 id="reaction-after-experiment">Reaction After Experiment</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-reaction.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-reaction.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>There seems to have been a lot of discussion of the result, both at the conference and on bulletin boards since. Such discussion is to be encouraged, and for ease of memory, it is worth pointing out that the approximate proportions of papers in each category can be nicely divided in to eigths as follows. Accept-Accept 1 in 8 papers, Accept-Reject 3 in 8 papers, Reject-Reject, 5 in 8 papers. This makes the statistics we’ve computed above: inconsistency 1 in 4 (25%) accept precision 1 in 2 (50%) reject precision 5 in 6 (83%) and agreed accept rate of 1 in 6 (20%). This compares with the accept rate of 1 in 4.</p>
<ul>
<li><p>Public reaction after experiment <a href="http://inverseprobability.com/2015/01/16/blogs-on-the-nips-experiment/">documented here</a></p></li>
<li><p><a href="http://inverseprobability.com/2014/07/01/open-data-science/">Open Data Science</a> (see Heidelberg Meeting)</p></li>
<li><p>NIPS was run in a very open way. <a href="https://github.com/sods/conference">Code</a> and <a href="http://inverseprobability.com/2014/12/16/the-nips-experiment/">blog posts</a> all available!</p></li>
<li><p>Reaction triggered by <a href="http://blog.mrtz.org/2014/12/15/the-nips-experiment.html">this blog post</a>.</p></li>
</ul>
<p>Much of the discussion speculates on the number of consistent accepts in the process (using the main conference accept rate as a proxy). It therefore produces numbers that don’t match ours above. This is because the computed accept rate of the individual committees is different from that of the main conference. This could be due to a bias for the duplicated papers, or statistical sampling error. We look at these questions below. First, to get the reader primed for thinking about these numbers we discuss some context for placing these numbers.</p>
<h2 id="a-random-committee-25">A Random Committee @ 25%</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-random-committee.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-random-committee.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The first context we can place around the numbers is what would have happened at the ‘Random Conference’ where we simply accept a quarter of papers at random. In this NIPS the expected numbers of accepts would then have been:</p>
<table>
<tr>
<td colspan="2">
</td>
<td colspan="2">
Committee 1
</td>
</tr>
<tr>
<td colspan="2">
</td>
<td>
Accept
</td>
<td>
Reject
</td>
</tr>
<tr>
<td rowspan="2">
Committee 2
</td>
<td>
Accept
</td>
<td>
10.4 (1 in 16)
</td>
<td>
31.1 (3 in 16)
</td>
</tr>
<tr>
<td>
Reject
</td>
<td>
31.1 (3 in 16)
</td>
<td>
93.4 (9 in 16)
</td>
</tr>
</table>
<p>And for this set up we would expect <em>inconsistency</em> of 3 in 8 (37.5%) <em>accept precision</em> of 1 in 4 (25%) and a <em>reject precision</em> of 3 in 4 (75%) and a <em>agreed accept rate</em> of 1 in 10 (10%). The actual committee made improvements on these numbers, in particular the accept precision was markedly better with 50%: twice as many consistent accept decisions were made than would be expected if the process had been performed at random and only around two thirds as many inconsistent decisions were made as would have been expected if decisions were made at random. However, we should treat all these figures with some skepticism until we’ve performed some estimate of the uncertainty associated with them.</p>
<h2 id="stats-for-random-committee">Stats for Random Committee</h2>
<ul>
<li>For random committee we expect:
<ul>
<li><em>inconsistency</em> of 3 in 8 (37.5%)</li>
<li><em>accept precision</em> of 1 in 4 (25%)</li>
<li><em>reject precision</em> of 3 in 4 (75%) and a</li>
<li><em>agreed accept rate</em> of 1 in 10 (10%).</li>
</ul></li>
</ul>
<p>Actual committee’s accept precision markedly better with 50% accept precision.</p>
<h2 id="uncertainty-accept-rate">Uncertainty: Accept Rate</h2>
<p>To get a handle on the uncertainty around these numbers we’ll start by making use of the (binomial distribution)[http://en.wikipedia.org/wiki/Binomial_distribution]. First, let’s explore the fact that for the overall conference the accept rate was around 23%, but for the duplication committees the accept rate was around 25%. If we assume decisions are made according to a binomial distribution, then is the accept rate for the duplicated papers too high?</p>
<p>Note that for all our accept probability statistics we used as a denominator the number of papers that were initially sent for review, rather than the number where a final decision was made by the program committee. These numbers are different because some papers are withdrawn before the program committee makes its decision. Most commonly this occurs after authors have seen their preliminary reviews: for NIPS 2014 we provided preliminary reviews that included paper scores. So for the official accept probability we use the 170 as denominator. The accept probabilities were therefore 43 out of 170 papers (25.3%) for Committee 1 and 44 out of 170 (25.8%) for Committee 2. This compares with the overall conference accept rate for papers outside the duplication process of 349 out of 1508 (23.1%).</p>
<p>If the true underlying probability of an accept were actually 0.23 independent of the paper, then the probability of generating accepts for any subset of the papers would be given by a binomial distribution. Combining across the two committees for the duplicated papers, we see that 87 papers in total were recommended for accept out of a total of 340 trials. out of 166 trials would be given by a binomial distribution as depicted below.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span></code></pre></div>
<div class="figure">
<div id="uncertainty-accept-rate-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/uncertainty-accept-rate.svg" width="70%" style=" ">
</object>
</div>
<div id="uncertainty-accept-rate-magnify" class="magnify" onclick="magnifyFigure(&#39;uncertainty-accept-rate&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uncertainty-accept-rate-caption" class="caption-frame">
<p>Figure: Number of accepted papers for <span class="math inline">\(p=0.23\)</span>.</p>
</div>
</div>
<p>From the plot, we can see that whilst the accept rate was slightly higher for duplicated papers it doesn’t seem that we can say that it was statistically significant that it was higher, it falls well within the probability mass of the Binomial.</p>
<p>Note that Area Chairs knew which papers were duplicates, whereas reviewers did not. Whilst we stipulated that duplicate papers should not be any given special treatment, we cannot discount the possibility that Area Chairs may have given slightly preferential treatment to duplicate papers.</p>
<h2 id="uncertainty-accept-precision">Uncertainty: Accept Precision</h2>
<p>For the accept precision, if we assume that accept decisions were drawn according to a binomial, then the distribution for consistent accepts is also binomial. Our best estimate of its parameter is 22/166 = 0.13 (13%). If we had a binomial distribution with these parameters, then the distribution of consistent accepts would be as follows.</p>
<ul>
<li>How reliable is the consistent accept score?</li>
</ul>
<div class="figure">
<div id="uncertainty-accept-precision-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/uncertainty-accept-rate.svg" width="70%" style=" ">
</object>
</div>
<div id="uncertainty-accept-precision-magnify" class="magnify" onclick="magnifyFigure(&#39;uncertainty-accept-precision&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uncertainty-accept-precision-caption" class="caption-frame">
<p>Figure: Number of consistent accepts given <span class="math inline">\(p=0.13\)</span>.</p>
</div>
</div>
<p>We see immediately that there is a lot of uncertainty around this number, for the scale of the experiment as we have it. This suggests a more complex analysis is required to extract our estimates with uncertainty.</p>
<h2 id="bayesian-analysis">Bayesian Analysis</h2>
<p>Before we start the analysis, it’s important to make some statements about the aims of our modelling here. We will make some simplifying modelling assumptions for the sake of a model that is understandable. In particular, we are looking to get a handle on the uncertainty associated with some of the probabilities associated with the NIPS experiment. <a href="http://inverseprobability.com/2015/01/16/blogs-on-the-nips-experiment/">Some preliminary analyses have already been conducted on blogs</a>. Those analyses don’t have access to information like paper scores etc. For that reason we also leave out such information in this preliminary analysis. We will focus only on the summary results from the experiment: how many papers were consistently accepted, consistently rejected or had inconsistent decisions. For the moment we disregard the information we have about paper scores.</p>
<p>In our analysis there are three possible outcomes for each paper: consistent accept, inconsistent decision and consistent reject. So we need to perform the analysis with the <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a>. The multinomial is parameterized by the probabilities of the different outcomes. These are our parameters of interest, we would like to estimate these probabilities alongside their uncertainties. To make a Bayesian analysis we place a prior density over these probabilities, then we update the prior with the observed data, that gives us a posterior density, giving us an uncertainty associated with these probabilities.</p>
<h3 id="prior-density">Prior Density</h3>
<p>Choice of prior for the multinomial is typically straightforward, the <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet density</a> is <a href="http://en.wikipedia.org/wiki/Conjugate_prior">conjugate</a> and has the additional advantage that its parameters can be set to ensure it is <em>uninformative</em>, i.e. uniform across the domain of the prior. Combination of a multinomial likelihood and a Dirichelt prior is not new, and in this domain if we were to consider the mean the posterior density only, then the approach is known as <a href="http://en.wikipedia.org/wiki/Additive_smoothing">Laplace smoothing</a>.</p>
<p>For our model we are assuming for our prior that the probabilities are drawn from a Dirichlet as follows, <span class="math display">\[
p \sim \text{Dir}(\alpha_1, \alpha_2, \alpha_3),
\]</span> with <span class="math inline">\(\alpha_1=\alpha_2=\alpha_3=1\)</span>. The Dirichlet density is conjugate to the <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a>, and we associate three different outcomes with the multinomial. For each of the 166 papers we expect to have a consistent accept (outcome 1), an inconsistent decision (outcome 2) or a consistent reject (outcome 3). If the counts four outcome 1, 2 and 3 are represented by <span class="math inline">\(k_1\)</span>, <span class="math inline">\(k_2\)</span> and <span class="math inline">\(k_3\)</span> and the associated probabilities are given by <span class="math inline">\(p_1\)</span>, <span class="math inline">\(p_2\)</span> and <span class="math inline">\(p_3\)</span> then our model is, <span class="math display">\[\begin{align*}
\mathbf{p}|\boldsymbol{\alpha} \sim \text{Dir}(\boldsymbol{\alpha}) \\
\mathbf{k}|\mathbf{p} \sim \text{mult}(\mathbf{p}).
\end{align*}\]</span> Due to the conjugacy the posterior is tractable and easily computed as a Dirichlet (see e.g. <a href="http://www.stat.columbia.edu/~gelman/book/">Gelman et al</a>), where the parameters of the Dirichlet are given by the original vector from the Dirichlet prior plus the counts associated with each outcome. <span class="math display">\[
\mathbf{p}|\mathbf{k}, \boldsymbol{\alpha} \sim \text{Dir}(\boldsymbol{\alpha} + \mathbf{k})
\]</span> The mean probability for each outcome is then given by, <span class="math display">\[
\bar{p}_i = \frac{\alpha_i+k_i}{\sum_{j=1}^3(\alpha_j + k_j)}.
\]</span> and the variance is <span class="math display">\[
\mathrm{Var}[p_i] = \frac{(\alpha_i+k_i) (\alpha_0-\alpha_i + n + k_i)}{(\alpha_0+n)^2 (\alpha_0+n+1)},
\]</span> where <span class="math inline">\(n\)</span> is the number of trials (166 in our case) and <span class="math inline">\(\alpha_0 = \sum_{i=1}^3\alpha_i\)</span>. This allows us to compute the expected value of the probabilities and their variances under the posterior as follows.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior_mean_var(k, alpha):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Compute the mean and variance of the Dirichlet posterior.&quot;&quot;&quot;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    alpha_0 <span class="op">=</span> alpha.<span class="bu">sum</span>()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> k.<span class="bu">sum</span>()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> (k <span class="op">+</span> alpha)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    m <span class="op">/=</span> m.<span class="bu">sum</span>()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> (alpha<span class="op">+</span>k)<span class="op">*</span>(alpha_0 <span class="op">-</span> alpha <span class="op">+</span> n <span class="op">+</span> k)<span class="op">/</span>((alpha_0<span class="op">+</span>n)<span class="op">**</span><span class="dv">2</span><span class="op">*</span>(alpha_0<span class="op">+</span>n<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> m, v</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.asarray([<span class="dv">22</span>, <span class="dv">43</span>, <span class="dv">101</span>])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.ones((<span class="dv">3</span>,))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>m, v <span class="op">=</span> posterior_mean_var(k, alpha)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>outcome <span class="op">=</span> [<span class="st">&#39;consistent accept&#39;</span>, <span class="st">&#39;inconsistent decision&#39;</span>, <span class="st">&#39;consistent reject&#39;</span>]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    display(HTML(<span class="st">&quot;&lt;h4&gt;Probability of &quot;</span> <span class="op">+</span> outcome[i] <span class="op">+</span><span class="st">&#39; &#39;</span> <span class="op">+</span> <span class="bu">str</span>(m[i]) <span class="op">+</span>  <span class="st">&quot;+/-&quot;</span> <span class="op">+</span> <span class="bu">str</span>(<span class="dv">2</span><span class="op">*</span>np.sqrt(v[i])) <span class="op">+</span> <span class="st">&quot;&lt;/h4&gt;&quot;</span>))</span></code></pre></div>
<p>So we have a probability of consistent accept as <span class="math inline">\(0.136 \pm 0.06\)</span>, the probability of inconsistent decision as <span class="math inline">\(0.260 \pm 0.09\)</span> and probability of consistent reject as <span class="math inline">\(0.60 \pm 0.15\)</span>. Recall that if we’d selected papers at random (with accept rate of 1 in 4) then these values would have been 1 in 16 (0.0625), 3 in 8 (0.375) and 9 in 16 (0.5625).</p>
<p>The other values we are interested in are the accept precision, reject precision and the agreed accept rate. Computing the probability density for these statistics is complex, because it involves <a href="http://en.wikipedia.org/wiki/Ratio_distribution">Ratio Distributions</a>. However, we can use Monte Carlo to estimate the expected accept precision, reject precision and agreed accept rate as well as their variances. We can use these results to give us error bars and histograms of these statistics.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_precisions(k, alpha, num_samps):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Helper function to sample from the posterior distibution of accept, </span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    reject and inconsistent probabilities and compute other statistics of interest </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    from the samples.&quot;&quot;&quot;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> np.random.dirichlet(k<span class="op">+</span>alpha, size<span class="op">=</span>num_samps)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Factors of 2 appear because inconsistent decisions </span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># are being accounted for across both committees.</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    ap <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>k[:, <span class="dv">0</span>]<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>k[:, <span class="dv">0</span>]<span class="op">+</span>k[:, <span class="dv">1</span>])</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    rp <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>k[:, <span class="dv">2</span>]<span class="op">/</span>(k[:, <span class="dv">1</span>]<span class="op">+</span><span class="dv">2</span><span class="op">*</span>k[:, <span class="dv">2</span>])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    aa <span class="op">=</span> k[:, <span class="dv">0</span>]<span class="op">/</span>(k[:, <span class="dv">0</span>]<span class="op">+</span>k[:, <span class="dv">2</span>])</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ap, rp, aa</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>ap, rp, aa <span class="op">=</span> sample_precisions(k, alpha, <span class="dv">10000</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ap.mean(), <span class="st">&#39;+/-&#39;</span>, <span class="dv">2</span><span class="op">*</span>np.sqrt(ap.var()))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rp.mean(), <span class="st">&#39;+/-&#39;</span>, <span class="dv">2</span><span class="op">*</span>np.sqrt(rp.var()))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(aa.mean(), <span class="st">&#39;+/-&#39;</span>, <span class="dv">2</span><span class="op">*</span>np.sqrt(aa.var()))</span></code></pre></div>
<p>Giving an accept precision of <span class="math inline">\(0.51 \pm 0.13\)</span>, a reject precision of <span class="math inline">\(0.82 \pm 0.05\)</span> and an agreed accept rate of <span class="math inline">\(0.18 \pm 0.07\)</span>. Note that the ‘random conference’ values of 1 in 4 for accept precision and 3 in 4 for reject decisions are outside the two standard deviation error bars. If it is preferred medians and percentiles could also be computed from the samples above, but as we will see when we histogram the results the densities look broadly symmetric, so this is unlikely to have much effect.</p>
<h3 id="histogram-of-monte-carlo-results">Histogram of Monte Carlo Results</h3>
<p>Just to ensure that the error bars are reflective of the underlying densities we histogram the Monte Carlo results for accept precision, reject precision and agreed accept below. Shown on each histogram is a line representing the result we would get for the ‘random committee.’</p>
<div class="figure">
<div id="random-committee-outcomes-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/random-committee-outcomes-vs-true.svg" width="90%" style=" ">
</object>
</div>
<div id="random-committee-outcomes-magnify" class="magnify" onclick="magnifyFigure(&#39;random-committee-outcomes&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="random-committee-outcomes-caption" class="caption-frame">
<p>Figure: Different statistics for the random committee oucomes versus the observed committee outcomes.</p>
</div>
</div>
<h3 id="model-choice-and-prior-values">Model Choice and Prior Values</h3>
<p>In the analysis above we’ve minimized the modeling choices: we made use of a Bayesian analysis to capture the uncertainty in counts that can be arising from statistical sampling error. To this end we chose an uninformative prior over these probabilities. However, one might argue that the prior should reflect something more about the underlying experimental structure: for example we <em>know</em> that if the committees made their decisions independently it is unlikely that we’d obtain an inconsistency figure much greater than 37.5% because that would require committees to explicitly collude to make inconsistent decisions: the random conference is the worst case. Due to the accept rate, we also expect a larger number of reject decisions than reject. This also isn’t captured in our prior. Such questions actually move us into the realms of modeling the process, rather then performing a sensitivity analysis. However, if we wish to model the decision process as a whole we have a lot more information available, and we should make use of it. The analysis above is intended to exploit our randomized experiment to explore how inconsistent we expect two committees to be. It focusses on that single question, it doesn’t attempt to give answers on what the reasons for that inconsistency are and how it may be reduced. The additional maths was needed only to give a sense of the uncertainty in the figures. That uncertainty arises due to the limited number of papers in the experiment.</p>
<!--include{_neurips/includes/neurips-experiment.md}-->
<h2 id="reviewer-calibration">Reviewer Calibration</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-reviewer-calibration.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-reviewer-calibration.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="reviewer-calibration-model">Reviewer Calibration Model</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/reviewer-calibration-model.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/reviewer-calibration-model.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In this note book we deal with reviewer calibration. Our assumption is that the score from the <span class="math inline">\(j\)</span>th reviwer for the <span class="math inline">\(i\)</span>th paper is given by <span class="math display">\[
y_{i,j} = f_i + b_j + \epsilon_{i, j}
\]</span> where <span class="math inline">\(f_i\)</span> is the ‘objective quality’ of paper <span class="math inline">\(i\)</span> and <span class="math inline">\(b_j\)</span> is an offset associated with reviewer <span class="math inline">\(j\)</span>. <span class="math inline">\(\epsilon_{i,j}\)</span> is a subjective quality estimate which reflects how a specific reviewer’s opinion differs from other reviewers (such differences in opinion may be due to differing expertise or perspective). The underlying ‘objective quality’ of the paper is assumed to be the same for all reviewers and the reviewer offset is assumed to be the same for all papers.</p>
<p>If we have <span class="math inline">\(n\)</span> papers and <span class="math inline">\(m\)</span> reviewers then this implies <span class="math inline">\(n\)</span> + <span class="math inline">\(m\)</span> + <span class="math inline">\(nm\)</span> values need to be estimated. Naturally this is too many, and we can start by assuming that the subjective quality is drawn from a normal density with variance <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
\epsilon_{i, j} \sim N(0, \sigma^2 \mathbf{I})
\]</span> which reduces us to <span class="math inline">\(n\)</span> + <span class="math inline">\(m\)</span> + 1 parameters. Further we can assume that the objective quality is also normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\alpha_f\)</span>, <span class="math display">\[
f_i \sim N(\mu, \alpha_f)
\]</span> this now reduces us to <span class="math inline">\(m\)</span>+3 parameters. However, we only have approximately <span class="math inline">\(4m\)</span> observations (4 papers per reviewer) so parameters may still not be that well determined (particularly for those reviewers that have only one review). We therefore, finally, assume that reviewer offset is normally distributed with zero mean, <span class="math display">\[
b_j \sim N(0, \alpha_b),
\]</span> leaving us only four parameters: <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\alpha_f\)</span> and <span class="math inline">\(\alpha_b\)</span>. Combined together these three assumptions imply that <span class="math display">\[
\mathbf{y} \sim N(\mu \mathbf{1}, \mathbf{K})
\]</span> where <span class="math inline">\(\mathbf{y}\)</span> is a vector of stacked scores <span class="math inline">\(\mathbf{1}\)</span> is the vector of ones and the elements of the covariance function are given by <span class="math display">\[
k(i,j; k,l) = \delta_{i,k} \alpha_f + \delta_{j,l} \alpha_b + \delta_{i, k}\delta_{j,l} \sigma^2
\]</span> where <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are the index of first paper and reviewer and <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> are the index of second paper and reviewer. The mean is easily estimated by maximum likelihood, and is given as the mean of all scores.</p>
<p>It is convenient to reparameterize slightly into an overall scale <span class="math inline">\(\alpha_f\)</span>, and normalized variance parameters, <span class="math display">\[
k(i,j; k,l) = \alpha_f(\delta_{i,k}  + \delta_{j,l} \frac{\alpha_b}{\alpha_f} + \delta_{i, k}\delta_{j,l} \frac{\sigma^2}{\alpha_f})
\]</span> which we rewrite to give two ratios: offset/signal ratio, <span class="math inline">\(\hat{\alpha}_b\)</span> and noise/signal <span class="math inline">\(\hat{\sigma}^2\)</span> ratio. <span class="math display">\[
k(i,j; k,l) = \alpha_f(\delta_{i,k}  + \delta_{j,l} \hat{\alpha}_b + \delta_{i, k}\delta_{j,l} \hat{\sigma}^2)
\]</span> The advantage of this parameterization is it allows us to optimize <span class="math inline">\(\alpha_f\)</span> directly (with a fixed point equation) and it will be very well determined. This leaves us with two free parameters, that we can explore on the grid. It is in these parameters that we expect the remaining underdetermindness of the model. We expect <span class="math inline">\(\alpha_f\)</span> to be well determined because the negative log likelihood is now <span class="math display">\[
\frac{|\mathbf{y}|}{2}\log\alpha_f + \frac{1}{2}\log  \left|\hat{\mathbf{K}}\right| + \frac{1}{2\alpha_f}\mathbf{y}^\top \hat{\mathbf{K}}^{-1} \mathbf{y}
\]</span> where <span class="math inline">\(|\mathbf{y}|\)</span> is the length of <span class="math inline">\(\mathbf{y}\)</span> (i.e. the number of reviews) and <span class="math inline">\(\hat{\mathbf{K}}=\alpha_f^{-1}\mathbf{K}\)</span> is the scale normalised covariance. This negative log likelihood is easily minimized to recover <span class="math display">\[
\alpha_f = \frac{1}{|\mathbf{y}|} \mathbf{y}^\top \hat{\mathbf{K}}^{-1} \mathbf{y}
\]</span> A Bayesian analysis of this parameter is possible with gamma priors, but it would merely shows that this parameter is extremely well determined (the degrees of freedom parameter of the associated Student-<span class="math inline">\(t\)</span> marginal likelihood scales will the number of reviews, which will be around <span class="math inline">\(|\mathbf{y}| \approx 6,000\)</span> in our case.</p>
<p>So, we propose to proceed as follows. Set the mean from the reviews (<span class="math inline">\(\mu\)</span>) and then choose a two dimensional grid of parameters for reviewer offset and diversity. For each parameter choice, optimize to find <span class="math inline">\(\alpha_f\)</span> and then evaluate the liklihood. Worst case this will require us inverting <span class="math inline">\(\hat{\mathbf{K}}\)</span>, but if the reviewer paper groups are disconnected, it can be done a lot quicker. Next stage is to load in the reviews for analysis.</p>
<h2 id="fitting-the-model">Fitting the Model</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/reviewer-calibration-fit-model.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/reviewer-calibration-fit-model.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cmtutils <span class="im">as</span> cu</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> GPy</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse.csgraph <span class="im">import</span> connected_components</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> solve_triangular </span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>date <span class="op">=</span> <span class="st">&#39;2014-09-06&#39;</span></span></code></pre></div>
<h2 id="loading-in-the-data">Loading in the Data</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>filename <span class="op">=</span> date <span class="op">+</span> <span class="st">&#39;_reviews.xls&#39;</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>reviews <span class="op">=</span> cu.CMT_Reviews_read(filename<span class="op">=</span>filename)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>papers <span class="op">=</span> <span class="bu">list</span>(<span class="bu">sorted</span>(<span class="bu">set</span>(reviews.reviews.index), key<span class="op">=</span><span class="bu">int</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>reviews.reviews <span class="op">=</span> reviews.reviews.loc[papers]</span></code></pre></div>
<p>The maximum likelihood solution for <span class="math inline">\(\mu\)</span> is simply the mean quality of the papers, this is easily computed.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> reviews.reviews.Quality.mean()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean value, mu = &quot;</span>, mu)</span></code></pre></div>
<h2 id="data-preparation">Data Preparation</h2>
<p>We take the reviews, which are indexed by the paper number, and create a new data frame, that indexes by paper id and email combined. From these reviews we tokenize the <code>PaperID</code> and the <code>Email</code> to extract two matrices that can be used in creation of covariance matrices. We also create a target vector which is the mean centred vector of scores.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> reviews.reviews.reset_index()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>r.rename(columns<span class="op">=</span>{<span class="st">&#39;ID&#39;</span>:<span class="st">&#39;PaperID&#39;</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>r.index <span class="op">=</span> r.PaperID <span class="op">+</span> <span class="st">&#39;_&#39;</span> <span class="op">+</span> r.Email</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> pd.get_dummies(r.PaperID)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> X1[<span class="bu">sorted</span>(X1.columns, key<span class="op">=</span><span class="bu">int</span>)]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> pd.get_dummies(r.Email)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X2[<span class="bu">sorted</span>(X2.columns, key<span class="op">=</span><span class="bu">str</span>.lower)]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> reviews.reviews.Quality <span class="op">-</span> mu</span></code></pre></div>
<h3 id="constructing-the-model-in-gpy">Constructing the Model in GPy</h3>
<p>Having reduced the model to two parameters, I was hopeful I could set parameters broadly by hand. My initial expectation was that <code>alpha_b</code> and <code>sigma2</code> would both be less than 1, but some playing with parameters showed this wasn’t the case. Rather than waste further time, I decided to use our <a href="https://github.com/SheffieldML/GPy"><code>GPy</code> Software</a> (see below) to find a maximum likelihood solution for the parameters.</p>
<p>Model construction firstly involves constructing covariance functions for the model and concatanating <code>X1</code> and <code>X2</code> to a new input matrix <code>X</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X1.join(X2)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>kern1 <span class="op">=</span> GPy.kern.Linear(input_dim<span class="op">=</span><span class="bu">len</span>(X1.columns), active_dims<span class="op">=</span>np.arange(<span class="bu">len</span>(X1.columns)))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>kern1.name <span class="op">=</span> <span class="st">&#39;K_f&#39;</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>kern2 <span class="op">=</span> GPy.kern.Linear(input_dim<span class="op">=</span><span class="bu">len</span>(X2.columns), active_dims<span class="op">=</span>np.arange(<span class="bu">len</span>(X1.columns), <span class="bu">len</span>(X.columns)))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>kern2.name <span class="op">=</span> <span class="st">&#39;K_b&#39;</span></span></code></pre></div>
<p>Next, the covariance function is used to create a Gaussian process regression model with <code>X</code> as input and <code>y</code> as target. The covariance function is given by <span class="math inline">\(\mathbf{K}_f + \mathbf{K}_b\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPy.models.GPRegression(X, y.to_numpy()[:, np.newaxis], kern1<span class="op">+</span>kern2)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>model.optimize()</span></code></pre></div>
<p>Now we can check the parameters of the result.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.log_likelihood())</span></code></pre></div>
<pre><code>    Name : GP regression
    Objective : 10071.679092815619
    Number of Parameters : 3
    Number of Optimization Parameters : 3
    Updates : True
    Parameters:
      GP_regression.           |               value  |  constraints  |  priors
      sum.K_f.variances        |  1.2782303448777643  |      +ve      |        
      sum.K_b.variances        |  0.2400098787580176  |      +ve      |        
      Gaussian_noise.variance  |  1.2683656892796749  |      +ve      |        
    -10071.679092815619</code></pre>
<h3 id="construct-the-model-without-gpy">Construct the Model Without GPy</h3>
<p>The answer from the GPy solution is introduced here, alongside the code where the covariance matrices are explicitly created (above they are created using GPy’s high level code for kernel matrices, which may be less clear on the details).</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set parameter values to ML solutions given by GPy.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>alpha_f <span class="op">=</span> model.<span class="bu">sum</span>.K_f.variances</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>alpha_b <span class="op">=</span> model.<span class="bu">sum</span>.K_b.variances<span class="op">/</span>alpha_f</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> model.Gaussian_noise.variance<span class="op">/</span>alpha_f</span></code></pre></div>
<p>Now we create the covariance functions based on the tokenized paper IDs and emails.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>K_f <span class="op">=</span> np.dot(X1, X1.T)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>K_b <span class="op">=</span> alpha_b<span class="op">*</span>np.dot(X2, X2.T)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> K_f <span class="op">+</span> K_b <span class="op">+</span> sigma2<span class="op">*</span>np.eye(X2.shape[<span class="dv">0</span>])</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>Kinv, L, Li, logdet <span class="op">=</span> GPy.util.linalg.pdinv(K) <span class="co"># since we have GPy loaded in use their positive definite inverse.</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> reviews.reviews.Quality <span class="op">-</span> mu</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.dot(Kinv, y)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>yTKinvy <span class="op">=</span> np.dot(y, alpha)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>alpha_f <span class="op">=</span> yTKinvy<span class="op">/</span><span class="bu">len</span>(y)</span></code></pre></div>
<p>Since we have removed the data mean, the log likelihood we are interested in is the likelihood of a multivariate Gaussian with covariance <span class="math inline">\(\mathbf{K}\)</span> and mean zero. This is computed below.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span><span class="bu">len</span>(y)<span class="op">*</span>np.log(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">*</span>alpha_f) <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>logdet <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>yTKinvy<span class="op">/</span>alpha_f </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;negative log likelihood: &quot;</span>, ll)</span></code></pre></div>
<h3 id="review-quality-prediction">Review Quality Prediction</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/review-quality-prediction.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/review-quality-prediction.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Now we wish to predict the bias corrected scores for the papers. That involves considering a variable <span class="math inline">\(s_{i,j} = f_i + e_{i,j}\)</span> which is the score with the bias removed. That variable has a covariance matrix, <span class="math inline">\(\mathbf{K}_s=\mathbf{K}_f + \sigma^2 \mathbf{I}\)</span> and a cross covariance between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{s}\)</span> is also given by <span class="math inline">\(\mathbf{K}_s\)</span>. This means we can compute the posterior distribution of the scores as follows:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute mean and covariance of quality scores</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>K_s <span class="op">=</span> K_f <span class="op">+</span> np.eye(K_f.shape[<span class="dv">0</span>])<span class="op">*</span>sigma2</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> pd.Series(np.dot(K_s, alpha) <span class="op">+</span> mu, index<span class="op">=</span>X1.index)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>covs <span class="op">=</span> alpha_f<span class="op">*</span>(K_s <span class="op">-</span> np.dot(K_s, np.dot(Kinv, K_s)))</span></code></pre></div>
<h3 id="monte-carlo-simulations-for-probability-of-acceptance">Monte Carlo Simulations for Probability of Acceptance</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/paper-acceptance-monte-carlo.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/paper-acceptance-monte-carlo.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We can now sample from this posterior distribution of bias-adjusted scores jointly, to get a set of scores for all papers. For this set of scores we can perform a ranking and accept the top 400 papers. This gives us a sampled conference. If we do that 1,000 times then we can see how many times each paper was accepted to get a probability of acceptance.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>number_accepts <span class="op">=</span> <span class="dv">420</span> <span class="co"># 440 because of the 10% replication</span></span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># place this in a separate box, because sampling can take a while.</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>s, cov<span class="op">=</span>covs, size<span class="op">=</span>samples).T</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Use X1 which maps papers to paper/reviewer pairings to get the average score for each paper.</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>paper_score <span class="op">=</span> pd.DataFrame(np.dot(np.diag(<span class="fl">1.</span><span class="op">/</span>X1.<span class="bu">sum</span>(<span class="dv">0</span>)), np.dot(X1.T, score)), index<span class="op">=</span>X1.columns)</span></code></pre></div>
<p>Now we can compute the probability of acceptance for each of the sampled rankings.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>prob_accept <span class="op">=</span> ((paper_score<span class="op">&gt;</span>paper_score.quantile(<span class="dv">1</span><span class="op">-</span>(<span class="bu">float</span>(number_accepts)<span class="op">/</span>paper_score.shape[<span class="dv">0</span>]))).<span class="bu">sum</span>(<span class="dv">1</span>)<span class="op">/</span><span class="dv">1000</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>prob_accept.name <span class="op">=</span> <span class="st">&#39;AcceptProbability&#39;</span></span></code></pre></div>
<p>Now we have the probability of accepts, we can decide on the boundaries of the grey area. These are set in <code>lower</code> and <code>upper</code>. The grey area is those papers that will be debated most heavily during the teleconferences between program chairs and area chairs.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>lower<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>upper<span class="op">=</span><span class="fl">0.9</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>grey_area <span class="op">=</span> ((prob_accept<span class="op">&gt;</span>lower) <span class="op">&amp;</span> (prob_accept<span class="op">&lt;</span>upper))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Number of papers in grey area:&#39;</span>, grey_area.<span class="bu">sum</span>())</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cmtutils.plot <span class="im">as</span> plot</span></code></pre></div>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Expected Papers Accepted:&#39;</span>, prob_accept.<span class="bu">sum</span>())</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> prob_accept.hist(bins<span class="op">=</span><span class="dv">40</span>, ax<span class="op">=</span>ax)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>ma.write_figure(directory<span class="op">=</span><span class="st">&quot;./neurips&quot;</span>, filename<span class="op">=</span><span class="st">&quot;probability-of-accept.svg&quot;</span>)</span></code></pre></div>
<div class="figure">
<div id="probability-of-accept-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/probability-of-accept.svg" width="70%" style=" ">
</object>
</div>
<div id="probability-of-accept-magnify" class="magnify" onclick="magnifyFigure(&#39;probability-of-accept&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="probability-of-accept-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="some-sanity-histograms">Some Sanity Histograms</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/calibration-sanity-checks.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/calibration-sanity-checks.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Here is the histogram of the reviewer scores after calibration.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>s.hist(bins<span class="op">=</span><span class="dv">100</span>, ax<span class="op">=</span>ax)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_title(<span class="st">&#39;Calibrated Reviewer Scores&#39;</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>ma.write_figure(directory<span class="op">=</span><span class="st">&quot;./neurips&quot;</span>, filename<span class="op">=</span><span class="st">&quot;calibrated-reviewer-scores.svg&quot;</span>)</span></code></pre></div>
<div class="figure">
<div id="calibrated-reviewer-scores-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/calibrated-reviewer-scores.svg" width="70%" style=" ">
</object>
</div>
<div id="calibrated-reviewer-scores-magnify" class="magnify" onclick="magnifyFigure(&#39;calibrated-reviewer-scores&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="calibrated-reviewer-scores-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h3 id="adjustments-to-reviewer-scores">Adjustments to Reviewer Scores</h3>
<p>We can also compute the posterior distribution for the adjustments to the reviewer scores.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute mean and covariance of review biases</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> pd.Series(np.dot(K_b, alpha), index<span class="op">=</span>X2.index)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>covb <span class="op">=</span> alpha_f<span class="op">*</span>(K_b <span class="op">-</span> np.dot(K_b, np.dot(Kinv, K_b)))</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>reviewer_bias <span class="op">=</span> pd.Series(np.dot(np.diag(<span class="fl">1.</span><span class="op">/</span>X2.<span class="bu">sum</span>(<span class="dv">0</span>)), np.dot(X2.T, b)), index<span class="op">=</span>X2.columns, name<span class="op">=</span><span class="st">&#39;ReviewerBiasMean&#39;</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>reviewer_bias_std <span class="op">=</span> pd.Series(np.dot(np.diag(<span class="fl">1.</span><span class="op">/</span>X2.<span class="bu">sum</span>(<span class="dv">0</span>)), np.dot(X2.T, np.sqrt(np.diag(covb)))), index<span class="op">=</span>X2.columns, name<span class="op">=</span><span class="st">&#39;ReviewerBiasStd&#39;</span>)</span></code></pre></div>
<p>Here is a histogram of the mean adjustment for the reviewers.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>reviewer_bias.hist(bins<span class="op">=</span><span class="dv">100</span>, ax<span class="op">=</span>ax)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_title(<span class="st">&#39;Reviewer Calibration Adjustments Histogram&#39;</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>ma.write_figure(directory<span class="op">=</span><span class="st">&quot;./neurips&quot;</span>, filename<span class="op">=</span><span class="st">&quot;reviewer-calibration-adjustments.svg&quot;</span>)</span></code></pre></div>
<div class="figure">
<div id="reviewer-calibration-adjustments-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/reviewer-calibration-adjustments.svg" width="70%" style=" ">
</object>
</div>
<div id="reviewer-calibration-adjustments-magnify" class="magnify" onclick="magnifyFigure(&#39;reviewer-calibration-adjustments&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="reviewer-calibration-adjustments-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>Export a version of the bias scores for use in CMT.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>bias_export <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>{<span class="st">&#39;Quality Score - Does the paper deserves to be published?&#39;</span>:reviewer_bias, </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&#39;Impact Score - Independently of the Quality Score above, this is your opportunity to identify papers that are very different, original, or otherwise potentially impactful for the NIPS community.&#39;</span>:pd.Series(np.zeros(<span class="bu">len</span>(reviewer_bias)), index<span class="op">=</span>reviewer_bias.index),</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&#39;Confidence&#39;</span>:pd.Series(np.zeros(<span class="bu">len</span>(reviewer_bias)), index<span class="op">=</span>reviewer_bias.index)})</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> bias_export.columns.tolist()</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> [cols[<span class="dv">2</span>], cols[<span class="dv">1</span>], cols[<span class="dv">0</span>]]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>bias_export <span class="op">=</span> bias_export[cols]</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">#bias_export.to_csv(os.path.join(cu.cmt_data_directory, &#39;reviewer_bias.csv&#39;), sep=&#39;\t&#39;, header=True, index_label=&#39;Reviewer Email&#39;)</span></span></code></pre></div>
<h2 id="sanity-check">Sanity Check</h2>
<p>As a sanity check Corinna suggested it makes sense to plot the average raw score for the papers vs the probability of accept, just to ensure nothing weird is going on. To clarify the plot, I’ve actually plotted raw score vs log odds of accept.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>raw_score <span class="op">=</span> pd.Series(np.dot(np.diag(<span class="fl">1.</span><span class="op">/</span>X1.<span class="bu">sum</span>(<span class="dv">0</span>)), np.dot(X1.T, r.Quality)), index<span class="op">=</span>X1.columns)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>prob_accept[prob_accept<span class="op">==</span><span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">10</span><span class="op">*</span>samples)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>prob_accept[prob_accept<span class="op">==</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span>(<span class="dv">10</span><span class="op">*</span>samples)</span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>ax.plot(raw_score, np.log(prob_accept)<span class="op">-</span> np.log(<span class="dv">1</span><span class="op">-</span>prob_accept), <span class="st">&#39;rx&#39;</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Raw Score vs Log odds of accept&#39;</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;raw score&#39;</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">&#39;log odds of accept&#39;</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>ma.write_figure(directory<span class="op">=</span><span class="st">&quot;./neurips&quot;</span>, filename<span class="op">=</span><span class="st">&quot;raw-score-vs-log-odds.svg&quot;</span>)</span></code></pre></div>
<div class="figure">
<div id="raw-score-vs-log-odds-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/raw-score-vs-log-odds.svg" width="70%" style=" ">
</object>
</div>
<div id="raw-score-vs-log-odds-magnify" class="magnify" onclick="magnifyFigure(&#39;raw-score-vs-log-odds&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="raw-score-vs-log-odds-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="calibraton-quality-sanity-checks">Calibraton Quality Sanity Checks</h2>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>s.name <span class="op">=</span> <span class="st">&#39;CalibratedQuality&#39;</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> r.join(s)</span></code></pre></div>
<p>We can also look at a scatter plot of the review quality vs the calibrated quality.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>ax.plot(r.Quality, r.CalibratedQuality, <span class="st">&#39;r.&#39;</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="dv">0</span>, <span class="dv">11</span>])</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;original review score&#39;</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.set_ylabel(<span class="st">&#39;calibrated review score&#39;</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>ma.write_figure(directory<span class="op">=</span><span class="st">&quot;./neurips&quot;</span>, filename<span class="op">=</span><span class="st">&quot;calibrated-review-score-vs-original-score.svg&quot;</span>)</span></code></pre></div>
<div class="figure">
<div id="-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/calibrated-review-score-vs-original-score.svg" width="70%" style=" ">
</object>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="correlation-of-duplicate-papers">Correlation of Duplicate Papers</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/calibration-correlation-of-duplicate-papers.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/calibration-correlation-of-duplicate-papers.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>For NeurIPS 2014 we experimented with duplicate papers: we pushed papers through the system twice, exposing them to different subsets of the reviewers. The first thing we’ll look at is the duplicate papers. Firstly we identify them by matching on title.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>filename <span class="op">=</span> date <span class="op">+</span> <span class="st">&#39;_paper_list.xls&#39;</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>papers <span class="op">=</span> cu.CMT_Papers_read(filename<span class="op">=</span>filename)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>duplicate_list <span class="op">=</span> []</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ID, title <span class="kw">in</span> papers.papers.Title.iteritems():</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">int</span>(ID)<span class="op">&gt;</span><span class="dv">1779</span> <span class="kw">and</span> <span class="bu">int</span>(ID) <span class="op">!=</span> <span class="dv">1949</span>:</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        pair <span class="op">=</span> <span class="bu">list</span>(papers.papers[papers.papers[<span class="st">&#39;Title&#39;</span>].<span class="bu">str</span>.contains(papers.papers.Title[ID].strip())].index)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        pair.sort(key<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        duplicate_list.append(pair)</span></code></pre></div>
<p>Next we compute the correlation coefficients for the duplicated papers for the average impact and quality scores.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>quality <span class="op">=</span> []</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>calibrated_quality <span class="op">=</span> []</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>accept <span class="op">=</span> []</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>impact <span class="op">=</span> []</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>confidence <span class="op">=</span> []</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> duplicate_pair <span class="kw">in</span> duplicate_list:</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    quality.append([np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">0</span>]].Quality), np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">1</span>]].Quality)])</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    calibrated_quality.append([np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">0</span>]].CalibratedQuality), np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">1</span>]].CalibratedQuality)])</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    impact.append([np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">0</span>]].Impact), np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">1</span>]].Impact)])</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    confidence.append([np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">0</span>]].Conf), np.mean(r[r.PaperID<span class="op">==</span>duplicate_pair[<span class="dv">1</span>]].Conf)])</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>quality <span class="op">=</span> np.array(quality)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>calibrated_quality <span class="op">=</span> np.array(calibrated_quality)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>impact <span class="op">=</span> np.array(impact)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>confidence <span class="op">=</span> np.array(confidence)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>quality_cor <span class="op">=</span> np.corrcoef(quality.T)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>calibrated_quality_cor <span class="op">=</span> np.corrcoef(calibrated_quality.T)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>impact_cor <span class="op">=</span> np.corrcoef(impact.T)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>confidence_cor <span class="op">=</span> np.corrcoef(confidence.T)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Quality correlation: &quot;</span>, quality_cor)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Calibrated Quality correlation: &quot;</span>, calibrated_quality_cor)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Impact correlation: &quot;</span>, impact_cor)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Confidence correlation: &quot;</span>, confidence_cor)</span></code></pre></div>
<pre><code>    Quality correlation:  0.54403674862622
    Calibrated Quality correlation:  0.5455958618174274
    Impact correlation:  0.26945269236041036
    Confidence correlation:  0.3854251559444674</code></pre>
<h2 id="correlation-plots">Correlation Plots</h2>
<p>To visualize the quality score correlation we plot the group 1 papers against the group 2 papers. Here we add a small amount of jitter to ensure points to help visualize points that would otherwise fall on the same position.</p>
<div class="figure">
<div id="quality-correlation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/quality-correlation.svg" width="60%" style=" ">
</object>
</div>
<div id="quality-correlation-magnify" class="magnify" onclick="magnifyFigure(&#39;quality-correlation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="quality-correlation-caption" class="caption-frame">
<p>Figure: Correlation between reviewer scores across the duplicated committes (scores have jitter added to prevent too many points sitting on top of each other).</p>
</div>
</div>
<p>Similarly for the calibrated quality of the papers.</p>
<div class="figure">
<div id="calibrated-quality-correlation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/calibrated-quality-correlation.svg" width="60%" style=" ">
</object>
</div>
<div id="calibrated-quality-correlation-magnify" class="magnify" onclick="magnifyFigure(&#39;calibrated-quality-correlation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="calibrated-quality-correlation-caption" class="caption-frame">
<p>Figure: Correlation between <em>calibrated</em> reviewer scores across the two independent committees.</p>
</div>
</div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Laplace smoothing to accept probabilities before incorporating them.</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>revs <span class="op">=</span> r.join((prob_accept<span class="op">+</span><span class="fl">0.0002</span>)<span class="op">/</span><span class="fl">1.001</span>, on<span class="op">=</span><span class="st">&#39;PaperID&#39;</span>).join(reviewer_bias, on<span class="op">=</span><span class="st">&#39;Email&#39;</span>).join(papers.papers[<span class="st">&#39;Number Of Discussions&#39;</span>], on<span class="op">=</span><span class="st">&#39;PaperID&#39;</span>).join(reviewer_bias_std, on<span class="op">=</span><span class="st">&#39;Email&#39;</span>).sort_values(by<span class="op">=</span>[<span class="st">&#39;AcceptProbability&#39;</span>,<span class="st">&#39;PaperID&#39;</span>, <span class="st">&#39;CalibratedQuality&#39;</span>], ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>revs.set_index([<span class="st">&#39;PaperID&#39;</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> len_comments(x):</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(x.Comments)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>revs[<span class="st">&#39;comment_length&#39;</span>]<span class="op">=</span>revs.<span class="bu">apply</span>(len_comments, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the computed information to disk</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co">#revs.to_csv(os.path.join(cu.cmt_data_directory, date + &#39;_processed_reviews.csv&#39;), encoding=&#39;utf-8&#39;)</span></span></code></pre></div>
<h2 id="conference-simulation">Conference Simulation</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-simulation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-simulation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Given the realisation that roughly 50% of the score seems to be ‘subjective’ and 50% of the score seems to be ‘objective,’ then we can simulate the conference and see what it does for the consistency of accepts for different probability of accept.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>subjectivity_portion <span class="op">=</span> <span class="fl">0.5</span></span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>accept_rates <span class="op">=</span> [<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.15</span>, <span class="fl">0.2</span>, <span class="fl">0.25</span>, <span class="fl">0.3</span>, <span class="fl">0.35</span>, <span class="fl">0.4</span>, <span class="fl">0.45</span>, <span class="fl">0.5</span>, <span class="fl">0.55</span>, <span class="fl">0.6</span>, <span class="fl">0.65</span>, <span class="fl">0.7</span>, <span class="fl">0.75</span>, <span class="fl">0.8</span>, <span class="fl">0.85</span>, <span class="fl">0.9</span>, <span class="fl">0.95</span>, <span class="fl">1.0</span>]</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>consistent_accepts <span class="op">=</span> []</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> accept_rate <span class="kw">in</span> accept_rates:</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    score_1 <span class="op">=</span> []</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    score_2 <span class="op">=</span> []</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(samples):</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        objective <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>subjectivity_portion)<span class="op">*</span>np.random.randn()</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        score_1.append(objective <span class="op">+</span> subjectivity_portion<span class="op">*</span>np.random.randn())</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>        score_2.append(objective <span class="op">+</span> subjectivity_portion<span class="op">*</span>np.random.randn())</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    score_1 <span class="op">=</span> np.asarray(score_1)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    score_2 <span class="op">=</span> np.asarray(score_2)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    accept_1 <span class="op">=</span> score_1.argsort()[:<span class="bu">int</span>(samples<span class="op">*</span>accept_rate)]</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    accept_2 <span class="op">=</span> score_2.argsort()[:<span class="bu">int</span>(samples<span class="op">*</span>accept_rate)]</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    consistent_accept <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(accept_1).intersection(<span class="bu">set</span>(accept_2)))</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    consistent_accepts.append(consistent_accept<span class="op">/</span>(samples<span class="op">*</span>accept_rate))</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Percentage consistently accepted: </span><span class="sc">{prop}</span><span class="st">&#39;</span>.<span class="bu">format</span>(prop<span class="op">=</span>consistent_accept<span class="op">/</span>(samples<span class="op">*</span>accept_rate)))</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>consistent_accepts <span class="op">=</span> np.array(consistent_accepts)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>accept_rate <span class="op">=</span> np.array(accept_rate)</span></code></pre></div>
<div class="figure">
<div id="consistency-vs-accept-rate-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//consistency-vs-accept-rate.svg" width="70%" style=" ">
</object>
</div>
<div id="consistency-vs-accept-rate-magnify" class="magnify" onclick="magnifyFigure(&#39;consistency-vs-accept-rate&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="consistency-vs-accept-rate-caption" class="caption-frame">
<p>Figure: Plot of the accept rate vs the consistency of the conference for 50% subjectivity.</p>
</div>
</div>
<div class="figure">
<div id="gain-in-consistency-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//gain-in-consistency.svg" width="70%" style=" ">
</object>
</div>
<div id="gain-in-consistency-magnify" class="magnify" onclick="magnifyFigure(&#39;gain-in-consistency&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gain-in-consistency-caption" class="caption-frame">
<p>Figure: Plot of the accept rate vs gain in consistency over a random conference for 50% subjectivity.</p>
</div>
</div>
<h2 id="where-do-rejected-papers-go">Where do Rejected Papers Go?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/where-do-the-rejected-papers-go.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/where-do-the-rejected-papers-go.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> yaml</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(nipsy.review_store, nipsy.outlet_name_mapping), <span class="st">&#39;r&#39;</span>) <span class="im">as</span> f:</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    mapping <span class="op">=</span> yaml.load(f, Loader<span class="op">=</span>yaml.FullLoader)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>date <span class="op">=</span> <span class="st">&quot;2021-06-11&quot;</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>citations <span class="op">=</span> nipsy.load_citation_counts(date<span class="op">=</span>date)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>decisions <span class="op">=</span> nipsy.load_decisions()</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>nipsy.augment_decisions(decisions)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>joindf <span class="op">=</span> nipsy.join_decisions_citations(decisions, citations)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>joindf[<span class="st">&#39;short_venue&#39;</span>] <span class="op">=</span> joindf.venue.replace(mapping)</span></code></pre></div>
<div class="figure">
<div id="where-do-neurips-papers-go-figure" class="figure-frame">
<iframe src="https://inverseprobability.com/talks/slides/diagrams//neurips/where-do-neurips-papers-go.html" width="600" height="450" allowtransparency="true" frameborder="0">
</iframe>
</div>
<div id="where-do-neurips-papers-go-magnify" class="magnify" onclick="magnifyFigure(&#39;where-do-neurips-papers-go&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="where-do-neurips-papers-go-caption" class="caption-frame">
<p>Figure: Sankey diagram showing the flow of NeurIPS papers through the system from submission to eventual publication.</p>
</div>
</div>
<p>This notebook analyzes the reduction in reviewer confidence between reviewers that submit their reviews early and those that arrive late. The reviews are first loaded in from files Corinna and Neil saved and stored in a pickle. The function for doing that is <code>nips.load_review_history</code>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cmtutils <span class="im">as</span> cu</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cmtutils.nipsy <span class="im">as</span> nipsy </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cmtutils.plot <span class="im">as</span> plot</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>reviews <span class="op">=</span> nipsy.load_review_history()</span></code></pre></div>
<h2 id="review-submission-times">Review Submission Times</h2>
<p>All reviews are now in pandas data frame called reviews, they are ready for processing. First of all, let's take a look at when the reviews were submitted. The function <code>nipsy.reviews_before</code> gives a snapshot of the reviews as they stood at a particular date. So we simply create a data series across the data range of reviews (<code>nipsy.review_data_range</code>) that shows the counts.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>review_count <span class="op">=</span> pd.Series(index<span class="op">=</span>nipsy.review_date_range)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> date <span class="kw">in</span> nipsy.review_date_range:</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    review_count.loc[date] <span class="op">=</span> nipsy.reviews_before(reviews, date).Quality.shape[<span class="dv">0</span>]</span></code></pre></div>
<div class="figure">
<div id="review-count-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-count.svg" width="70%" style=" ">
</object>
</div>
<div id="review-count-magnify" class="magnify" onclick="magnifyFigure(&#39;review-count&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-count-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>We worked hard to try and ensure that all papers had three reviews before the start of the rebuttal. This next plot shows the numbers of papers that had less than three reviews across the review period. First let’s look at the overall statistics of what the count of reviewers per paper were. Below we plot mean, maximum, median and minimum over time.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>lastseen <span class="op">=</span> reviews.drop_duplicates(subset<span class="op">=</span><span class="st">&#39;ID&#39;</span>).set_index(<span class="st">&#39;ID&#39;</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>lastseen <span class="op">=</span> lastseen[<span class="st">&#39;LastSeen&#39;</span>]</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>review_count <span class="op">=</span> pd.DataFrame(index<span class="op">=</span>reviews.ID.unique(), columns<span class="op">=</span>nipsy.review_date_range)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> date <span class="kw">in</span> nipsy.review_date_range:</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> nipsy.reviews_status(reviews, date, column<span class="op">=</span><span class="st">&#39;Quality&#39;</span>).count(level<span class="op">=</span><span class="st">&#39;ID&#39;</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    review_count[date] <span class="op">=</span> counts.fillna(<span class="dv">0</span>)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>review_count.fillna(<span class="dv">0</span>, inplace<span class="op">=</span><span class="va">True</span>)    </span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>review_count <span class="op">=</span> review_count.T</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> review_count.columns:</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pd.notnull(lastseen[col]):</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>        review_count[col][review_count.index<span class="op">&gt;</span>lastseen[col]] <span class="op">=</span> np.NaN</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>review_count <span class="op">=</span> review_count.T</span></code></pre></div>
<div class="figure">
<div id="number-of-reviews-over-time-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/number-of-reviews-over-time.svg" width="70%" style=" ">
</object>
</div>
<div id="number-of-reviews-over-time-magnify" class="magnify" onclick="magnifyFigure(&#39;number-of-reviews-over-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="number-of-reviews-over-time-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>But perhaps the more important measure is how many papers had less than 3 reviewers over time. In this plot you can see that by the time rebuttal starts almost all papers have three reviewers.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> pd.Series(index<span class="op">=</span>nipsy.review_date_range)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> date <span class="kw">in</span> nipsy.review_date_range:</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    count[date] <span class="op">=</span> (review_count[date]<span class="op">&lt;</span><span class="dv">3</span>).<span class="bu">sum</span>()</span></code></pre></div>
<div class="figure">
<div id="paper-short-reviews-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/paper-short-reviews.svg" width="70%" style=" ">
</object>
</div>
<div id="paper-short-reviews-magnify" class="magnify" onclick="magnifyFigure(&#39;paper-short-reviews&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="paper-short-reviews-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<h2 id="review-confidence">Review Confidence</h2>
<p>Now we will check the confidence of reviews as the come in over time. We’ve written a small helper function that looks in a four day window around each time point and summarises the associated score (in the first case, confidence, <code>Conf</code>) with its across the four day window and 95% confidence intervals computed from the standard error of the mean estimate.</p>
<div class="figure">
<div id="review-confidence-time-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-confidence-time.svg" width="70%" style=" ">
</object>
</div>
<div id="review-confidence-time-magnify" class="magnify" onclick="magnifyFigure(&#39;review-confidence-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-confidence-time-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>It looks like there might be a reduction in confidence as we pass the review deadline on 21st July, but is the difference in confidence for the reviews that came in later really significant?</p>
<p>We now simplify the question by looking at the average confidence for reviews that arrived before 21st July (the reviewing deadline) and reviews that arrived after the 21st July (i.e. those that were chased or were allocated late) but before the rebuttal period started (4th August). Below we select these two groups and estimate the estimate of the mean confidence with (again with error bars).</p>
<div class="figure">
<div id="review-confidence-early-late-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-confidence-early-late.svg" width="50%" style=" ">
</object>
</div>
<div id="review-confidence-early-late-magnify" class="magnify" onclick="magnifyFigure(&#39;review-confidence-early-late&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-confidence-early-late-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>So it looks like there is a small but significant difference between the average confidence of the submitted reviews before and after the deadline, the statistical significance is confirmed with a <span class="math inline">\(t\)</span>-test with a <span class="math inline">\(p\)</span>-value at 0.048%. The magnitude of the difference is small (about 0.1) but may indicate a tendency for later reviewers to be a little more rushed.</p>
<h3 id="quality-score">Quality Score</h3>
<p>This begs the question, is there an effect on the other scores of their reviews which cover 'quality' and 'impact'. Quality of papers is scored on a 10 point scale with a recommendation of 6 being accept and We can form a similar plot for quality as follows.</p>
<div class="figure">
<div id="review-quality-time-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-quality-time.svg" width="70%" style=" ">
</object>
</div>
<div id="review-quality-time-magnify" class="magnify" onclick="magnifyFigure(&#39;review-quality-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-quality-time-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<div class="figure">
<div id="review-quality-early-late-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-quality-early-late.svg" width="50%" style=" ">
</object>
</div>
<div id="review-quality-early-late-magnify" class="magnify" onclick="magnifyFigure(&#39;review-quality-early-late&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-quality-early-late-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>There is another statistically significant difference between perceived quality scores after the reviewing deadline than before. On average reviewers tend to be more generous in their quality perceptions when the review is late. The <span class="math inline">\(p\)</span>-value is computed as 0.007%. We can also check if there is a similar on the impact score. The impact score was introduced by Ghahramani and Welling in an effort to get reviewers not just to think about the technical side of the paper, but whether it is driving the field forward. The score is binary, with 1 being for a paper that is unlikey to have high impact and 2 being for a paper that is likely to have a high impact.</p>
<div class="figure">
<div id="review-impact-time-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-impact-time.svg" width="70%" style=" ">
</object>
</div>
<div id="review-impact-time-magnify" class="magnify" onclick="magnifyFigure(&#39;review-impact-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-impact-time-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<div class="figure">
<div id="review-impact-early-late-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-impact-early-late.svg" width="50%" style=" ">
</object>
</div>
<div id="review-impact-early-late-magnify" class="magnify" onclick="magnifyFigure(&#39;review-impact-early-late&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-impact-early-late-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>We find the difference is not quite statistically significant for the impact score (<span class="math inline">\(p\)</span>-value of 5.9%), but if anything there is a trend to have slightly higher impacts for later reviews.</p>
<h3 id="review-length">Review Length</h3>
<p>A final potential indicator of review quality is the length of the reviews, we can check if there is a difference between the combined length of the review summary and the main body comments for late and early reviews.</p>
<div class="figure">
<div id="review-length-time-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-length-time.svg" width="70%" style=" ">
</object>
</div>
<div id="review-length-time-magnify" class="magnify" onclick="magnifyFigure(&#39;review-length-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-length-time-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<div class="figure">
<div id="review-length-early-late-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/review-length-early-late.svg" width="50%" style=" ">
</object>
</div>
<div id="review-length-early-late-magnify" class="magnify" onclick="magnifyFigure(&#39;review-length-early-late&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="review-length-early-late-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>Once again we find a small but statitically significant difference, here, as we might expect late reviews are shorter than those submitted on time, by about 100 words in a 2,400 word review.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>review_quality <span class="op">=</span> pd.DataFrame(index<span class="op">=</span>reviews.ID.unique(), columns<span class="op">=</span>nipsy.review_date_range)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> date <span class="kw">in</span> nipsy.review_date_range:</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    qual <span class="op">=</span> nipsy.reviews_status(reviews, date, column<span class="op">=</span><span class="st">&#39;Quality&#39;</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    review_quality[date] <span class="op">=</span> qual.<span class="bu">sum</span>(level<span class="op">=</span><span class="st">&#39;ID&#39;</span>)<span class="op">/</span>qual.count(level<span class="op">=</span><span class="st">&#39;ID&#39;</span>) <span class="co"># There&#39;s a bug where mean doesn&#39;t work in Pandas 1.2.4??</span></span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>original_pairs <span class="op">=</span> pd.read_csv(os.path.join(nipsy.review_store, <span class="st">&#39;Duplicate_PaperID_Pairs.csv&#39;</span>), index_col<span class="op">=</span><span class="st">&#39;original&#39;</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>duplicate_pairs <span class="op">=</span> pd.read_csv(os.path.join(nipsy.review_store, <span class="st">&#39;Duplicate_PaperID_Pairs.csv&#39;</span>), index_col<span class="op">=</span><span class="st">&#39;duplicate&#39;</span>)</span></code></pre></div>
<p>Perform an 'inner join' on duplicate papers and their originals with their reviews, and set the index of the duplicated papers to match the original. This gives us data frames with matching indices containing scores over time of the duplicate and original papers.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>duplicate_reviews <span class="op">=</span> duplicate_pairs.join(review_quality, how<span class="op">=</span><span class="st">&quot;inner&quot;</span>).set_index(<span class="st">&#39;original&#39;</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>original_reviews <span class="op">=</span> original_pairs.join(review_quality, how<span class="op">=</span><span class="st">&quot;inner&quot;</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> original_reviews[<span class="st">&quot;duplicate&quot;</span>]</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>corr_series <span class="op">=</span> duplicate_reviews.corrwith(original_reviews)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>corr_series.index <span class="op">=</span> pd.to_datetime(corr_series.index)</span></code></pre></div>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>bootstrap_corr_df <span class="op">=</span> pd.DataFrame(index<span class="op">=</span>corr_series.index)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> bootstrap_index(original_reviews)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    b_corr_series <span class="op">=</span> duplicate_reviews.loc[ind].corrwith(original_reviews.loc[ind])</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    b_corr_series.index <span class="op">=</span> pd.to_datetime(b_corr_series.index)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    bootstrap_corr_df[i] <span class="op">=</span> b_corr_series</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime <span class="im">as</span> dt</span></code></pre></div>
<div class="figure">
<div id="correlation-duplicate-reviews-bootstrap-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/correlation-duplicate-reviews-bootstrap.svg" width="70%" style=" ">
</object>
</div>
<div id="correlation-duplicate-reviews-bootstrap-magnify" class="magnify" onclick="magnifyFigure(&#39;correlation-duplicate-reviews-bootstrap&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="correlation-duplicate-reviews-bootstrap-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>Plot the correlation of the duplicated papers over time (do bootstrap samples here??)</p>
<div class="figure">
<div id="correlation-duplicate-reviews-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/slides/diagrams//neurips/correlation-duplicate-reviews.svg" width="70%" style=" ">
</object>
</div>
<div id="correlation-duplicate-reviews-magnify" class="magnify" onclick="magnifyFigure(&#39;correlation-duplicate-reviews&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="correlation-duplicate-reviews-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<p>We need to do a bit more analysis on the estimation of the correlation for the earlier submissions, but from what we see above, it looks like the correlation is being damaged by late reviews, and we never quite recover the consistency of reviews we had at the submission deadline even after the discussion phase is over.</p>
<h2 id="late-reviewers-summary">Late Reviewers Summary</h2>
<p>In summary we find that late reviews are on average less confident and shorter, but rate papers as higher quality and perhaps as higher impact. Each of the effects is small (around 5%) but overall a picture emerges of a different category of review from those that delay their assesment.</p>
<h2 id="conclusion">Conclusion</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-conclusion.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_neurips/includes/neurips-experiment-conclusion.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Under the simple model we have outlined, we can be confident that there is inconsistency between two independent committees, but the level of inconsistency is much less than we would find for a random committee. If we accept that the bias introduced by the Area Chairs knowing when they were dealing with duplicates was minimal, then if we were to revisit the NIPS 2014 conference with an independent committee then we would expect between <strong>38% and 64% of the presented papers to be the same</strong>. If the conference was run at random, then we would only expect 25% of the papers to be the same.</p>
<p>It’s apparent from comments and speculation about what these results mean, that some people might be surprised by the size of this figure. However, it only requires a little thought to see that this figure is likely to be large for any highly selective conference if there is even a small amount of inconsistency in the decision making process. This is because once the conference has chosen to be ‘highly selective’ then because by definition only a small percentage of papers are to be accepted. Now if we think of a type I error as accepting a paper which should be rejected, such errors are easier to make because by definition many more papers should be rejected. Type II errors (rejecting a paper that should be accepted) are less likely becaue (by setting the accept rate low) there are fewer papers that should be accepted in the first place. When there is a difference of opinion between reviewers, it does seem that many of the arugments can be distilled down to (a subjective opinion) about whether controlling for type I or type II errors is more important. Further, normally when discussing type I and type II errors we believe that the underlying system of study is genuinely binary: e.g. diseased or not diseased. However, for conferences the accept/reject boundary is not a clear separation point, there is a continuum (or spectrum) of paper quality (as there also is for some diseases). And the decision boundary often falls in a region of very high density. To better quantify these ideas we can explore our duplication experiment in more detail, by introducing the paper scores, that’s a task we will perform in a fresh notebook.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="dv">1000000</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>accept_rate <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>score_1 <span class="op">=</span> []</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>score_2 <span class="op">=</span> []</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(samples):</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    objective <span class="op">=</span> np.random.randn()</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    score_1.append(objective <span class="op">+</span> np.random.randn())</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    score_2.append(objective <span class="op">+</span> np.random.randn())</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>score_1 <span class="op">=</span> np.asarray(score_1)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>score_2 <span class="op">=</span> np.asarray(score_2)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>accept_1 <span class="op">=</span> score_1.argsort()[:<span class="bu">int</span>(samples<span class="op">*</span>accept_rate)]</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>accept_2 <span class="op">=</span> score_2.argsort()[:<span class="bu">int</span>(samples<span class="op">*</span>accept_rate)]</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>consistent_accept <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(accept_1).intersection(<span class="bu">set</span>(accept_2)))</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Percentage consistently accepted: </span><span class="sc">{prop}</span><span class="st">&#39;</span>.<span class="bu">format</span>(prop<span class="op">=</span>consistent_accept<span class="op">/</span>(samples<span class="op">*</span>accept_rate)))</span></code></pre></div>
<h1 id="introduction-1">Introduction</h1>
<p>Experimental setup</p>
<p>Model calibration</p>
<p>Results</p>
<p>Model of Results</p>
<p>Community Reaction</p>
<p>What happened to rejected papers?</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 id="references">References</h1>

