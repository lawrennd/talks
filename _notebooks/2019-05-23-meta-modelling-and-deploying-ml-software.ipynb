{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Modelling and Deploying ML Software\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), Amazon Cambridge and University of Sheffield\n",
    "### 2019-05-23\n",
    "\n",
    "**Abstract**: Data is not so much the new oil, it is the new software. Data driven\n",
    "algorithms are increasingly present in continuously deployed production\n",
    "software. What challenges does this present and how can the mathematical\n",
    "sciences help?\n",
    "\n",
    "$$\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$\n",
    "\n",
    "<!-- Front matter -->\n",
    ".\n",
    "\n",
    "<!---->\n",
    "<!--Back matter-->\n",
    ".\n",
    "\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "# Introduction\n",
    "\n",
    "# Deep Learning [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-overview.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-overview.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "<!-- No slide titles in this context -->\n",
    "### DeepFace [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-face.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-face.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "::: {.figure}\n",
    "::: {#deep-face-figure .figure-frame}\n",
    "::: {.centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/deepface_neg.png\" width=\"100%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#deep-face-magnify .magnify onclick=\"magnifyFigure('deep-face')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#deep-face-caption .caption-frame}\n",
    "Figure: The DeepFace architecture [@Taigman:deepface14], visualized\n",
    "through colors to represent the functional mappings at each layer. There\n",
    "are 120 million parameters in the model.\n",
    ":::\n",
    ":::\n",
    "\n",
    "The DeepFace architecture [@Taigman:deepface14] consists of layers that\n",
    "deal with *translation* and *rotational* invariances. These layers are\n",
    "followed by three locally-connected layers and two fully-connected\n",
    "layers. Color illustrates feature maps produced at each layer. The\n",
    "neural network includes more than 120 million parameters, where more\n",
    "than 95% come from the local and fully connected layers.\n",
    "\n",
    "### Deep Learning as Pinball [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-as-pinball.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-as-pinball.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "::: {.figure}\n",
    "::: {#early-pinball-figure .figure-frame}\n",
    "::: {.centered .centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/576px-Early_Pinball.jpg\" width=\"50%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#early-pinball-magnify .magnify onclick=\"magnifyFigure('early-pinball')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#early-pinball-caption .caption-frame}\n",
    "Figure: Deep learning models are composition of simple functions. We can\n",
    "think of a pinball machine as an analogy. Each layer of pins corresponds\n",
    "to one of the layers of functions in the model. Input data is\n",
    "represented by the location of the ball from left to right when it is\n",
    "dropped in from the top. Output class comes from the position of the\n",
    "ball as it leaves the pins at the bottom.\n",
    ":::\n",
    ":::\n",
    "\n",
    "Sometimes deep learning models are described as being like the brain, or\n",
    "too complex to understand, but one analogy I find useful to help the\n",
    "gist of these models is to think of them as being similar to early pin\n",
    "ball machines.\n",
    "\n",
    "In a deep neural network, we input a number (or numbers), whereas in\n",
    "pinball, we input a ball.\n",
    "\n",
    "Think of the location of the ball on the left-right axis as a single\n",
    "number. Our simple pinball machine can only take one number at a time.\n",
    "As the ball falls through the machine, each layer of pins can be thought\n",
    "of as a different layer of 'neurons'. Each layer acts to move the ball\n",
    "from left to right.\n",
    "\n",
    "In a pinball machine, when the ball gets to the bottom it might fall\n",
    "into a hole defining a score, in a neural network, that is equivalent to\n",
    "the decision: a classification of the input object.\n",
    "\n",
    "An image has more than one number associated with it, so it is like\n",
    "playing pinball in a *hyper-space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('pinball{sample:0>3}.svg', \n",
    "                            '../slides/diagrams',\n",
    "                            sample=IntSlider(1, 1, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.figure}\n",
    "::: {#pinball-initialization-figure .figure-frame}\n",
    "<img src=\"../slides/diagrams/pinball001.svg\" class=\"\" align=\"80%\" style=\"vertical-align:middle;\">\n",
    ":::\n",
    "\n",
    "::: {#pinball-initialization-magnify .magnify onclick=\"magnifyFigure('pinball-initialization')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#pinball-initialization-caption .caption-frame}\n",
    "Figure: At initialization, the pins, which represent the parameters of\n",
    "the function, aren't in the right place to bring the balls to the\n",
    "correct decisions.\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {.figure}\n",
    "::: {#pinball-trained-figure .figure-frame}\n",
    "<img src=\"../slides/diagrams/pinball002.svg\" class=\"\" align=\"80%\" style=\"vertical-align:middle;\">\n",
    ":::\n",
    "\n",
    "::: {#pinball-trained-magnify .magnify onclick=\"magnifyFigure('pinball-trained')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#pinball-trained-caption .caption-frame}\n",
    "Figure: After learning the pins are now in the right place to bring the\n",
    "balls to the correct decisions.\n",
    ":::\n",
    ":::\n",
    "\n",
    "Learning involves moving all the pins to be in the correct position, so\n",
    "that the ball ends up in the right place when it's fallen through the\n",
    "machine. But moving all these pins in hyperspace can be difficult.\n",
    "\n",
    "In a hyper-space you have to put a lot of data through the machine for\n",
    "to explore the positions of all the pins. Even when you feed many\n",
    "millions of data points through the machine, there are likely to be\n",
    "regions in the hyper-space where no ball has passed. When future test\n",
    "data passes through the machine in a new route unusual things can\n",
    "happen.\n",
    "\n",
    "*Adversarial examples* exploit this high dimensional space. If you have\n",
    "access to the pinball machine, you can use gradient methods to find a\n",
    "position for the ball in the hyper space where the image looks like one\n",
    "thing, but will be classified as another.\n",
    "\n",
    "Probabilistic methods explore more of the space by considering a range\n",
    "of possible paths for the ball through the machine. This helps to make\n",
    "them more data efficient and gives some robustness to adversarial\n",
    "examples.\n",
    "\n",
    "## Machine Learning Systems Design [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/ml-systems-design-short.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/ml-systems-design-short.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "The challenges of integrating different machine learning components into\n",
    "a whole that acts effectively as a system seem unresolved. In software\n",
    "engineering, separating parts of a system in this way is known as\n",
    "[component-based software engineering](). The core idea is that the\n",
    "different parts of the system can be independently designed according to\n",
    "a sub-specfication. This is sometimes known as *separation of concerns*.\n",
    "However, once the components are machine learning based, tighter\n",
    "coupling becomes a side effect of the learned nature of the system. For\n",
    "example if a driverless car's detection of cyclist is dependent on its\n",
    "detection of the road surface, a change in the road surface detection\n",
    "algorithm will have downstream effects on the cyclist detection. Even if\n",
    "the road detection system has been improved by objective measures, the\n",
    "cyclist detection system may have become sensitive to the foibles of the\n",
    "previous version of road detection and will need to be retrained.\n",
    "\n",
    "Most of our experience with deployment relies on some approximation to\n",
    "the component based model, this is also important for verification of\n",
    "the system. If the components of the system can be verified then the\n",
    "composed system can also, potentially, be verified.\n",
    "\n",
    "## Pigeonholing [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/pigeonholing.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/pigeonholing.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "::: {.figure}\n",
    "::: {#too-many-pigeons-figure .figure-frame}\n",
    "::: {.centered .centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/TooManyPigeons.jpg\" width=\"60%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#too-many-pigeons-magnify .magnify onclick=\"magnifyFigure('too-many-pigeons')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#too-many-pigeons-caption .caption-frame}\n",
    "Figure: Decompartmentalization of the model into parts can be seen as\n",
    "pigeonholing the separate tasks that are required.\n",
    ":::\n",
    ":::\n",
    "\n",
    "To deal with the complexity of systems design, a common approach is to\n",
    "break complex systems down into a series of tasks. An approach we can\n",
    "think of as \"pigeonholing\". Classically, a sub-task could be thought of\n",
    "as a particular stage in machining (by analogy to productionlines in\n",
    "factories) or a sub-routine call in computing. Machine learning allows\n",
    "any complex sub-task, that was difficult to decompose by classical\n",
    "methods, to be reconstituted by acquiring data. In particular, when we\n",
    "think of emulating a human, we can ask many humans to perform the\n",
    "sub-task many times and fit machine learning models to reconstruct the\n",
    "performance, or to *emulate* the human in the performance of the task.\n",
    "For example, the decomposition of a complex process such as driving a\n",
    "car into apparently obvious sub-tasks (following the road, identifying\n",
    "pedestrians, etc).\n",
    "\n",
    "The practitioner's approach to deploying artificial intelligence systems\n",
    "is to build up systems of machine learning components. To build a\n",
    "machine learning system, we decompose the task into parts, each of which\n",
    "we can emulate with ML methods. These parts are typically independently\n",
    "constructed and verified. For example, in a driverless car we can\n",
    "decompose the tasks into components such as \"pedestrian detection\" and\n",
    "\"road line detection\". Each of these components can be constructed with,\n",
    "for example, a classification algorithm. Nowadays, people will often\n",
    "deploy a deep neural network, but for many tasks a random forest\n",
    "algorithm may be sufficient. We can then superimpose a logic on top. For\n",
    "example, \"Follow the road line unless you detect a pedestrian in the\n",
    "road\".\n",
    "\n",
    "This allows for verification of car performance, as long as we can\n",
    "verify the individual components. However, it also implies that the AI\n",
    "systems we deploy are *fragile*.\n",
    "\n",
    "Our intelligent systems are composed by \"pigeonholing\" each indvidual\n",
    "task, then substituting with a machine learning model.\n",
    "\n",
    "But this is not a robust approach to systems design. The definition of\n",
    "sub-tasks can lead to a single point of failure, where if any sub-task\n",
    "fails, the entire system fails.\n",
    "\n",
    "## Rapid Reimplementation\n",
    "\n",
    "This is also the classical approach to automation, but in traditional\n",
    "automation we also ensure the *environment* in which the system operates\n",
    "becomes controlled. For example, trains run on railway lines, fast cars\n",
    "run on motorways, goods are manufactured in a controlled factory\n",
    "environment.\n",
    "\n",
    "The difference with modern automated decision making systems is our\n",
    "intention is to deploy them in the *uncontrolled* environment that makes\n",
    "up our own world.\n",
    "\n",
    "This exposes us to either unforseen circumstances or adversarial action.\n",
    "And yet it is unclear our our intelligent systems are capable of\n",
    "adapting to this.\n",
    "\n",
    "We become exposed to mischief and adversaries. Adversaries intentially\n",
    "may wish to take over the artificial intelligence system, and mischief\n",
    "is the constant practice of many in our society. Simply watching a 10\n",
    "year old interact with a voice agent such as Alexa or Siri shows that\n",
    "they are delighted when the can make the the \"intelligent\" agent seem\n",
    "foolish.\n",
    "\n",
    "## The Centrifugal Governor [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/centrifugal-governor.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/centrifugal-governor.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "::: {.figure}\n",
    "::: {#science-holborn-viaduct-figure .figure-frame}\n",
    "::: {.centered .centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/science-holborn-viaduct.jpg\" width=\"50%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#science-holborn-viaduct-magnify .magnify onclick=\"magnifyFigure('science-holborn-viaduct')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#science-holborn-viaduct-caption .caption-frame}\n",
    "Figure: Centrifugal governor as held by \"Science\" on Holborn Viaduct\n",
    ":::\n",
    ":::\n",
    "\n",
    "## Boulton and Watt's Steam Engine [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/watt-steam-engine.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/watt-steam-engine.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "::: {.figure}\n",
    "::: {#steam-engine-boulton-watt-figure .figure-frame}\n",
    "::: {.centered style=\"\"}\n",
    "<img class=\"negate\" src=\"../slides/diagrams/SteamEngine_Boulton&Watt_1784.png\" width=\"70%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#steam-engine-boulton-watt-magnify .magnify onclick=\"magnifyFigure('steam-engine-boulton-watt')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#steam-engine-boulton-watt-caption .caption-frame}\n",
    "Figure: Watt's Steam Engine which made Steam Power Efficient and\n",
    "Practical.\n",
    ":::\n",
    ":::\n",
    "\n",
    "James Watt's steam engine contained an early machine learning device. In\n",
    "the same way that modern systems are component based, his engine was\n",
    "composed of components. One of which is a speed regulator sometimes\n",
    "known as *Watt's governor*. The two balls in the center of the image,\n",
    "when spun fast, rise, and through a linkage mechanism.\n",
    "\n",
    "The centrifugal governor was made famous by Boulton and Watt when it was\n",
    "deployed in the steam engine. Studying stability in the governor is the\n",
    "main subject of James Clerk Maxwell's paper on the theoretical analysis\n",
    "of governors [@Maxwell:governors1867]. This paper is a founding paper of\n",
    "control theory. In an acknowledgment of its influence, Wiener used the\n",
    "name [*cybernetics*](https://en.wikipedia.org/wiki/Cybernetics) to\n",
    "describe the field of control and communication in animals and the\n",
    "machine [@Wiener:cybernetics48]. Cybernetics is the Greek word for\n",
    "governor, which comes from the latin for helmsman.\n",
    "\n",
    "A governor is one of the simplest artificial intelligence systems. It\n",
    "senses the speed of an engine, and acts to change the position of the\n",
    "valve on the engine to slow it down.\n",
    "\n",
    "Although it's a mechanical system a governor can be seen as automating a\n",
    "role that a human would have traditionally played. It is an early\n",
    "example of artificial intelligence.\n",
    "\n",
    "The centrifugal governor has several parameters, the weight of the balls\n",
    "used, the length of the linkages and the limits on the balls movement.\n",
    "\n",
    "Two principle differences exist between the centrifugal governor and\n",
    "artificial intelligence systems of today.\n",
    "\n",
    "1.  The centrifugal governor is a physical system and it is an integral\n",
    "    part of a wider physical system that it regulates (the engine).\n",
    "2.  The parameters of the governor were set by hand, our modern\n",
    "    artificial intelligence systems have their parameters set by *data*.\n",
    "\n",
    "::: {.figure}\n",
    "::: {#centrifugal-governor-figure .figure-frame}\n",
    "::: {.centered style=\"\"}\n",
    "<img class=\"negate\" src=\"../slides/diagrams/Centrifugal_governor.png\" width=\"70%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#centrifugal-governor-magnify .magnify onclick=\"magnifyFigure('centrifugal-governor')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#centrifugal-governor-caption .caption-frame}\n",
    "Figure: The centrifugal governor, an early example of a decision making\n",
    "system. The parameters of the governor include the lengths of the\n",
    "linkages (which effect how far the throttle opens in response to\n",
    "movement in the balls), the weight of the balls (which effects inertia)\n",
    "and the limits of to which the balls can rise.\n",
    ":::\n",
    ":::\n",
    "\n",
    "This has the basic components of sense and act that we expect in an\n",
    "intelligent system, and this system saved the need for a human operator\n",
    "to manually adjust the system in the case of overspeed. Overspeed has\n",
    "the potential to destroy an engine, so the governor operates as a safety\n",
    "device.\n",
    "\n",
    "The first wave of automation did bring about sabotoage as a worker's\n",
    "response. But if machinery was sabotaged, for example, if the linkage\n",
    "between sensor (the spinning balls) and action (the valve closure) was\n",
    "broken, this would be obvious to the engine operator at start up time.\n",
    "The machine could be repaired before operation.\n",
    "\n",
    "The centrifugal governor was a key component in the Boulton-Watt steam\n",
    "engine. It senses increases in speed in the engine and closed the steam\n",
    "valve to prevent the engine overspeeding and destroying itself. Until\n",
    "the invention of this device, it was a human job to do this.\n",
    "\n",
    "The formal study of governors and other feedback control devices was\n",
    "then began by [James Clerk\n",
    "Maxwell](https://en.wikipedia.org/wiki/James_Clerk_Maxwell), the\n",
    "Scottish physicist. This field became the foundation of our modern\n",
    "techniques of artificial intelligence through Norbert Wiener's book\n",
    "*Cybernetics* [@Wiener:cybernetics48]. Cybernetics is Greek for\n",
    "governor, a word that in itself simply means helmsman in English.\n",
    "\n",
    "The recent WannaCry virus that had a wide impact on our health services\n",
    "ecosystem was exploiting a security flaw in Windows systems that was\n",
    "first exploited by a virus called Stuxnet.\n",
    "\n",
    "Stuxnet was a virus designed to infect the Iranian nuclear program's\n",
    "Uranium enrichment centrifuges. A centrifuge is prevented from overspeed\n",
    "by a controller, just like the centrifugal governor. Only now it is\n",
    "implemented in control logic, in this case on a Siemens PLC controller.\n",
    "\n",
    "Stuxnet infected these controllers and took over the response signal in\n",
    "the centrifuge, fooling the system into thinking that no overspeed was\n",
    "occuring. As a result, the centrifuges destroyed themselves through\n",
    "spinning too fast.\n",
    "\n",
    "This is equivalent to detaching the governor from the steam engine. Such\n",
    "sabotage would be easily recognized by a steam engine operator. The\n",
    "challenge for the operators of the Iranian Uranium centrifuges was that\n",
    "the sabotage was occurring inside the electronics.\n",
    "\n",
    "That is the effect of an adversary on an intelligent system, but even\n",
    "without adveraries, the mischief of a 10 year old can confuse our AIs.\n",
    "\n",
    "## Peppercorns [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/peppercorn.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/peppercorn.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "::: {.figure}\n",
    "::: {#peppercorn-siri-figure .figure-frame}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('1y2UKz47gew')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "::: {#peppercorn-siri-magnify .magnify onclick=\"magnifyFigure('peppercorn-siri')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#peppercorn-siri-caption .caption-frame}\n",
    "Figure: A peppercorn is a system design failure which is not a bug, but\n",
    "a conformance to design specification that causes problems when the\n",
    "system is deployed in the real world with mischevious and adversarial\n",
    "actors.\n",
    ":::\n",
    ":::\n",
    "\n",
    "Asking Siri \"What is a trillion to the power of a thousand minus one?\"\n",
    "leads to a 30 minute response consisting of only 9s. I found this out\n",
    "because my nine year old grabbed my phone and did it. The only way to\n",
    "stop Siri was to force closure. This is an interesting example of a\n",
    "system feature that's *not* a bug, in fact it requires clever processing\n",
    "from Wolfram Alpha. But it's an unexpected result from the system\n",
    "performing correctly.\n",
    "\n",
    "This challenge of facing a circumstance that was unenvisaged in design\n",
    "but has consequences in deployment becomes far larger when the\n",
    "environment is uncontrolled. Or in the extreme case, where actions of\n",
    "the intelligent system effect the wider environment and change it.\n",
    "\n",
    "These unforseen circumstances are likely to lead to need for much more\n",
    "efficient turn-around and update for our intelligent systems. Whether we\n",
    "are correcting for security flaws (which *are* bugs) or unenvisaged\n",
    "circumstantial challenges: an issue I'm referring to as *peppercorns*.\n",
    "Rapid deployment of system updates is required. For example, Apple have\n",
    "\"fixed\" the problem of Siri returning long numbers.\n",
    "\n",
    "The challenge is particularly acute because of the *scale* at which we\n",
    "can deploy AI solutions. This means when something does go wrong, it may\n",
    "be going wrong in billions of households simultaneously.\n",
    "\n",
    "You can also check my blog post on [\"Decision Making and\n",
    "Diversity\"](http://inverseprobability.com/2017/11/15/decision-making)\n",
    "You can also check my blog post on [\"Natural vs Artifical\n",
    "Intelligence\"](http://inverseprobability.com/2018/02/06/natural-and-artificial-intelligence)\n",
    "\n",
    "# The Three Ds of ML Systems Design\n",
    "\n",
    "# The Three Ds of Machine Learning Systems Design [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/the-3ds-of-ml-systems-design.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/the-3ds-of-ml-systems-design.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "We can characterize the challenges for integrating machine learning\n",
    "within our systems as the three Ds. Decomposition, Data and Deployment.\n",
    "\n",
    "You can also check my blog post on [\"The 3Ds of Machine Learning Systems\n",
    "Design\"](http://inverseprobability.com/2018/11/05/the-3ds-of-machine-learning-systems-design)\n",
    "\n",
    "The first two components *decomposition* and *data* are interlinked, but\n",
    "we will first outline the decomposition challenge. Below we will mainly\n",
    "focus on *supervised learning* because this is arguably the technology\n",
    "that is best understood within machine learning.\n",
    "\n",
    "## Decomposition [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-decomposition-challenge.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-decomposition-challenge.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "Machine learning is not magical pixie dust, we cannot simply automate\n",
    "all decisions through data. We are constrained by our data (see below)\n",
    "and the models we use.[^1] Machine learning models are relatively simple\n",
    "function mappings that include characteristics such as smoothness. With\n",
    "some famous exceptions, e.g. speech and image data, inputs are\n",
    "constrained in the form of vectors and the model consists of a\n",
    "mathematically well behaved function. This means that some careful\n",
    "thought has to be put in to the right sub-process to automate with\n",
    "machine learning. This is the challenge of *decomposition* of the\n",
    "machine learning system.\n",
    "\n",
    "Any repetitive task is a candidate for automation, but many of the\n",
    "repetitive tasks we perform as humans are more complex than any\n",
    "individual algorithm can replace. The selection of which task to\n",
    "automate becomes critical and has downstream effects on our overall\n",
    "system design.\n",
    "\n",
    "### Pigeonholing\n",
    "\n",
    "::: {.figure}\n",
    "::: {#too-many-pigeons2-figure .figure-frame}\n",
    "::: {.centered .centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/TooManyPigeons.jpg\" width=\"60%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#too-many-pigeons2-magnify .magnify onclick=\"magnifyFigure('too-many-pigeons2')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#too-many-pigeons2-caption .caption-frame}\n",
    "Figure: The machine learning systems decomposition process calls for\n",
    "separating a complex task into decomposable separate entities. A process\n",
    "we can think of as\n",
    "<a href=\"https://en.wikipedia.org/wiki/Pigeonholing\" target=\"_blank\" >pigeonholing</a>.\n",
    ":::\n",
    ":::\n",
    "\n",
    "Some aspects to take into account are\n",
    "\n",
    "1.  Can we refine the decision we need to a set of repetitive tasks\n",
    "    where input information and output decision/value is well defined?\n",
    "2.  Can we represent each sub-task we've defined with a mathematical\n",
    "    mapping?\n",
    "\n",
    "The representation necessary for the second aspect may involve massaging\n",
    "of the problem: feature selection or adaptation. It may also involve\n",
    "filtering out exception cases (perhaps through a pre-classification).\n",
    "\n",
    "All else being equal, we'd like to keep our models simple and\n",
    "interpretable. If we can convert a complex mapping to a linear mapping\n",
    "through clever selection of sub-tasks and features this is a big win.\n",
    "\n",
    "For example, Facebook have *feature engineers*, individuals whose main\n",
    "role is to design features they think might be useful for one of their\n",
    "tasks (e.g. newsfeed ranking, or ad matching). Facebook have a\n",
    "training/testing pipeline called\n",
    "[FBLearner](https://www.facebook.com/Engineering/posts/fblearner-flow-is-a-machine-learning-platform-capable-of-easily-reusing-algorith/10154077833317200/).\n",
    "Facebook have predefined the sub-tasks they are interested in, and they\n",
    "are tightly connected to their business model.\n",
    "\n",
    "It is easier for Facebook to do this because their business model is\n",
    "heavily focused on user interaction. A challenge for companies that have\n",
    "a more diversified portfolio of activities driving their business is the\n",
    "identification of the most appropriate sub-task. A potential solution to\n",
    "feature and model selection is known as *AutoML* [@Feurer:automl15]. Or\n",
    "we can think of it as using Machine Learning to assist Machine Learning.\n",
    "It's also called meta-learning. Learning about learning. The input to\n",
    "the ML algorithm is a machine learning task, the output is a proposed\n",
    "model to solve the task.\n",
    "\n",
    "One trap that is easy to fall in is too much emphasis on the type of\n",
    "model we have deployed rather than the appropriateness of the task\n",
    "decomposition we have chosen.\n",
    "\n",
    "**Recommendation**: Conditioned on task decomposition, we should\n",
    "automate the process of model improvement. Model updates should not be\n",
    "discussed in management meetings, they should be deployed and updated as\n",
    "a matter of course. Further details below on model deployment, but model\n",
    "updating needs to be considered at design time. This is the domain of\n",
    "AutoML.\n",
    "\n",
    "::: {.figure}\n",
    "::: {#chicken-and-egg2-figure .figure-frame}\n",
    "::: {.centered .centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/ai/chicken-and-egg.jpg\" width=\"50%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#chicken-and-egg2-magnify .magnify onclick=\"magnifyFigure('chicken-and-egg2')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#chicken-and-egg2-caption .caption-frame}\n",
    "Figure: The answer to the question which comes first, the chicken or the\n",
    "egg is simple, they co-evolve [@Popper:conjectures63]. Similarly, when\n",
    "we place components together in a complex machine learning system, they\n",
    "will tend to co-evolve and compensate for one another.\n",
    ":::\n",
    ":::\n",
    "\n",
    "To form modern decision making systems, many components are interlinked.\n",
    "We decompose our complex decision making into individual tasks, but the\n",
    "performance of each component is dependent on those upstream of it.\n",
    "\n",
    "This naturally leads to co-evolution of systems, upstream errors can be\n",
    "compensated by downstream corrections.\n",
    "\n",
    "To embrace this characteristic, end-to-end training could be considered.\n",
    "Why produce the best forecast by metrics when we can just produce the\n",
    "best forecast for our systems? End-to-end training can lead to\n",
    "improvements in performance, but it would also damage our systems\n",
    "decomposability and its interpretability, and perhaps its adaptability.\n",
    "\n",
    "The less human interpretable our systems are, the harder they are to\n",
    "adapt to different circumstances or diagnose when there's a challenge.\n",
    "The trade-off between interpretability and performance is a constant\n",
    "tension which we should always retain in our minds when performing our\n",
    "system design.\n",
    "\n",
    "## Data [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-data-challenge.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-data-challenge.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "It is difficult to overstate the importance of data. It is half of the\n",
    "equation for machine learning, but is often utterly neglected. We can\n",
    "speculate that there are two reasons for this. Firstly, data cleaning is\n",
    "perceived as tedious. It doesn't seem to consist of the same\n",
    "intellectual challenges that are inherent in constructing complex\n",
    "mathematical models and implementing them in code. Secondly, data\n",
    "cleaning is highly complex, it requires a deep understanding of how\n",
    "machine learning systems operate and good intuitions about the data\n",
    "itself, the domain from which data is drawn (e.g. Supply Chain) and what\n",
    "downstream problems might be caused by poor data quality.\n",
    "\n",
    "A consequence of these two reasons, data cleaning seems difficult to\n",
    "formulate into a readily teachable set of principles. As a result it is\n",
    "heavily neglected in courses on machine learning and data science.\n",
    "Despite data being half the equation, most University courses spend\n",
    "little to no time on its challenges.\n",
    "\n",
    "Anecdotally, talking to data modelling scientists. Most say they spend\n",
    "80% of their time acquiring and cleaning data. This is precipitating\n",
    "what I refer to as the \"data crisis\". This is an analogy with software.\n",
    "The \"software crisis\" was the phenomenon of inability to deliver\n",
    "software solutions due to increasing complexity of implementation. There\n",
    "was no single shot solution for the software crisis, it involved better\n",
    "practice (scrum, test orientated development, sprints, code review),\n",
    "improved programming paradigms (object orientated, functional) and\n",
    "better tools (CVS, then SVN, then git).\n",
    "\n",
    "## The Data Crisis [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/the-data-crisis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/the-data-crisis.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "Anecdotally, talking to data modelling scientists. Most say they spend\n",
    "80% of their time acquiring and cleaning data. This is precipitating\n",
    "what I refer to as the \"data crisis\". This is an analogy with software.\n",
    "The \"software crisis\" was the phenomenon of inability to deliver\n",
    "software solutions due to increasing complexity of implementation. There\n",
    "was no single shot solution for the software crisis, it involved better\n",
    "practice (scrum, test orientated development, sprints, code review),\n",
    "improved programming paradigms (object orientated, functional) and\n",
    "better tools (CVS, then SVN, then git).\n",
    "\n",
    "However, these challenges aren't new, they are merely taking a different\n",
    "form. From the computer's perspective software *is* data. The first wave\n",
    "of the data crisis was known as the *software crisis*.\n",
    "\n",
    "### The Software Crisis\n",
    "\n",
    "In the late sixties early software programmers made note of the\n",
    "increasing costs of software development and termed the challenges\n",
    "associated with it as the \"[Software\n",
    "Crisis](https://en.wikipedia.org/wiki/Software_crisis)\". Edsger Dijkstra\n",
    "referred to the crisis in his 1972 Turing Award winner's address.\n",
    "\n",
    "> The major cause of the software crisis is that the machines have\n",
    "> become several orders of magnitude more powerful! To put it quite\n",
    "> bluntly: as long as there were no machines, programming was no problem\n",
    "> at all; when we had a few weak computers, programming became a mild\n",
    "> problem, and now we have gigantic computers, programming has become an\n",
    "> equally gigantic problem.\n",
    ">\n",
    "> Edsger Dijkstra (1930-2002), The Humble Programmer\n",
    "\n",
    "> The major cause of the data crisis is that machines have become more\n",
    "> interconnected than ever before. Data access is therefore cheap, but\n",
    "> data quality is often poor. What we need is cheap high quality data.\n",
    "> That implies that we develop processes for improving and verifying\n",
    "> data quality that are efficient.\n",
    ">\n",
    "> There would seem to be two ways for improving efficiency. Firstly, we\n",
    "> should not duplicate work. Secondly, where possible we should automate\n",
    "> work.\n",
    "\n",
    "What I term \"The Data Crisis\" is the modern equivalent of this problem.\n",
    "The quantity of modern data, and the lack of attention paid to data as\n",
    "it is initially \"laid down\" and the costs of data cleaning are bringing\n",
    "about a crisis in data-driven decision making. This crisis is at the\n",
    "core of the challenge of *technical debt* in machine learning\n",
    "[@Sculley:debt15].\n",
    "\n",
    "Just as with software, the crisis is most correctly addressed by\n",
    "'scaling' the manner in which we process our data. Duplication of work\n",
    "occurs because the value of data cleaning is not correctly recognised in\n",
    "management decision making processes. Automation of work is increasingly\n",
    "possible through techniques in \"artificial intelligence\", but this will\n",
    "also require better management of the data science pipeline so that data\n",
    "about data science (meta-data science) can be correctly assimilated and\n",
    "processed. The Alan Turing institute has a program focussed on this\n",
    "area, [AI for Data\n",
    "Analytics](https://www.turing.ac.uk/research_projects/artificial-intelligence-data-analytics/).\n",
    "\n",
    "Data is the new software, and the data crisis is already upon us. It is\n",
    "driven by the cost of cleaning data, the paucity of tools for monitoring\n",
    "and maintaining our deployments, the provenance of our models (e.g. with\n",
    "respect to the data they're trained on).\n",
    "\n",
    "Three principal changes need to occur in response. They are cultural and\n",
    "infrastructural.\n",
    "\n",
    "### The Data First Paradigm\n",
    "\n",
    "First of all, to excel in data driven decision making we need to move\n",
    "from a *software first* paradigm to a *data first* paradigm. That means\n",
    "refocusing on data as the product. Software is the intermediary to\n",
    "producing the data, and its quality standards must be maintained, but\n",
    "not at the expense of the data we are producing. Data cleaning and\n",
    "maintenance need to be prized as highly as software debugging and\n",
    "maintenance. Instead of *software* as a service, we should refocus\n",
    "around *data* as a service. This first change is a cultural change in\n",
    "which our teams think about their outputs in terms of data. Instead of\n",
    "decomposing our systems around the software components, we need to\n",
    "decompose them around the data generating and consuming components.[^2]\n",
    "Software first is only an intermediate step on the way to be coming\n",
    "*data first*. It is a necessary, but not a sufficient condition for\n",
    "efficient machine learning systems design and deployment. We must move\n",
    "from *software orientated architecture* to a *data orientated\n",
    "architecture*.\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "Secondly, we need to improve our language around data quality. We cannot\n",
    "assess the costs of improving data quality unless we generate a language\n",
    "around what data quality means. Data Readiness Levels[^3] are an\n",
    "assessment of data quality that is based on the usage to which data is\n",
    "put.\n",
    "\n",
    "### Data Readiness Levels\n",
    "\n",
    "[Data Readiness\n",
    "Levels](http://inverseprobability.com/2017/01/12/data-readiness-levels)\n",
    "[@Lawrence:drl17] are an attempt to develop a language around data\n",
    "quality that can bridge the gap between technical solutions and decision\n",
    "makers such as managers and project planners. The are inspired by\n",
    "Technology Readiness Levels which attempt to quantify the readiness of\n",
    "technologies for deployment.b\n",
    "\n",
    "### Three Grades of Data Readiness [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/three-grades-of-data-readiness.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/three-grades-of-data-readiness.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "Data-readiness describes, at its coarsest level, three separate stages\n",
    "of data graduation.\n",
    "\n",
    "-   Grade C - accessibility\n",
    "    -   Transition: data becomes electronically available\n",
    "-   Grade B - validity\n",
    "    -   Transition: pose a question to the data.\n",
    "-   Grade A - usability\n",
    "\n",
    "The important definitions are at the transition. The move from Grade C\n",
    "data to Grade B data is delimited by the *electronic availability* of\n",
    "the data. The move from Grade B to Grade A data is delimited by posing a\n",
    "question or task to the data [@Lawrence:drl17].\n",
    "\n",
    "**Recommendation**: Build a shared understanding of the language of data\n",
    "readiness levels for use in planning documents and costing of data\n",
    "cleaning and the benefits of reusing cleaned data.\n",
    "\n",
    "### Move Beyond Software Engineering to Data Engineering\n",
    "\n",
    "Thirdly, we need to improve our mental model of the separation of data\n",
    "science from applied science. A common trap in our thinking around data\n",
    "is to see data science (and data engineering, data preparation) as a\n",
    "sub-set of the software engineer's or applied scientist's skill set. As\n",
    "a result we recruit and deploy the wrong type of resource. Data\n",
    "preparation and question formulation is superficially similar to both\n",
    "because of the need for programming skills, but the day to day problems\n",
    "faced are very different.\n",
    "\n",
    "## Combining Data and Systems Design [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-combining-data-and-systems-design-challenge.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-combining-data-and-systems-design-challenge.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "## Data Science as Debugging [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/data-science-as-debugging.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/data-science-as-debugging.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "One challenge for existing information technology professionals is\n",
    "realizing the extent to which a software ecosystem based on data differs\n",
    "from a classical ecosystem. In particular, by ingesting data we bring\n",
    "unknowns/uncontrollables into our decision making system. This presents\n",
    "opportunity for adversarial exploitation and unforeseen operation.\n",
    "\n",
    "You can also check my blog post on [\"Data Science as\n",
    "Debugging\"](http://inverseprobability.com/2017/03/14/data-science-as-debugging)\n",
    "\n",
    "Starting with the analysis of a data set, the nature of data science is\n",
    "somewhat difference from classical software engineering.\n",
    "\n",
    "One analogy I find helpful for understanding the depth of change we need\n",
    "is the following. Imagine as a software engineer, you find a USB stick\n",
    "on the ground. And for some reason you *know* that on that USB stick is\n",
    "a particular API call that will enable you to make a significant\n",
    "positive difference on a business problem. You don't know which of the\n",
    "many library functions on the USB stick are the ones that will help. And\n",
    "it could be that some of those library functions will hinder, perhaps\n",
    "because they are just inappropriate or perhaps because they have been\n",
    "placed there malisciously. The most secure thing to do would be to *not*\n",
    "introduce this code into your production system at all. But what if your\n",
    "manager told you to do so, how would you go about incorporating this\n",
    "code base?\n",
    "\n",
    "The answer is *very* carefully. You would have to engage in a process\n",
    "more akin to debugging than regular software engineering. As you\n",
    "understood the code base, for your work to be reproducible, you should\n",
    "be documenting it, not just what you discovered, but how you discovered\n",
    "it. In the end, you typically find a single API call that is the one\n",
    "that most benefits your system. But more thought has been placed into\n",
    "this line of code than any line of code you have written before.\n",
    "\n",
    "An enormous amount of debugging would be required. As the nature of the\n",
    "code base is understood, software tests to verify it also need to be\n",
    "constructed. At the end of all your work, the lines of software you\n",
    "write to actually interact with the software on the USB stick are likely\n",
    "to be minimal. But more thought would be put into those lines than\n",
    "perhaps any other lines of code in the system.\n",
    "\n",
    "Even then, when your API code is introduced into your production system,\n",
    "it needs to be deployed in an environment that monitors it. We cannot\n",
    "rely on an individual's decision making to ensure the quality of all our\n",
    "systems. We need to create an environment that includes quality\n",
    "controls, checks and bounds, tests, all designed to ensure that\n",
    "assumptions made about this foreign code base are remaining valid.\n",
    "\n",
    "This situation is akin to what we are doing when we incorporate data in\n",
    "our production systems. When we are consuming data from others, we\n",
    "cannot assume that it has been produced in alignment with our goals for\n",
    "our own systems. Worst case, it may have been adversarialy produced. A\n",
    "further challenge is that data is dynamic. So, in effect, the code on\n",
    "the USB stick is evolving over time.\n",
    "\n",
    "It might see that this process is easy to formalize now, we simply need\n",
    "to check what the formal software engineering process is for debugging,\n",
    "because that is the current software engineering activity that data\n",
    "science is closest to. But when we look for a formalization of debugging\n",
    "we find that there is none. Indeed, modern software engineering mainly\n",
    "focusses on ensuring that code is written without bugs in the first\n",
    "place.\n",
    "\n",
    "**Recommendation**: Anecdotally, resolving a machine learning challenge\n",
    "requires 80% of the resource to be focused on the data and perhaps 20%\n",
    "to be focused on the model. But many companies are too keen to employ\n",
    "machine learning engineers who focus on the models, not the data. We\n",
    "should change our hiring priorities and training. Universities cannot\n",
    "provide the understanding of how to data-wrangle. Companies must fill\n",
    "this gap.\n",
    "\n",
    "::: {.figure}\n",
    "::: {#derwent-valley-resevoir-figure .figure-frame}\n",
    "::: {.centered .centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/data-science/water-bridge-hill-transport-arch-calm-544448-pxhere.jpg\" width=\"80%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#derwent-valley-resevoir-magnify .magnify onclick=\"magnifyFigure('derwent-valley-resevoir')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#derwent-valley-resevoir-caption .caption-frame}\n",
    "Figure: A reservoir of data has more value if the data is consumbable.\n",
    "The data crisis can only be addressed if we focus on outputs rather than\n",
    "inputs.\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {.figure}\n",
    "::: {#lake-district-stream-figure .figure-frame}\n",
    "::: {.centered .centered style=\"\"}\n",
    "<img class=\"\" src=\"../slides/diagrams/data-science/1024px-Lake_District_picture.jpg\" width=\"80%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#lake-district-stream-magnify .magnify onclick=\"magnifyFigure('lake-district-stream')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#lake-district-stream-caption .caption-frame}\n",
    "Figure: For a data first architecture we need to clean our data at\n",
    "source, rather than individually cleaning data for each task. This\n",
    "involves a shift of focus from our inputs to our outputs. We should\n",
    "provide data streams that are consumable by many teams without\n",
    "purification.\n",
    ":::\n",
    ":::\n",
    "\n",
    "**Recommendation**: We need to share best practice around data\n",
    "deployment across our teams. We should make best use of our processes\n",
    "where applicable, but we need to develop them to become *data first*\n",
    "organizations. Data needs to be cleaned at *output* not at *input*.\n",
    "\n",
    "## Deployment [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-deployment-challenge.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/ml-deployment-challenge.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "Much of the academic machine learning systems point of view is based on\n",
    "a software systems point of view that is around 20 years out of date. In\n",
    "particular we build machine learning models on fixed training data sets,\n",
    "and we test them on stationary test data sets.\n",
    "\n",
    "In practice modern software systems involve continuous deployment of\n",
    "models into an ever evolving world of data. These changes are indicated\n",
    "in the software world by greater availaiblity of technologies like\n",
    "*streaming* technologies.\n",
    "\n",
    "### Continuous Deployment\n",
    "\n",
    "Once the decomposition is understood, the data is sourced and the models\n",
    "are created, the model code needs to be deployed.\n",
    "\n",
    "To extend the USB stick analogy further, how would we deploy that code\n",
    "if we thought it was likely to evolve in production? This is what\n",
    "datadoes. We cannot assume that the conditions under which we trained\n",
    "our model will be retained as we move forward, indeed the only constant\n",
    "we have is change.\n",
    "\n",
    "This means that when any data dependent model is deployed into\n",
    "production, it requires *continuous monitoring* to ensure the\n",
    "assumptions of design have not been invalidated. Software changes are\n",
    "qualified through testing, in particular a regression test ensures that\n",
    "existing functionality is not broken by change. Since data is\n",
    "continually evolving, machine learning systems require 'continual\n",
    "regression testing': oversight by systems that ensure their existing\n",
    "functionality has not been broken as the world evolves around them. An\n",
    "approach we refer to as *progression testing*. Unfortunately, standards\n",
    "around ML model deployment yet been developed. The modern world of\n",
    "continuous deployment does rely on testing, but it does not recognize\n",
    "the continuous evolution of the world around us.\n",
    "\n",
    "Progression tests are likely to be *statistical* tests in contrast to\n",
    "classical software tests. The should be monitoring model performance and\n",
    "quality measures. They could also monitor conformance to standardized\n",
    "*fairness* measures.\n",
    "\n",
    "If the world has changed around our decision making ecosystem, how are\n",
    "we alerted to those changes?\n",
    "\n",
    "**Recommendation**: We establish best practice around model deployment.\n",
    "We need to shift our culture from standing up a software service, to\n",
    "standing up a *data as a service*. Data as a Service would involve\n",
    "continual monitoring of our deployed models in production. This would be\n",
    "regulated by 'hypervisor' systems[^4] that understand the context in\n",
    "which models are deployed and recognize when circumstance has changed\n",
    "and models need retraining or restructuring.\n",
    "\n",
    "**Recommendation**: We should consider a major re-architecting of\n",
    "systems around our services. In particular we should scope the use of a\n",
    "*streaming architecture* (such as Apache Kafka) that ensures data\n",
    "persistence and enables asynchronous operation of our systems.[^5] This\n",
    "would enable the provision of QC streams, and real time dash boards as\n",
    "well as hypervisors..\n",
    "\n",
    "Importantly a streaming architecture implies the services we build are\n",
    "*stateless*, internal state is deployed on streams alongside external\n",
    "state. This allows for rapid assessment of other services' data.\n",
    "\n",
    "## Uncertainty Quantification [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/uncertainty-quantification.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/uncertainty-quantification.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "> Uncertainty quantification (UQ) is the science of quantitative\n",
    "> characterization and reduction of uncertainties in both computational\n",
    "> and real world applications. It tries to determine how likely certain\n",
    "> outcomes are if some aspects of the system are not exactly known.\n",
    "\n",
    "We will to illustrate different concepts of [Uncertainty\n",
    "Quantification](https://en.wikipedia.org/wiki/Uncertainty_quantification)\n",
    "(UQ) and the role that Gaussian processes play in this field. Based on a\n",
    "simple simulator of a car moving between a valley and a mountain, we are\n",
    "going to illustrate the following concepts:\n",
    "\n",
    "-   **Systems emulation**. Many real world decisions are based on\n",
    "    simulations that can be computationally very demanding. We will show\n",
    "    how simulators can be replaced by *emulators*: Gaussian process\n",
    "    models fitted on a few simulations that can be used to replace the\n",
    "    *simulator*. Emulators are cheap to compute, fast to run, and always\n",
    "    provide ways to quantify the uncertainty of how precise they are\n",
    "    compared the original simulator.\n",
    "\n",
    "-   **Emulators in optimization problems**. We will show how emulators\n",
    "    can be used to optimize black-box functions that are expensive to\n",
    "    evaluate. This field is also called Bayesian Optimization and has\n",
    "    gained an increasing relevance in machine learning as emulators can\n",
    "    be used to optimize computer simulations (and machine learning\n",
    "    algorithms) quite efficiently.\n",
    "\n",
    "-   **Multi-fidelity emulation methods**. In many scenarios we have\n",
    "    simulators of different quality about the same measure of interest.\n",
    "    In these cases the goal is to merge all sources of information under\n",
    "    the same model so the final emulator is cheaper and more accurate\n",
    "    than an emulator fitted only using data from the most accurate and\n",
    "    expensive simulator.\n",
    "\n",
    "## Mountain Car Simulator [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/mountain-car-simulation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/mountain-car-simulation.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "To illustrate the above mentioned concepts we we use the [mountain car\n",
    "simulator](https://github.com/openai/gym/wiki/MountainCarContinuous-v0).\n",
    "This simulator is widely used in machine learning to test reinforcement\n",
    "learning algorithms. The goal is to define a control policy on a car\n",
    "whose objective is to climb a mountain. Graphically, the problem looks\n",
    "as follows:\n",
    "\n",
    "::: {.figure}\n",
    "::: {#mountain-car-figure .figure-frame}\n",
    "::: {.centered style=\"\"}\n",
    "<img class=\"negate\" src=\"../slides/diagrams/uq/mountaincar.png\" width=\"60%\" height=\"auto\" align=\"center\" style=\"background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle\">\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-magnify .magnify onclick=\"magnifyFigure('mountain-car')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-caption .caption-frame}\n",
    "Figure: The mountain car simulation from the Open AI gym.\n",
    ":::\n",
    ":::\n",
    "\n",
    "The goal is to define a sequence of actions (push the car right or left\n",
    "with certain intensity) to make the car reach the flag after a number\n",
    "$T$ of time steps.\n",
    "\n",
    "At each time step $t$, the car is characterized by a vector\n",
    "$\\inputVector_{t} = (p_t,v_t)$ of states which are respectively the the\n",
    "position and velocity of the car at time $t$. For a sequence of states\n",
    "(an episode), the dynamics of the car is given by\n",
    "\n",
    "$$\\inputVector_{t+1} = \\mappingFunction(\\inputVector_{t},\\textbf{u}_{t})$$\n",
    "\n",
    "where $\\textbf{u}_{t}$ is the value of an action force, which in this\n",
    "example corresponds to push car to the left (negative value) or to the\n",
    "right (positive value). The actions across a full episode are\n",
    "represented in a policy $\\textbf{u}_{t} = \\pi(\\inputVector_{t},\\theta)$\n",
    "that acts according to the current state of the car and some parameters\n",
    "$\\theta$. In the following examples we will assume that the policy is\n",
    "linear which allows us to write $\\pi(\\inputVector_{t},\\theta)$ as\n",
    "\n",
    "$$\\pi(\\inputVector,\\theta)= \\theta_0 + \\theta_p p + \\theta_vv.$$\n",
    "\n",
    "For $t=1,\\dots,T$ now given some initial state $\\inputVector_{0}$ and\n",
    "some some values of each $\\textbf{u}_{t}$, we can **simulate** the full\n",
    "dynamics of the car for a full episode using\n",
    "[Gym](https://gym.openai.com/envs/). The values of $\\textbf{u}_{t}$ are\n",
    "fully determined by the parameters of the linear controller.\n",
    "\n",
    "After each episode of length $T$ is complete, a reward function\n",
    "$R_{T}(\\theta)$ is computed. In the mountain car example the reward is\n",
    "computed as 100 for reaching the target of the hill on the right hand\n",
    "side, minus the squared sum of actions (a real negative to push to the\n",
    "left and a real positive to push to the right) from start to goal. Note\n",
    "that our reward depend on $\\theta$ as we make it dependent on the\n",
    "parameters of the linear controller.\n",
    "\n",
    "## Emulate the Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this section is to find the parameters $\\theta$ of the\n",
    "linear controller such that\n",
    "\n",
    "$$\\theta^* = arg \\max_{\\theta} R_T(\\theta).$$\n",
    "\n",
    "In this section, we directly use Bayesian optimization to solve this\n",
    "problem. We will use [GPyOpt](https://sheffieldml.github.io/GPyOpt/) so\n",
    "we first define the objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mountain_car as mc\n",
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func = lambda x: mc.run_simulation(env, x)[0]\n",
    "objective = GPyOpt.core.task.SingleObjective(obj_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set of parameter values of the linear controller we can run an\n",
    "episode of the simulator (that we fix to have a horizon of $T=500$) to\n",
    "generate the reward. Using as input the parameters of the controller and\n",
    "as outputs the rewards we can build a Gaussian process emulator of the\n",
    "reward.\n",
    "\n",
    "We start defining the input space, which is three-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- We define the input space of the emulator\n",
    "\n",
    "space= [{'name':'postion_parameter', 'type':'continuous', 'domain':(-1.2, +1)},\n",
    "        {'name':'velocity_parameter', 'type':'continuous', 'domain':(-1/0.07, +1/0.07)},\n",
    "        {'name':'constant', 'type':'continuous', 'domain':(-1, +1)}]\n",
    "\n",
    "design_space = GPyOpt.Design_space(space=space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initizialize a Gaussian process emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian optimization an acquisition function is used to balance\n",
    "exploration and exploitation to evaluate new locations close to the\n",
    "optimum of the objective. In this notebook we select the expected\n",
    "improvement (EI). For further details have a look to the review paper of\n",
    "[Shahriari et al\n",
    "(2015)](http://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(design_space)\n",
    "acquisition = GPyOpt.acquisitions.AcquisitionEI(model, design_space, optimizer=aquisition_optimizer)\n",
    "evaluator = GPyOpt.core.evaluators.Sequential(acquisition) # Collect points sequentially, no parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initalize the model we start sampling some initial points (25) for\n",
    "the linear controler randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPyOpt.experiment_design.random_design import RandomDesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_initial_points = 25\n",
    "random_design = RandomDesign(design_space)\n",
    "initial_design = random_design.get_samples(n_initial_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start any optimization, lets have a look to the behavior of\n",
    "the car with the first of these initial points that we have selected\n",
    "randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_controller = initial_design[0,:]\n",
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(random_controller), render=True)\n",
    "anim=mc.animate_frames(frames, 'Random linear controller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.figure}\n",
    "::: {#mountain-car-random-figure .figure-frame}\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_random.html\" width=\"100%\" height=\"auto\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-random-magnify .magnify onclick=\"magnifyFigure('mountain-car-random')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-random-caption .caption-frame}\n",
    "Figure: Random linear controller for the Mountain car. It fails to move\n",
    "the car to the top of the mountain.\n",
    ":::\n",
    ":::\n",
    "\n",
    "As we can see the random linear controller does not manage to push the\n",
    "car to the top of the mountain. Now, let's optimize the regret using\n",
    "Bayesian optimization and the emulator for the reward. We try 50 new\n",
    "parameters chosen by the EI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 50\n",
    "bo = GPyOpt.methods.ModularBayesianOptimization(model, design_space, objective, acquisition, evaluator, initial_design)\n",
    "bo.run_optimization(max_iter = max_iter )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the result for the best controller that we have found\n",
    "with Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(bo.x_opt), render=True)\n",
    "anim=mc.animate_frames(frames, 'Best controller after 50 iterations of Bayesian optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.figure}\n",
    "::: {#mountain-car-similated-bayes-opt-figure .figure-frame}\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_simulated.html\" width=\"100%\" height=\"auto\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-similated-bayes-opt-magnify .magnify onclick=\"magnifyFigure('mountain-car-similated-bayes-opt')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-similated-bayes-opt-caption .caption-frame}\n",
    "Figure: Mountain car simulator trained using Bayesian optimization and\n",
    "the simulator of the dynamics. Fifty iterations of Bayesian optimization\n",
    "are used to optimize the controler.\n",
    ":::\n",
    ":::\n",
    "\n",
    "he car can now make it to the top of the mountain! Emulating the reward\n",
    "function and using the EI helped as to find a linear controller that\n",
    "solves the problem.\n",
    "\n",
    "## Data Efficient Emulation [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/mountain-car-data-efficient.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/mountain-car-data-efficient.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "In the previous section we solved the mountain car problem by directly\n",
    "emulating the reward but no considerations about the dynamics\n",
    "$\\inputVector_{t+1} = \\mappingFunction(\\inputVector_{t},\\textbf{u}_{t})$\n",
    "of the system were made. Note that we had to run 75 episodes of 500\n",
    "steps each to solve the problem, which required to call the simulator\n",
    "$500\\times 75 =37500$ times. In this section we will show how it is\n",
    "possible to reduce this number by building an emulator for $f$ that can\n",
    "later be used to directly optimize the control.\n",
    "\n",
    "The inputs of the model for the dynamics are the velocity, the position\n",
    "and the value of the control so create this space accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_dynamics = [{'name':'position', 'type':'continuous', 'domain':[-1.2, +0.6]},\n",
    "                  {'name':'velocity', 'type':'continuous', 'domain':[-0.07, +0.07]},\n",
    "                  {'name':'action', 'type':'continuous', 'domain':[-1, +1]}]\n",
    "design_space_dynamics = GPyOpt.Design_space(space=space_dynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are the velocity and the position. Indeed our model will\n",
    "capture the change in position and velocity on time. That is, we will\n",
    "model\n",
    "\n",
    "$$\\Delta v_{t+1} = v_{t+1} - v_{t}$$\n",
    "\n",
    "$$\\Delta x_{t+1} = p_{t+1} - p_{t}$$\n",
    "\n",
    "with Gaussian processes with prior mean $v_{t}$ and $p_{t}$\n",
    "respectively. As a covariance function, we use a Matern52. We need\n",
    "therefore two models to capture the full dynamics of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)\n",
    "velocity_model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sample some input parameters and use the simulator to compute\n",
    "the outputs. Note that in this case we are not running the full\n",
    "episodes, we are just using the simulator to compute\n",
    "$\\inputVector_{t+1}$ given $\\inputVector_{t}$ and $\\textbf{u}_{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from GPyOpt.experiment_design.random_design import RandomDesign\n",
    "import mountain_car as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Random locations of the inputs\n",
    "n_initial_points = 500\n",
    "random_design_dynamics = RandomDesign(design_space_dynamics)\n",
    "initial_design_dynamics = random_design_dynamics.get_samples(n_initial_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Simulation of the (normalized) outputs\n",
    "y = np.zeros((initial_design_dynamics.shape[0], 2))\n",
    "for i in range(initial_design_dynamics.shape[0]):\n",
    "    y[i, :] = mc.simulation(initial_design_dynamics[i, :])\n",
    "\n",
    "# Normalize the data from the simulation\n",
    "y_normalisation = np.std(y, axis=0)\n",
    "y_normalised = y/y_normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we might use much smarter strategies to design our emulation\n",
    "of the simulator. For example, we could use the variance of the\n",
    "predictive distributions of the models to collect points using\n",
    "uncertainty sampling, which will give us a better coverage of the space.\n",
    "For simplicity, we move ahead with the 500 randomly selected points.\n",
    "\n",
    "Now that we have a data set, we can update the emulators for the\n",
    "location and the velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_model.updateModel(initial_design_dynamics, y[:, [0]], None, None)\n",
    "velocity_model.updateModel(initial_design_dynamics, y[:, [1]], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have a look to how the emulator and the simulator match.\n",
    "First, we show a contour plot of the car aceleration for each pair of\n",
    "can position and velocity. You can use the bar bellow to play with the\n",
    "values of the controler to compare the emulator and the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the emulator is doing a fairly good job approximating the\n",
    "simulator. On the edges, however, it struggles to captures the dynamics\n",
    "of the simulator.\n",
    "\n",
    "Given some input parameters of the linear controlling, how do the\n",
    "dynamics of the emulator and simulator match? In the following figure we\n",
    "show the position and velocity of the car for the 500 time steps of an\n",
    "episode in which the parameters of the linear controller have been fixed\n",
    "beforehand. The value of the input control is also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_gains = np.atleast_2d([0, .6, 1])  # change the valus of the linear controller to observe the trayectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.figure}\n",
    "::: {#emu-sim-comparison-figure .figure-frame}\n",
    "<img src=\"../slides/diagrams/uq/emu_sim_comparison.svg\" class=\"\" align=\"80%\" style=\"vertical-align:middle;\">\n",
    ":::\n",
    "\n",
    "::: {#emu-sim-comparison-magnify .magnify onclick=\"magnifyFigure('emu-sim-comparison')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#emu-sim-comparison-caption .caption-frame}\n",
    "Figure: Comparison between the mountain car simulator and the emulator.\n",
    ":::\n",
    ":::\n",
    "\n",
    "We now make explicit use of the emulator, using it to replace the\n",
    "simulator and optimize the linear controller. Note that in this\n",
    "optimization, we don't need to query the simulator anymore as we can\n",
    "reproduce the full dynamics of an episode using the emulator. For\n",
    "illustrative purposes, in this example we fix the initial location of\n",
    "the car.\n",
    "\n",
    "We define the objective reward function in terms of the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Optimize control parameters with emulator\n",
    "car_initial_location = np.asarray([-0.58912799, 0]) \n",
    "\n",
    "### --- Reward objective function using the emulator\n",
    "obj_func_emulator = lambda x: mc.run_emulation([position_model, velocity_model], x, car_initial_location)[0]\n",
    "objective_emulator = GPyOpt.core.task.SingleObjective(obj_func_emulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before, we use Bayesian optimization to find the best possible\n",
    "linear controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Elements of the optimization that will use the multi-fidelity emulator\n",
    "model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design space is the three continuous variables that make up the\n",
    "linear controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= [{'name':'linear_1', 'type':'continuous', 'domain':(-1/1.2, +1)},\n",
    "        {'name':'linear_2', 'type':'continuous', 'domain':(-1/0.07, +1/0.07)},\n",
    "        {'name':'constant', 'type':'continuous', 'domain':(-1, +1)}]\n",
    "\n",
    "design_space         = GPyOpt.Design_space(space=space)\n",
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(design_space)\n",
    "\n",
    "random_design = RandomDesign(design_space)\n",
    "initial_design = random_design.get_samples(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the acquisition function to be expected improvement using\n",
    "`GPyOpt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition          = GPyOpt.acquisitions.AcquisitionEI(model, design_space, optimizer=aquisition_optimizer)\n",
    "evaluator            = GPyOpt.core.evaluators.Sequential(acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_emulator = GPyOpt.methods.ModularBayesianOptimization(model, design_space, objective_emulator, acquisition, evaluator, initial_design)\n",
    "bo_emulator.run_optimization(max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(bo_emulator.x_opt), render=True)\n",
    "anim=mc.animate_frames(frames, 'Best controller using the emulator of the dynamics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.figure}\n",
    "::: {#mountain-car-emulated-figure .figure-frame}\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_emulated.html\" width=\"100%\" height=\"auto\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-emulated-magnify .magnify onclick=\"magnifyFigure('mountain-car-emulated')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-emulated-caption .caption-frame}\n",
    "Figure: Mountain car controller learnt through emulation. Here 500 calls\n",
    "to the simulator are used to fit the controller rather than 37,500 calls\n",
    "to the simulator required in the standard learning.\n",
    ":::\n",
    ":::\n",
    "\n",
    "And the problem is again solved, but in this case we have replaced the\n",
    "simulator of the car dynamics by a Gaussian process emulator that we\n",
    "learned by calling the simulator only 500 times. Compared to the 37500\n",
    "calls that we needed when applying Bayesian optimization directly on the\n",
    "simulator this is a great gain.\n",
    "\n",
    "## Multi-Fidelity Emulation [\\[</span>[<a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/mountain-car-multi-fidelity.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/mountain-car-multi-fidelity.md', 13);\">edit</a>]{.editsection style=\"\"}<span class=\"editsection-bracket\" style=\"\">\\]]{.editsection-bracket style=\"\"}\n",
    "\n",
    "In some scenarios we have simulators of the same environment that have\n",
    "different fidelities, that is that reflect with different level of\n",
    "accuracy the dynamics of the real world. Running simulations of the\n",
    "different fidelities also have a different cost: hight fidelity\n",
    "simulations are more expensive the cheaper ones. If we have access to\n",
    "these simulators we can combine high and low fidelity simulations under\n",
    "the same model.\n",
    "\n",
    "So let's assume that we have two simulators of the mountain car\n",
    "dynamics, one of high fidelity (the one we have used) and another one of\n",
    "low fidelity. The traditional approach to this form of multi-fidelity\n",
    "emulation is to assume that\n",
    "\n",
    "$$\\mappingFunction_i\\left(\\inputVector\\right) = \\rho\\mappingFunction_{i-1}\\left(\\inputVector\\right) + \\delta_i\\left(\\inputVector \\right)$$\n",
    "\n",
    "where $\\mappingFunction_{i-1}\\left(\\inputVector\\right)$ is a low\n",
    "fidelity simulation of the problem of interest and\n",
    "$\\mappingFunction_i\\left(\\inputVector\\right)$ is a higher fidelity\n",
    "simulation. The function $\\delta_i\\left(\\inputVector \\right)$ represents\n",
    "the difference between the lower and higher fidelity simulation, which\n",
    "is considered additive. The additive form of this covariance means that\n",
    "if $\\mappingFunction_{0}\\left(\\inputVector\\right)$ and\n",
    "$\\left\\{\\delta_i\\left(\\inputVector \\right)\\right\\}_{i=1}^m$ are all\n",
    "Gaussian processes, then the process over all fidelities of simuation\n",
    "will be a joint Gaussian process.\n",
    "\n",
    "But with Deep Gaussian processes we can consider the form\n",
    "\n",
    "$$\\mappingFunction_i\\left(\\inputVector\\right) = \\mappingFunctionTwo_{i}\\left(\\mappingFunction_{i-1}\\left(\\inputVector\\right)\\right) + \\delta_i\\left(\\inputVector \\right),$$\n",
    "\n",
    "where the low fidelity representation is non linearly transformed by\n",
    "$\\mappingFunctionTwo(\\cdot)$ before use in the process. This is the\n",
    "approach taken in @Perdikaris:multifidelity17. But once we accept that\n",
    "these models can be composed, a highly flexible framework can emerge. A\n",
    "key point is that the data enters the model at different levels, and\n",
    "represents different aspects. For example these correspond to the two\n",
    "fidelities of the mountain car simulator.\n",
    "\n",
    "We start by sampling both of them at 250 random input locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Collect points from low and high fidelity simulator --- ###\n",
    "\n",
    "space = GPyOpt.Design_space([\n",
    "        {'name':'position', 'type':'continuous', 'domain':(-1.2, +1)},\n",
    "        {'name':'velocity', 'type':'continuous', 'domain':(-0.07, +0.07)},\n",
    "        {'name':'action', 'type':'continuous', 'domain':(-1, +1)}])\n",
    "\n",
    "n_points = 250\n",
    "random_design = GPyOpt.experiment_design.RandomDesign(space)\n",
    "x_random = random_design.get_samples(n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the high and low fidelity simualtors at those\n",
    "locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mountain_car as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_position_hf = np.zeros((n_points, 1))\n",
    "d_velocity_hf = np.zeros((n_points, 1))\n",
    "d_position_lf = np.zeros((n_points, 1))\n",
    "d_velocity_lf = np.zeros((n_points, 1))\n",
    "\n",
    "# --- Collect high fidelity points\n",
    "for i in range(0, n_points):\n",
    "    d_position_hf[i], d_velocity_hf[i] = mc.simulation(x_random[i, :])\n",
    "\n",
    "# --- Collect low fidelity points  \n",
    "for i in range(0, n_points):\n",
    "    d_position_lf[i], d_velocity_lf[i] = mc.low_cost_simulation(x_random[i, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to build the multi-fidelity model for both the position and\n",
    "the velocity.\n",
    "\n",
    "As we did in the previous section we use the emulator to optimize the\n",
    "simulator. In this case we use the high fidelity output of the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Optimize controller parameters \n",
    "obj_func = lambda x: mc.run_simulation(env, x)[0]\n",
    "obj_func_emulator = lambda x: mc.run_emulation([position_model, velocity_model], x, car_initial_location)[0]\n",
    "objective_multifidelity = GPyOpt.core.task.SingleObjective(obj_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we optimize using Bayesian optimzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPyOpt.experiment_design.random_design import RandomDesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)\n",
    "space= [{'name':'linear_1', 'type':'continuous', 'domain':(-1/1.2, +1)},\n",
    "        {'name':'linear_2', 'type':'continuous', 'domain':(-1/0.07, +1/0.07)},\n",
    "        {'name':'constant', 'type':'continuous', 'domain':(-1, +1)}]\n",
    "\n",
    "design_space = GPyOpt.Design_space(space=space)\n",
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(design_space)\n",
    "\n",
    "n_initial_points = 25\n",
    "random_design = RandomDesign(design_space)\n",
    "initial_design = random_design.get_samples(n_initial_points)\n",
    "acquisition = GPyOpt.acquisitions.AcquisitionEI(model, design_space, optimizer=aquisition_optimizer)\n",
    "evaluator = GPyOpt.core.evaluators.Sequential(acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_multifidelity = GPyOpt.methods.ModularBayesianOptimization(model, design_space, objective_multifidelity, acquisition, evaluator, initial_design)\n",
    "bo_multifidelity.run_optimization(max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(bo_multifidelity.x_opt), render=True)\n",
    "anim=mc.animate_frames(frames, 'Best controller with multi-fidelity emulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Controller with Multi-Fidelity Emulator\n",
    "\n",
    "::: {.figure}\n",
    "::: {#mountain-car-multi-fidelity-figure .figure-frame}\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_multi_fidelity.html\" width=\"100%\" height=\"auto\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-multi-fidelity-magnify .magnify onclick=\"magnifyFigure('mountain-car-multi-fidelity')\"}\n",
    "<img class=\"img-button\" src=\"/icons/Magnify_Large.svg\" style=\"width:1.5ex\">\n",
    ":::\n",
    "\n",
    "::: {#mountain-car-multi-fidelity-caption .caption-frame}\n",
    "Figure: Mountain car learnt with multi-fidelity model. Here 250\n",
    "observations of the high fidelity simulator and 250 observations of the\n",
    "low fidelity simulator are used to learn the controller.\n",
    ":::\n",
    ":::\n",
    "\n",
    "And problem solved! We see how the problem is also solved with 250\n",
    "observations of the high fidelity simulator and 250 of the low fidelity\n",
    "simulator.\n",
    "\n",
    "# References {#references .unnumbered}\n",
    "\n",
    "[^1]: We can also become constrained by our tribal thinking, just as\n",
    "    each of the other groups can.\n",
    "\n",
    "[^2]: This is related to challenges of machine learning and technical\n",
    "    debt [@Sculley:debt15], although we are trying to frame the solution\n",
    "    here rather than the problem.\n",
    "\n",
    "[^3]: [Data Readiness\n",
    "    Levels](http://inverseprobability.com/2017/01/12/data-readiness-levels)\n",
    "    [@Lawrence:drl17] are an attempt to develop a language around data\n",
    "    quality that can bridge the gap between technical solutions and\n",
    "    decision makers such as managers and project planners. The are\n",
    "    inspired by Technology Readiness Levels which attempt to quantify\n",
    "    the readiness of technologies for deployment.\n",
    "\n",
    "[^4]: Emulation, or surrogate modelling, is one very promising approach\n",
    "    to forming such a hypervisor. Emulators are models we fit to other\n",
    "    models, often simulations, but the could also be other machine\n",
    "    learning models. These models operate at the meta-level, not on the\n",
    "    systems directly. This means they can be used to model how the\n",
    "    sub-systems interact. As well as emulators we should consider real\n",
    "    time dash boards, anomaly detection, mutlivariate analysis, data\n",
    "    visualization and classical statistical approaches for hypervision\n",
    "    of our deployed systems.\n",
    "\n",
    "[^5]: These approaches are one area of focus for my own team's\n",
    "    reasearch. A data first architecture is a prerequisite for efficient\n",
    "    deployment of machine learning systems."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
