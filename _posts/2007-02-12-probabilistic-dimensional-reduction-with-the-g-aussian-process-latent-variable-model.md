---
title: Probabilistic Dimensional Reduction with the {G}aussian Process Latent Variable
  Model
abstract: Density modelling in high dimensions is a very difficult problem. Traditional
  approaches, such as mixtures of Gaussians, typically fail to capture the structure
  of data sets in high dimensional spaces. In this talk we will argue that for many
  data sets of interest, the data can be represented as a lower dimensional manifold
  immersed in the higher dimensional space. We will then present the Gaussian Process
  Latent Variable Model (GP-LVM), a non-linear probabilistic variant of principal
  component analysis (PCA) which implicitly assumes that the data lies on a lower
  dimensional space. Having introduced the GP-LVM we will review extensions to the
  algorithm, including dynamics, learning of large data sets and back constraints.
  We will demonstrate the application of the model and its extensions to a range of
  data sets, including human motion data, a vowel data set and a robot mapping problem.
venue: Google Research, New York, N.Y., U.S.A.
linkvideo: http://video.google.com/videoplay?docid=-5127068978792458641
youtube: DS853uA0u4I
linkpdf: ftp://ftp.dcs.shef.ac.uk/home/neil/gplvm_07_02.pdf
label1: Demos Software
link1: http://inverseprobability.com/oxford/
label2: Main Software
link2: https://github.com/SheffieldML/GPmat/
year: '2007'
month: 2
day: '12'
group: gplvm
layout: talk
key: Lawrence:google07
categories:
- Lawrence:google07
authors:
- firstname: Neil D.
  lastname: Lawrence
published: 2007-02-12
---
