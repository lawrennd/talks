---
title: "Artificial Intelligence: Reclaiming Control"
abstract: |
  Though artificial intelligence is ubiquitous in our homes and
  workplaces, there is widespread misunderstanding of what it really
  is. Join us for this public lecture as Neil Lawrence, DeepMind
  Professor of Machine Learning encourages us to reframe our view of
  AI.
 
  He’ll discuss how the artificial systems we have developed operate
  in a fundamentally different way to our own intelligence. He’ll
  describe how this difference in operational capability leads us to
  misunderstand the influence that decisions made by machine
  intelligence are having on our lives. Without this understanding we
  cannot take back control of those decisions from the machine. Along
  the way, he’ll chat with fellow Cambridge University researchers
  about how we maximise the benefits of these technologies while
  minimising the harms.
author:
- family: Lawrence
  given: Neil D.
  gscholar: r3SJcvoAAAAJ
  institute: University of Cambridge
  twitter: lawrennd
  url: http://inverseprobability.com
date: 2022-04-09
venue: "LT1, William Gates Building, Cambridge Festival"
transition: None
---
 

\notes{\section{Introduction}}

\notes{It’s said that Henry Ford’s customers wanted a “a faster horse”. If Henry Ford was selling us artificial intelligence today, what would the customer call for, “a smarter human”? That’s certainly the picture of machine intelligence we find in science fiction narratives, but the reality of what we’ve developed is much more mundane. 
 
Car engines produce prodigious power from petrol. Machine intelligences deliver decisions derived from data. In both cases the scale of consumption enables a speed of operation that is far beyond the capabilities of their natural counterparts. Unfettered energy consumption has consequences in the form of climate change. Does unbridled data consumption also have consequences for us? 
 
 If we devolve decision making to machines, we depend on those machines to accommodate our needs. If we don’t understand how those machines operate, we lose control over our destiny. Our mistake has been to see machine intelligence as a reflection of our intelligence. We cannot understand the smarter human without understanding the human. To understand the machine, we need to better understand ourselves.}

\newslide

\notes{Cambridge has been involved in the formulation of the methods used in the current wave of AI solutions since their beginnings. In 1997 I attend the Machine Learning and Generalisation Summer School at the Newton Institute. There we heard from many of those who developed the methods that are foundational to the recent wave of progress, including Geoff Hinton and Yann LeCun.}
 
\figure{\includejpg{\diagramsDir/people/1997-08-02-neil-newton-institute}{70%}}{Neil standing outside the Newton Institute on 2nd August 1997, just after arriving for "Generalisation in Neural Networks and Machine Learning", [see page 26-30 of this report](http://www.newton.ac.uk/files/reports/annual/ini_annual_report_97-98.pdf).}{neil-newton-institute}

\newslide

\figure{\includejpg{\diagramsDir/art/sistine-chapel-ceiling}{100%}}{The ceiling of the Sistine Chapel.}{cappella-sistina-ceiling}


\notes{[Patrick Boyde](https://www.mmll.cam.ac.uk/pb127)'s talks on the Sistine Chapel focussed on both the structure of the chapel ceiling, describing the impression of height it was intended to give, as well as the significance and positioning of each of the panels and the meaning of the individual figures.}

\newslide

\figure{\includejpg{\diagramsDir/art/the-creation-of-man-michelangelo}{80%}}{Photo of Detail of Creation of Man from the Sistine chapel ceiling.}{the-creation-of-man-michelangelo}

\notes{One of the most famous panels is central in the ceiling, it's the creation of man. Here, God in the guise of a pink-robed bearded man reaches out to a languid Adam.}

\newslide

\notes{The representation of God in this form seems typical of the time, because elsewhere in the Vatican Museums there are similar representations.}

\figure{\includejpg{\diagramsDir/art/the-creation-of-man-detail-god-michelangelo}{80%}}{Photo detail of God.}{the-creation-of-man-detail-god-michelangelo}

\notes{<https://commons.wikimedia.org/wiki/File:Michelangelo,_Creation_of_Adam_04.jpg>}


\newslide

\notes{For a time at the head of all articles about AI, an [image of the terminator](https://www.flickr.com/photos/tom-margie/2144882415/sizes/o/) was included.}

\figure{\includejpg{\diagramsDir/ai/terminator-image}{70%}}{Image of James Cameron's terminator. Images like this have been used to illustrate articles about artificial intelligence.}{terminator-image}


\newslide

\notes{Sometimes, this image is even combined with that of God to create what [Beth Singler](https://bvsingler.com), a digital anthropologist who is a JRF at Hmerton College, refers to as the creation meme [@Singler-aicreation20].} 

\figure{\includejpg{\diagramsDir/people/beth-singler}{80%}}{Beth Singler is a digital anthropologist who holds a JRF at Homerton College. She has explored parallels between the Michelangelo image of creation and our own notion of robotic creation}{beth-singler}

\notes{So in a very real sense, we can see that both God and AI are viewed by us as embodied intelligences, whether creator or created. We show these other-intelligences in a humanoid form.}

\newslide

\figure{\includejpg{\diagramsDir/people/david-j-c-mackay}{40%}}{Photo of David Mackay on Bicycle by David Stern. Taken for the book "Sustainable Energy without the Hot Air".}{david-j-c-mackay}

\notes{My own understanding of why we might want to picture these intelligences as embodied goes back to lectures I heard at the institute by David MacKay. By the time I arrived in Cambridge, David was very focussed on the relationships between learning and information theory, and as well as his lecture at the Newton Institute, his group meetings were focussed on information theory and machine learning.}

\newslide

\notes{The key idea I wasnt to communicate next is related to our ability to share our thoughts.}

\include{_ai/includes/the-diving-bell-butterfly.md}

\include{_ai/includes/jean-dominique-bauby.md}

\newslide

\figure{\columns{\aligncenter{\includejpg{\diagramsDir/ai/Jean-Dominique_Bauby}{100%}}}{\aligncenter{\includejpg{\diagramsDir/ClaudeShannon_MFO3807}{70%}}}}{Claude Shannon developed information theory which allows us to quantify how much Bauby can communicate. This allows us to compare how locked in he is to us.}{bauby-shannon}

\include{_ai/includes/embodiment-factors-short.md}

\include{_ai/includes/heider-simmel.md}
\include{_ai/includes/conversation.md}
\include{_ai/includes/conversation-computer.md}

\newslide

\figure{\includediagramclass{\diagramsDir/ai/anne-imagine-ai}{50%}}{Our tendency to anthrox means that even when an intelligence is very different from ours we tend to embody it and represent it as having objectives similar to human.}{anne-imageine-ai}

\newslide

\figure{\includediagramclass{\diagramsDir/ai/anne-imagine-god}{50%}}{Our tendency to anthrox means that even when an intelligence is very different from ours we tend to embody it and represent it as having objectives similar to human.}{anne-imageine-god}

\include{_data-science/includes/evolved-relationship.md}

\newslide

\aligncenter{\dianaRobinsonPicture{30%}}

\notes{In the group [Diana Robinson]() has been focussing on how we can communicate the machine's understanding of *uncertainty* to clinicians, within the context blood plasma infusions and surgical operations.}

\notes{
\subsection{Fairness in Decision Making}}

\notes{As a more general example, let's consider fairness in decision making. Computers make decisions on the basis of our data, how can we have confidence in those decisions?}

\newslide{}

\figure{\includepng{\diagramsDir/data-science/convention-108-coe}{70%}}{The convention for the protection of individuals with regard to the processing of personal data was opened for signature on 28th January 1981. It was the first legally binding international instrument in the field of data protection.}{convention-108-coe}

\notes{\include{_governance/includes/gdpr-overview.md}}

\notes{The GDPR gives us some indications of the aspects we might consider when judging whether or not a decision is "fair".}

\notes{But when considering fairness, it seems that there's two forms that we might consider.}

\newslide


\figure{\includediagramclass{\diagramsDir/ai/n-p-fairness}{80%}}{We seem to have two different aspects to fairness, which in practice can be in tension.}{n-p-fairness}

\notes{We've outlined $n$-fairness and $p$-fairness. By $n$-fairness we mean the sort of considerations that are associated with *substantive* equality of opportunity vs *formal* equality of opportunity. Formal equality of community is related to $p$-fairness. This is sometimes called procedural fairness and we might think of it as a *performative* form of fairness. It's about clarity of rules, for example as applied in sport. $n$-Fairness is more nuanced. It's a reflection of society's normative judgment about how individuals may have been disadvantaged, e.g. due to their upbringing.}

\notes{The important point here is that these forms of fairness are in tension. Good procedural fairness needs to be clear and understandable. It should be clear to everyone what the rules are, they shouldn't be obscured by jargon or overly subtle concepts. $p$-Fairness should not be easily undermined by adversaries, it should be difficult to "cheat" good $p$-fairness. However, $n$-fairness requires nuance, understanding of the human condition, where we came from and how different individuals in our society have been advantaged or disadvantaged in their upbringing and their access to opportunity.}

\notes{Pure $n$-fairness and pure $p$-fairness both have the feeling of dystopias. In practice, any decision making system needs to balance the two. The correct point of operation will depend on the context of the decision. Consider fair rules of a game of football, against fair distribution of social benefit. It is unlikely that there is ever an objectively correct balance between the two for any given context. Different individuals will favour $p$ vs $n$ according to their personal values.}

\notes{Given the tension between the two forms of fairness, with $p$ fairness requiring simple rules that are understandable by all, and $n$ fairness requiring nuance and subtlety, how do we resolve this tension in practice?}

\notes{Normally in human systems, significant decisions involve trained professionals. For example, judges, or accountants or doctors.}

\notes{Training a professional involves lifting their "reflexive" response to a situation with "reflective" thinking about the consequences of their decision that rely not just on the professional's expertise, but also their knowledge of what it is to be a human.}

\notes{This *marvellous* resolution exploits the fact that while humans are increadibly complicated nuanced entities, other humans have an intuitive ability to understand their motivations and values. So the human is a complex entity that seems simple to other humans.}

\newslide

$$\text{reflect} \Longleftrightarrow \text{reflex}$$

\newslide

\slides{\aligncenter{The Great AI Fallacy}}

\notes{\include{_ai/includes/the-great-ai-fallacy.md}}

\notes{In large part, these challenges associated with AI are because AI has no understanding of the human condition. But there's also a problem that we don't have an intuitive understanding of AI and how it is working.}

\notes{The marvellous resolution does not apply to machine driven decisions, because we *don't* have an intuitive understanding of what motivates the machine.}

\newslide

\aligncenter{If AI isn't a tool for us, then we are the tool of AI.}

\notes{The consequence is that the AI, driven by it's detailed knowledge of who we are, arising from its access to large quantities of our data, can undermine the delicate balance of our decision-making, and replace our objectives with it's own simplistic ideas of how things should be.}

\notes{So, what are the resolutions for this problem? At Cambridge we are focussed on three different interventions.}

\newslide{Accelerate Program: Empower the User}

\notes{The first example is empowering those who want to use AI through *education* and tool development. The [Accelerate Programme for Scientific Discovery](https://acceleratescience.github.io/index.html), sponsored by Schmidt Futures, focusses on empowering scientists and other domains across the University with the tools and understanding they need to make use of AI in practice.}

\figure{\includepng{\diagramsDir/ai/accelerate-science-project-page}{60%}}{Empower domain experts to ensure that they are using AI as a tool, understanding the implications of how they deploy their solutions, and how they can refine their scientific explorations with these new capabilities.}{accelerate-science-project}

\newslide

\aligncenter{\sarahMorganPicture{30%}}

\notes{Sarah Morgan is one of our fellows, she'll tell us about how she makes use of the machine's capabilities in improving understanding and diagnostics of schizophrenia.}

\notes{Other examples of this form of work include our collaboration with [Data Science Africa](http://www.datascienceafrica.org/), which focusses on empowering individuals with solutions for solving challenges that emerge in the African context.}

\newslide{Auto AI: Resolve Intellectual Debt}

\figure{\includepng{\diagramsDir/ai/autoai-project-page}{60%}}{Address challenges in the way that complex software systems involving machine learning components are constructed to deal with the challenge of Intellectual Debt.}{autoai-project-page}

\notes{A second intervention is dealing with the complexity of the software systems that underpin modern AI solutions. Even if two individuals, say African masters students, who are technically capable and have an interesting idea, deploy their idea. One challenge they face is the operational load in *maintaining* and *explaining* their software systems. The challenge of *maintaining* is known as intellectual debt [@Sculley:debt15], the problem of *explaining* is known as [intellectual debt](https://medium.com/berkman-klein-center/from-technical-debt-to-intellectual-debt-in-ai-e05ac56a502c).}

\notes{The [AutoAI project](https://mlatcl.github.io/projects/autoai.html), sponsored by an ATI Senior AI Fellowship addresses this challenge.}

\newslide{Data Trusts: Empower People through their Data}

\notes{The third intervention goes direct to the source of the machine's power. What we are seeing is an emergent *[digital oligarchy](https://www.theguardian.com/media-network/2015/mar/05/digital-oligarchy-algorithms-personal-data)* based on the power that comes with aggregation of data. [Data Trusts](https://www.theguardian.com/media-network/2016/jun/03/data-trusts-privacy-fears-feudalism-democracy) are form of data intermediary designed to reutrn the power associated with this data accumulation to the originators of the data, that is us.}

\notes{The [Data Trusts Initiative](https://datatrusts.uk/), funded by the Patrick J. McGovern Foundation is supporting three pilot projects that consider how bottom-up empowerment can redress the imbalance associated with the digital oligarchy.}

\figure{\includepng{\diagramsDir/ai/data-trusts-initiative-project-page}{60%}}{Address challenges in the way that complex software systems involving machine learning components are constructed to deal with the challenge of Intellectual Debt.}{data-trusts-initiative-project-page}

\newslide{AI@Cam}

\figure{\includegif{\diagramsDir/ai/inference-group-logo}{20%}}{AI@Cam is a Flagship Programme that supports AI research across the University.}{ai-at-cam-flagship}

\notes{Finally, we are working across the University to empower the diversity ofexpertise and capability we have to focus on these broad societal problems. We will shortly be launching AI@Cam, with a landscaping document, outlining these challenges and exploring different strategies the University has to address them.} 

\subsection{Conclusions}

* Humans view intelligence as embodied, but AI is not embodied.
* Critical decisions are dependent on human nuance to reconcile the tension between $p$-Fairness and $n$-Fairness.
* This implies that AI should only ever be seen as a tool of humans.
* Much of current technology makes us a tool of the AI.

\newslide{Conclusions Contd}

* Three interventions to address this.
  * Empower the humans to better use the tools.
  * Develop better standards for creating AI systems.
  * Develop data intermediaries to allow citizens to have a voice in how their data is used.
  
\thanks

\references


\comment{Start with some of the explanation of the difference between artificial and human intelligence. (With Pepper and Nao at the front to illustrate anthropomorphism.) 

The embodiment factor – we represent an all-seeing, all-knowing God as a bearded, robed character. We represent AI as the Terminator. But these are completely ‘other’ intelligence and neither is embodied in that way. We do so because of our human tendency to anthropomorphise, but doing that with machines is problematic. If you try and interact in a human way with an artificial intelligence, that’s how you lose control.
Pepper and Nao can communicate as one, and as one with the internet, whereas we are restricted. That goes to the heart of the theme. 
Differences between human and machine decision-making
1 Fairness
Re human decision-making, there’s 2 types of fairness. Performative fairness (procedural fairness) is if we all understand and engage with these rules to do this thing, it will lead to this expected outcome. Normative fairness (substantive fairness) is where we take the individual with all their human experience into account. It’s a quite different intention. And normative fairness can be abused, while performative fairness can simply embed existing unfairness. So no decision should ever be based purely on one form of fairness. 
One requires simplicity because it requires everyone to be engaged and to understand what to do to get a desired outcome. The other requires nuance. There are tensions between the two approaches and these can be resolved if a human is involved in the decision making. But it’s much harder to employ a machine to make that kind of nuanced decision-making because what is it going to take into account when making the decision? We are unable to replace humans with machines to do that. 
Not only the difference between thinking speed (faster in humans) and communication speed (much, much lower in humans), but also the fact that while humans model other humans in their head, adapting their communication to that model when they communicate, computers’ intelligence is based upon large amounts of data. Humans are bad at large data, but good with other humans. Computers are very good with large data, but bad with humans. So when we are interacting with computers it is very important to remember that they are not another human. 
2 Differences in computation and communication capacities
Computers don’t understand us because we are not yet capable of designing things that understand our moods. And humans empathise in order to communicate. Computers don’t. As humans, we anthropomorphise – whether it’s our cats, our dogs, or our cars. We give them all human characteristics in an effort to understand them. Our computers are very different: their computation is being based on very large amounts of data. Our fears around AI are being based on this anthopomorphism. We fear we are creating a better version of us – something with the same motivations as ourselves, only more powerful, because it has additional capabilities. But we are not. 
The real danger is a very different one – it’s that those computers do not anthropomorphise us. They work below our cognitive radar, they make decisions about us without understanding the human condition. Artificial intelligence is artificial in the way a plastic plant is artificial. It has some aspects of real intelligence, but it is constructed in a very different way. There are real dangers with AI as there are with any new technology. But if we are to face those dangers head-on as a society, we need to understand what we are dealing with.
The AI fallacy, that AI will adapt to us. It won’t.
The great AI fallacy is that we are building the first wave of automation that will adapt to us, instead of expecting us to adapt to it. If you look historically, all our automation has required humans to accommodate it: roads now prioritise cars over people, we’ve had to build railways for trains, factories require us to turn up on time to operate machines. One thing that’s common to almost everything that’s written about AI in public, is the idea that because AI is “intelligent”, it is going to be accommodating us in ways other automation hasn’t. I just don’t think that that is true and I see no evidence for it. Indeed, it seems more like everything is still going in the opposite direction. The way that AI is employed, we are having to adapt to it rather than it adapting to us. 
So what do we do about this? We intervene.
Examples of interventions
1) Bring Diana Robinson in here to talk about her work on human-machine interfaces.
Her work is on helping doctors to use machine-derived data in what they’re doing and helping them understand what the machine is doing. Removing uncertainty: i.e. looking at how a machine can represent its uncertainty in a way that a doctor can assimilate it. 
2) The need to view AI as a tool and bring in Sarah Morgan to talk about Accelerate.
If you don’t understand AI as a tool and can’t wield it yourself for it to do things for you, then it is doing things to you, or using your data and if you don’t understand the ways in which that is happening, then that is problematic. 
Because AI is a tool, that’s why we have the Accelerate Programme which is equipping scientists to use machine learning techniques. Sarah Morgan – substantive vs procedural fairness. When computer does give a decision, if you don’t understand basis of decision making, you can’t negotiate with it. Set Sarah up to talk about what she does. (Computer says no.)
We need better understanding and more interventions to prevent existing problems… People deploy systems that operate in ways that are far more complex than they understand. For example, Facebook didn’t understand the extent of interference in the election of Donald Trump, and Mark Zuckerberg said it was a crazy idea that the Russians would influence the election. They then found out several months later that yes, they had influenced the election. 
Illustrations of the use and misuse of data leads us on to consider why we should be much more careful about the use of our personal data and how much of it we give away.
Our views about machines taking over from us reflect our own lack of understanding of what is really going on. The more immediate problem is taking back control of our data, and then further on thinking more about how the machines see us and what we need to do to help that understanding and making sure AI does accommodate us if we understand better to what AI really is. 
Example of intervention around use of data: the Data Trusts Initiative 
The new pilot projects show how some community groups are using a data trust to give their communities/groups more control over their data: Brixham and deprivation being an example. Coastal communities are deprived communities, but deprived in a different way than an inner-city area like Rotherham. So interventions to help them need to be different, the idea of a ‘one size fits all’ intervention doesn’t work. Data from the Brixham community could help inform that intervention, though it raises the question, how can a community have a say as to how that data is being used about them to set up a state-level intervention?
CONCLUSION
We are imagining enemies – but there are ones out there that we really need to be worrying about. Particularly the ones we are not really aware of, like the crimes committed in international waters where it’s far harder to say where responsibility lies. Everyone has heard about Deepwater Horizon, but not Trafigura who dumped oil off the Ivory Coast. At least with Facebook and BP those accidents were a failure of process, not a conscious transgression of law. The long-term problem is that everyone wants to talk about Google and Facebook and DeepMind and they are highly incentivised to align their activities with what the public wants. But there are lots of companies working behind the scenes that we aren’t so conscious of.
Take-home messages
If we are going to be empowered by AI, rather than ruled by it, it’s important that we have more understanding of these things. 

15 minutes for questions – could take some as we go along (gathering them via Slido), or we could leave them for the end. 

}
